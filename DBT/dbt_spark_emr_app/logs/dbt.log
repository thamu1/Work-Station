[0m08:03:23.117329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbbf3d6e7b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbbf4098140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbbf3d6e720>]}
[0m08:03:23.122531 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 08:03:23.122986 | fddba46d-3d4c-4904-92c5-909c84f86b7c ==============================
[0m08:03:23.122986 [info ] [MainThread]: Running with dbt=1.9.2
[0m08:03:23.126483 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m08:03:23.183715 [info ] [MainThread]: dbt version: 1.9.2
[0m08:03:23.185724 [info ] [MainThread]: python version: 3.12.3
[0m08:03:23.186960 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m08:03:23.187980 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m08:03:23.410912 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m08:03:23.411435 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m08:03:23.411812 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m08:03:23.444074 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m08:03:23.445587 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m08:03:23.446779 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m08:03:23.448157 [info ] [MainThread]: adapter type: spark
[0m08:03:23.449094 [info ] [MainThread]: adapter version: 1.9.1
[0m08:03:23.599446 [info ] [MainThread]: Configuration:
[0m08:03:23.600642 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m08:03:23.601408 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m08:03:23.602103 [info ] [MainThread]: Required dependencies:
[0m08:03:23.602856 [debug] [MainThread]: Executing "git --help"
[0m08:03:24.149767 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m08:03:24.150735 [debug] [MainThread]: STDERR: "b''"
[0m08:03:24.151349 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m08:03:24.153014 [info ] [MainThread]: Connection:
[0m08:03:24.154053 [info ] [MainThread]:   host: localhost
[0m08:03:24.155020 [info ] [MainThread]:   port: 10001
[0m08:03:24.156217 [info ] [MainThread]:   cluster: None
[0m08:03:24.157495 [info ] [MainThread]:   endpoint: None
[0m08:03:24.158560 [info ] [MainThread]:   schema: default
[0m08:03:24.159943 [info ] [MainThread]:   organization: 0
[0m08:03:24.161521 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m08:03:24.718893 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m08:03:24.719384 [debug] [MainThread]: Using spark connection "debug"
[0m08:03:24.719708 [debug] [MainThread]: On debug: select 1 as id
[0m08:03:24.719990 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:03:24.723952 [error] [MainThread]: Could not connect to any of [('127.0.0.1', 10001)]
[0m08:03:24.725331 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m08:03:24.725836 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m08:03:24.726301 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m08:03:24.727773 [info ] [MainThread]: [31m1 check failed:[0m
[0m08:03:24.728940 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m08:03:24.733894 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 1.6850278, "process_in_blocks": "0", "process_kernel_time": 1.015625, "process_mem_max_rss": "94520", "process_out_blocks": "0", "process_user_time": 1.578125}
[0m08:03:24.734493 [debug] [MainThread]: Command `dbt debug` failed at 08:03:24.734355 after 1.69 seconds
[0m08:03:24.734908 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m08:03:24.735719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbbf4725b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbbf34b9df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbbf3de6cc0>]}
[0m08:03:24.736368 [debug] [MainThread]: An error was encountered while trying to send an event
[0m08:03:24.736975 [debug] [MainThread]: Flushing usage events
[0m08:06:19.862569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7583d10890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7582675ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7582952de0>]}
[0m08:06:19.867276 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 08:06:19.867690 | 2765cb21-ca77-4166-b0a1-d01b7862e9df ==============================
[0m08:06:19.867690 [info ] [MainThread]: Running with dbt=1.9.2
[0m08:06:19.869489 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m08:06:19.898781 [info ] [MainThread]: dbt version: 1.9.2
[0m08:06:19.901251 [info ] [MainThread]: python version: 3.12.3
[0m08:06:19.902646 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m08:06:19.904090 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m08:06:20.123982 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m08:06:20.124492 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m08:06:20.124853 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m08:06:20.157530 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m08:06:20.158849 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m08:06:20.159886 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m08:06:20.161369 [info ] [MainThread]: adapter type: spark
[0m08:06:20.162785 [info ] [MainThread]: adapter version: 1.9.1
[0m08:06:20.264979 [info ] [MainThread]: Configuration:
[0m08:06:20.266755 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m08:06:20.268009 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m08:06:20.269247 [info ] [MainThread]: Required dependencies:
[0m08:06:20.270520 [debug] [MainThread]: Executing "git --help"
[0m08:06:20.300174 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m08:06:20.301309 [debug] [MainThread]: STDERR: "b''"
[0m08:06:20.302012 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m08:06:20.304013 [info ] [MainThread]: Connection:
[0m08:06:20.304947 [info ] [MainThread]:   host: localhost
[0m08:06:20.306121 [info ] [MainThread]:   port: 10001
[0m08:06:20.307145 [info ] [MainThread]:   cluster: None
[0m08:06:20.308070 [info ] [MainThread]:   endpoint: None
[0m08:06:20.309385 [info ] [MainThread]:   schema: default
[0m08:06:20.310839 [info ] [MainThread]:   organization: 0
[0m08:06:20.312654 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m08:06:20.529478 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m08:06:20.529925 [debug] [MainThread]: Using spark connection "debug"
[0m08:06:20.530244 [debug] [MainThread]: On debug: select 1 as id
[0m08:06:20.530540 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:06:20.534238 [error] [MainThread]: Could not connect to any of [('127.0.0.1', 10001)]
[0m08:06:20.535582 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m08:06:20.535969 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m08:06:20.536498 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m08:06:20.537789 [info ] [MainThread]: [31m1 check failed:[0m
[0m08:06:20.538861 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m08:06:20.544996 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.7495153, "process_in_blocks": "0", "process_kernel_time": 0.796875, "process_mem_max_rss": "94528", "process_out_blocks": "0", "process_user_time": 1.671875}
[0m08:06:20.546283 [debug] [MainThread]: Command `dbt debug` failed at 08:06:20.546003 after 0.75 seconds
[0m08:06:20.546920 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m08:06:20.547449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7582cb14f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7581c79df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7581c79c70>]}
[0m08:06:20.547871 [debug] [MainThread]: An error was encountered while trying to send an event
[0m08:06:20.548289 [debug] [MainThread]: Flushing usage events
[0m08:10:54.878570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc898c0ec00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc89930c140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc898c0eba0>]}
[0m08:10:54.883625 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 08:10:54.884060 | e0cbfe4b-4679-49a0-9f81-69dc57058a7b ==============================
[0m08:10:54.884060 [info ] [MainThread]: Running with dbt=1.9.2
[0m08:10:54.886161 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m08:10:54.912784 [info ] [MainThread]: dbt version: 1.9.2
[0m08:10:54.915051 [info ] [MainThread]: python version: 3.12.3
[0m08:10:54.916507 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m08:10:54.918112 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m08:10:55.147619 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m08:10:55.148126 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m08:10:55.148496 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m08:10:55.179835 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m08:10:55.181396 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m08:10:55.182395 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m08:10:55.184001 [info ] [MainThread]: adapter type: spark
[0m08:10:55.185259 [info ] [MainThread]: adapter version: 1.9.1
[0m08:10:55.292464 [info ] [MainThread]: Configuration:
[0m08:10:55.294334 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m08:10:55.295834 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m08:10:55.297180 [info ] [MainThread]: Required dependencies:
[0m08:10:55.298426 [debug] [MainThread]: Executing "git --help"
[0m08:10:55.329349 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m08:10:55.330191 [debug] [MainThread]: STDERR: "b''"
[0m08:10:55.330562 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m08:10:55.332355 [info ] [MainThread]: Connection:
[0m08:10:55.333528 [info ] [MainThread]:   host: localhost
[0m08:10:55.335439 [info ] [MainThread]:   port: 10001
[0m08:10:55.336995 [info ] [MainThread]:   cluster: None
[0m08:10:55.338824 [info ] [MainThread]:   endpoint: None
[0m08:10:55.339948 [info ] [MainThread]:   schema: default
[0m08:10:55.341520 [info ] [MainThread]:   organization: 0
[0m08:10:55.343004 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m08:10:55.549626 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m08:10:55.550100 [debug] [MainThread]: Using spark connection "debug"
[0m08:10:55.550457 [debug] [MainThread]: On debug: select 1 as id
[0m08:10:55.550768 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:10:55.554370 [error] [MainThread]: Could not connect to any of [('127.0.0.1', 10001)]
[0m08:10:55.556491 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m08:10:55.556885 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m08:10:55.557265 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m08:10:55.559093 [info ] [MainThread]: [31m1 check failed:[0m
[0m08:10:55.560978 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m08:10:55.566604 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.7558958, "process_in_blocks": "0", "process_kernel_time": 0.828125, "process_mem_max_rss": "94532", "process_out_blocks": "0", "process_user_time": 1.71875}
[0m08:10:55.567706 [debug] [MainThread]: Command `dbt debug` failed at 08:10:55.567484 after 0.76 seconds
[0m08:10:55.568333 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m08:10:55.568946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc898f38230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc898359490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc898cff4d0>]}
[0m08:10:55.569710 [debug] [MainThread]: An error was encountered while trying to send an event
[0m08:10:55.570207 [debug] [MainThread]: Flushing usage events
[0m08:19:48.875103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845d4fe8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845d1965a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845b66ade0>]}
[0m08:19:48.878485 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 08:19:48.878800 | bfe1fd58-85d0-442a-9c29-f9d1e4a01f53 ==============================
[0m08:19:48.878800 [info ] [MainThread]: Running with dbt=1.9.2
[0m08:19:48.880211 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m08:19:48.897160 [info ] [MainThread]: dbt version: 1.9.2
[0m08:19:48.898284 [info ] [MainThread]: python version: 3.12.3
[0m08:19:48.899485 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m08:19:48.900534 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m08:19:49.088057 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m08:19:49.088473 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m08:19:49.088759 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m08:19:49.114540 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m08:19:49.115983 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m08:19:49.116946 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m08:19:49.118364 [info ] [MainThread]: adapter type: spark
[0m08:19:49.119307 [info ] [MainThread]: adapter version: 1.9.1
[0m08:19:49.206975 [info ] [MainThread]: Configuration:
[0m08:19:49.208314 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m08:19:49.209281 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m08:19:49.210339 [info ] [MainThread]: Required dependencies:
[0m08:19:49.211366 [debug] [MainThread]: Executing "git --help"
[0m08:19:49.228212 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m08:19:49.228738 [debug] [MainThread]: STDERR: "b''"
[0m08:19:49.229044 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m08:19:49.230141 [info ] [MainThread]: Connection:
[0m08:19:49.230920 [info ] [MainThread]:   host: localhost
[0m08:19:49.231619 [info ] [MainThread]:   port: 10001
[0m08:19:49.232212 [info ] [MainThread]:   cluster: None
[0m08:19:49.233012 [info ] [MainThread]:   endpoint: None
[0m08:19:49.234140 [info ] [MainThread]:   schema: default
[0m08:19:49.234819 [info ] [MainThread]:   organization: 0
[0m08:19:49.235765 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m08:19:49.417936 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m08:19:49.418675 [debug] [MainThread]: Using spark connection "debug"
[0m08:19:49.419161 [debug] [MainThread]: On debug: select 1 as id
[0m08:19:49.419563 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:19:49.423177 [error] [MainThread]: Could not connect to any of [('127.0.0.1', 10001)]
[0m08:19:49.425236 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m08:19:49.425864 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m08:19:49.426437 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m08:19:49.427942 [info ] [MainThread]: [31m1 check failed:[0m
[0m08:19:49.429111 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m08:19:49.434436 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.6117288, "process_in_blocks": "0", "process_kernel_time": 0.875, "process_mem_max_rss": "94544", "process_out_blocks": "0", "process_user_time": 1.125}
[0m08:19:49.435233 [debug] [MainThread]: Command `dbt debug` failed at 08:19:49.435092 after 0.61 seconds
[0m08:19:49.435630 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m08:19:49.435946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845b809580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845ab2e450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f845aa05520>]}
[0m08:19:49.436213 [debug] [MainThread]: An error was encountered while trying to send an event
[0m08:19:49.436412 [debug] [MainThread]: Flushing usage events
[0m10:10:21.462004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32f65427e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32f78c3380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32f6c4eea0>]}
[0m10:10:21.501937 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 10:10:21.502419 | 13ad3408-1f8c-489f-bad6-2613c7963b20 ==============================
[0m10:10:21.502419 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:10:21.503816 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m10:10:21.810431 [info ] [MainThread]: dbt version: 1.9.2
[0m10:10:21.811750 [info ] [MainThread]: python version: 3.12.3
[0m10:10:21.812606 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m10:10:21.813352 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m10:10:24.711377 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:10:24.711811 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:10:24.712093 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:10:24.743243 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m10:10:24.744133 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m10:10:24.744748 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m10:10:24.745434 [info ] [MainThread]: adapter type: spark
[0m10:10:24.746072 [info ] [MainThread]: adapter version: 1.9.1
[0m10:10:24.837696 [info ] [MainThread]: Configuration:
[0m10:10:24.838946 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m10:10:24.839819 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:10:24.840328 [info ] [MainThread]: Required dependencies:
[0m10:10:24.840831 [debug] [MainThread]: Executing "git --help"
[0m10:10:25.156782 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:10:25.157297 [debug] [MainThread]: STDERR: "b''"
[0m10:10:25.157573 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:10:25.158835 [info ] [MainThread]: Connection:
[0m10:10:25.159760 [info ] [MainThread]:   host: localhost
[0m10:10:25.160449 [info ] [MainThread]:   port: 10001
[0m10:10:25.161092 [info ] [MainThread]:   cluster: None
[0m10:10:25.161611 [info ] [MainThread]:   endpoint: None
[0m10:10:25.162352 [info ] [MainThread]:   schema: default
[0m10:10:25.162952 [info ] [MainThread]:   organization: 0
[0m10:10:25.163761 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m10:10:25.535134 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m10:10:25.535474 [debug] [MainThread]: Using spark connection "debug"
[0m10:10:25.535716 [debug] [MainThread]: On debug: select 1 as id
[0m10:10:25.535948 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:10:25.537980 [error] [MainThread]: Could not connect to any of [('127.0.0.1', 10001)]
[0m10:10:25.539272 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m10:10:25.539765 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m10:10:25.540260 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m10:10:25.541496 [info ] [MainThread]: [31m1 check failed:[0m
[0m10:10:25.542350 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m10:10:25.555439 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 4.558205, "process_in_blocks": "0", "process_kernel_time": 1.3125, "process_mem_max_rss": "94532", "process_out_blocks": "0", "process_user_time": 1.484375}
[0m10:10:25.555918 [debug] [MainThread]: Command `dbt debug` failed at 10:10:25.555832 after 4.56 seconds
[0m10:10:25.556245 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m10:10:25.556543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32f99e3140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32f58fad20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32f62a82f0>]}
[0m10:10:25.556792 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:10:25.557012 [debug] [MainThread]: Flushing usage events
[0m10:58:05.900000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f424adec5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f424b2559a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f424be12de0>]}
[0m10:58:05.903614 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 10:58:05.904038 | 5021d6aa-e6b9-4e3c-8d01-bc68308cc629 ==============================
[0m10:58:05.904038 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:58:05.905219 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:58:05.925230 [info ] [MainThread]: dbt version: 1.9.2
[0m10:58:05.926908 [info ] [MainThread]: python version: 3.12.3
[0m10:58:05.928089 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m10:58:05.928894 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m10:58:06.100800 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:58:06.101208 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:58:06.101496 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:58:06.127818 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m10:58:06.128921 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m10:58:06.129718 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m10:58:06.130745 [info ] [MainThread]: adapter type: spark
[0m10:58:06.131401 [info ] [MainThread]: adapter version: 1.9.1
[0m10:58:06.213032 [info ] [MainThread]: Configuration:
[0m10:58:06.214428 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m10:58:06.215455 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:58:06.216067 [info ] [MainThread]: Required dependencies:
[0m10:58:06.216987 [debug] [MainThread]: Executing "git --help"
[0m10:58:06.235249 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:58:06.236280 [debug] [MainThread]: STDERR: "b''"
[0m10:58:06.237205 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:58:06.239348 [info ] [MainThread]: Connection:
[0m10:58:06.240464 [info ] [MainThread]:   host: localhost
[0m10:58:06.241337 [info ] [MainThread]:   port: 10001
[0m10:58:06.242068 [info ] [MainThread]:   cluster: None
[0m10:58:06.242902 [info ] [MainThread]:   endpoint: None
[0m10:58:06.243760 [info ] [MainThread]:   schema: default
[0m10:58:06.244533 [info ] [MainThread]:   organization: 0
[0m10:58:06.245362 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m10:58:06.406212 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m10:58:06.406608 [debug] [MainThread]: Using spark connection "debug"
[0m10:58:06.406901 [debug] [MainThread]: On debug: select 1 as id
[0m10:58:06.407138 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:58:06.407432 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m10:58:06.407677 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m10:58:06.407944 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m10:58:06.409373 [info ] [MainThread]: [31m1 check failed:[0m
[0m10:58:06.410272 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m10:58:06.413975 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.5694582, "process_in_blocks": "0", "process_kernel_time": 0.75, "process_mem_max_rss": "94436", "process_out_blocks": "0", "process_user_time": 1.375}
[0m10:58:06.414358 [debug] [MainThread]: Command `dbt debug` failed at 10:58:06.414272 after 0.57 seconds
[0m10:58:06.414636 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m10:58:06.414926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f424ad2b800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f424a7582f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f424a30ed80>]}
[0m10:58:06.415193 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:58:06.415417 [debug] [MainThread]: Flushing usage events
[0m11:03:22.594886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ab2e47b00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ab47263f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ab3197fe0>]}
[0m11:03:22.598990 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:03:22.599377 | 23fd36b0-43f1-49ba-b72b-47694193e9c0 ==============================
[0m11:03:22.599377 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:03:22.600345 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:03:22.619847 [info ] [MainThread]: dbt version: 1.9.2
[0m11:03:22.620950 [info ] [MainThread]: python version: 3.12.3
[0m11:03:22.621921 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m11:03:22.622799 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m11:03:22.793141 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:03:22.793561 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:03:22.793846 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:03:22.821108 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m11:03:22.822293 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m11:03:22.823158 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m11:03:22.824216 [info ] [MainThread]: adapter type: spark
[0m11:03:22.825048 [info ] [MainThread]: adapter version: 1.9.1
[0m11:03:22.903747 [info ] [MainThread]: Configuration:
[0m11:03:22.905038 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:03:22.906050 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:03:22.906964 [info ] [MainThread]: Required dependencies:
[0m11:03:22.907816 [debug] [MainThread]: Executing "git --help"
[0m11:03:22.926151 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:03:22.926610 [debug] [MainThread]: STDERR: "b''"
[0m11:03:22.927250 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:03:22.928537 [info ] [MainThread]: Connection:
[0m11:03:22.929313 [info ] [MainThread]:   host: localhost
[0m11:03:22.930230 [info ] [MainThread]:   port: 10001
[0m11:03:22.931110 [info ] [MainThread]:   cluster: None
[0m11:03:22.931825 [info ] [MainThread]:   endpoint: None
[0m11:03:22.932668 [info ] [MainThread]:   schema: default
[0m11:03:22.933432 [info ] [MainThread]:   organization: 0
[0m11:03:22.934201 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:03:23.095442 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m11:03:23.095921 [debug] [MainThread]: Using spark connection "debug"
[0m11:03:23.096243 [debug] [MainThread]: On debug: select 1 as id
[0m11:03:23.096497 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:03:23.096818 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m11:03:23.097083 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m11:03:23.097342 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m11:03:23.098708 [info ] [MainThread]: [31m1 check failed:[0m
[0m11:03:23.099596 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m11:03:23.102383 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.5619842, "process_in_blocks": "0", "process_kernel_time": 0.765625, "process_mem_max_rss": "94420", "process_out_blocks": "0", "process_user_time": 1.359375}
[0m11:03:23.102778 [debug] [MainThread]: Command `dbt debug` failed at 11:03:23.102688 after 0.56 seconds
[0m11:03:23.103073 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:03:23.103364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ab339eed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ab2856cc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ab1fa92b0>]}
[0m11:03:23.103615 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:03:23.103856 [debug] [MainThread]: Flushing usage events
[0m11:43:13.829796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29ae4eade0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29aea3dd60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29ae5deba0>]}
[0m11:43:13.833707 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:43:13.834027 | 9488d8b8-d2d5-4017-a53e-c6a9b7ced167 ==============================
[0m11:43:13.834027 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:43:13.835250 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:43:13.858023 [info ] [MainThread]: dbt version: 1.9.2
[0m11:43:13.859423 [info ] [MainThread]: python version: 3.12.3
[0m11:43:13.860357 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m11:43:13.861041 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m11:43:14.057982 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:43:14.058427 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:43:14.058735 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:43:14.086653 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m11:43:14.087966 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m11:43:14.089042 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m11:43:14.090101 [info ] [MainThread]: adapter type: spark
[0m11:43:14.091097 [info ] [MainThread]: adapter version: 1.9.1
[0m11:43:14.173172 [info ] [MainThread]: Configuration:
[0m11:43:14.174469 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:43:14.175464 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:43:14.176286 [info ] [MainThread]: Required dependencies:
[0m11:43:14.177119 [debug] [MainThread]: Executing "git --help"
[0m11:43:14.459760 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:43:14.460238 [debug] [MainThread]: STDERR: "b''"
[0m11:43:14.460491 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:43:14.461722 [info ] [MainThread]: Connection:
[0m11:43:14.462560 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m11:43:14.463259 [info ] [MainThread]:   port: 10001
[0m11:43:14.464054 [info ] [MainThread]:   cluster: None
[0m11:43:14.464821 [info ] [MainThread]:   endpoint: None
[0m11:43:14.465476 [info ] [MainThread]:   schema: default
[0m11:43:14.466310 [info ] [MainThread]:   organization: 0
[0m11:43:14.467277 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:43:14.643489 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m11:43:14.643862 [debug] [MainThread]: Using spark connection "debug"
[0m11:43:14.644114 [debug] [MainThread]: On debug: select 1 as id
[0m11:43:14.644330 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:43:14.644604 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m11:43:14.644897 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m11:43:14.645170 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m11:43:14.646611 [info ] [MainThread]: [31m1 check failed:[0m
[0m11:43:14.647561 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m11:43:14.652031 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 1.3779843, "process_in_blocks": "0", "process_kernel_time": 1.015625, "process_mem_max_rss": "94420", "process_out_blocks": "0", "process_user_time": 1.328125}
[0m11:43:14.652563 [debug] [MainThread]: Command `dbt debug` failed at 11:43:14.652446 after 1.38 seconds
[0m11:43:14.652961 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:43:14.653317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29ae393500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29ad87ed80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29ae12aba0>]}
[0m11:43:14.653647 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:43:14.653934 [debug] [MainThread]: Flushing usage events
[0m12:45:22.059285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec79872f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec8d7c650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec72a2330>]}
[0m12:45:22.062950 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:45:22.063281 | baa25d33-3497-4e9b-8a22-84a05d90c4f7 ==============================
[0m12:45:22.063281 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:45:22.064528 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:45:22.084388 [info ] [MainThread]: dbt version: 1.9.2
[0m12:45:22.085328 [info ] [MainThread]: python version: 3.12.3
[0m12:45:22.085906 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m12:45:22.086682 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m12:45:22.262325 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:45:22.262735 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:45:22.263027 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:45:22.288401 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m12:45:22.289802 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m12:45:22.290681 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m12:45:22.291513 [info ] [MainThread]: adapter type: spark
[0m12:45:22.292300 [info ] [MainThread]: adapter version: 1.9.1
[0m12:45:22.367270 [info ] [MainThread]: Configuration:
[0m12:45:22.368629 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:45:22.369503 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:45:22.370319 [info ] [MainThread]: Required dependencies:
[0m12:45:22.371206 [debug] [MainThread]: Executing "git --help"
[0m12:45:22.664392 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:45:22.664996 [debug] [MainThread]: STDERR: "b''"
[0m12:45:22.665286 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:45:22.666647 [info ] [MainThread]: Connection:
[0m12:45:22.667421 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m12:45:22.668194 [info ] [MainThread]:   port: 10001
[0m12:45:22.668891 [info ] [MainThread]:   cluster: None
[0m12:45:22.669627 [info ] [MainThread]:   endpoint: None
[0m12:45:22.670230 [info ] [MainThread]:   schema: default
[0m12:45:22.670753 [info ] [MainThread]:   organization: 0
[0m12:45:22.671554 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:45:22.831342 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:45:22.831750 [debug] [MainThread]: Using spark connection "debug"
[0m12:45:22.832018 [debug] [MainThread]: On debug: select 1 as id
[0m12:45:22.832247 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:45:43.947628 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m12:45:43.949541 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:45:43.950102 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:45:43.950729 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:45:43.952607 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:45:43.953816 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:45:43.958603 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 22.73573, "process_in_blocks": "0", "process_kernel_time": 0.96875, "process_mem_max_rss": "94572", "process_out_blocks": "0", "process_user_time": 1.125}
[0m12:45:43.959459 [debug] [MainThread]: Command `dbt debug` failed at 12:45:43.959355 after 22.74 seconds
[0m12:45:43.959822 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:45:43.960099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec8c41220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec6929df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec65bb2c0>]}
[0m12:45:43.960337 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:45:43.960547 [debug] [MainThread]: Flushing usage events
[0m05:08:38.983556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc74989e5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc747d8be60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc747d1e000>]}
[0m05:08:39.003974 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 05:08:39.004320 | 3a0eaaa4-e96a-4d69-9433-27db8a2ff0ea ==============================
[0m05:08:39.004320 [info ] [MainThread]: Running with dbt=1.9.2
[0m05:08:39.005800 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:08:39.246496 [info ] [MainThread]: dbt version: 1.9.2
[0m05:08:39.247924 [info ] [MainThread]: python version: 3.12.3
[0m05:08:39.249141 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m05:08:39.250365 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m05:08:44.666088 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m05:08:44.666673 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m05:08:44.666954 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m05:08:44.702027 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m05:08:44.703963 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m05:08:44.705272 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m05:08:44.706770 [info ] [MainThread]: adapter type: spark
[0m05:08:44.708004 [info ] [MainThread]: adapter version: 1.9.1
[0m05:08:44.795142 [info ] [MainThread]: Configuration:
[0m05:08:44.796796 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m05:08:44.797916 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m05:08:44.799028 [info ] [MainThread]: Required dependencies:
[0m05:08:44.800134 [debug] [MainThread]: Executing "git --help"
[0m05:08:45.216074 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m05:08:45.216684 [debug] [MainThread]: STDERR: "b''"
[0m05:08:45.216953 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m05:08:45.218678 [info ] [MainThread]: Connection:
[0m05:08:45.219805 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m05:08:45.221004 [info ] [MainThread]:   port: 10001
[0m05:08:45.222180 [info ] [MainThread]:   cluster: None
[0m05:08:45.223213 [info ] [MainThread]:   endpoint: None
[0m05:08:45.223924 [info ] [MainThread]:   schema: s3_database
[0m05:08:45.225404 [info ] [MainThread]:   organization: 0
[0m05:08:45.226805 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m05:08:45.633407 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m05:08:45.633896 [debug] [MainThread]: Using spark connection "debug"
[0m05:08:45.634257 [debug] [MainThread]: On debug: select 1 as id
[0m05:08:45.634583 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:09:06.772237 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m05:09:06.775227 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m05:09:06.776130 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m05:09:06.777035 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m05:09:06.779719 [info ] [MainThread]: [31m1 check failed:[0m
[0m05:09:06.781788 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m05:09:06.822508 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 27.935427, "process_in_blocks": "0", "process_kernel_time": 2.25, "process_mem_max_rss": "94580", "process_out_blocks": "0", "process_user_time": 1.578125}
[0m05:09:06.823759 [debug] [MainThread]: Command `dbt debug` failed at 05:09:06.823526 after 27.94 seconds
[0m05:09:06.824423 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m05:09:06.824993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc74788cf20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc747d1e000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7479563f0>]}
[0m05:09:06.825476 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:09:06.825922 [debug] [MainThread]: Flushing usage events
[0m06:03:29.778449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54279273e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54285b48f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5427e690d0>]}
[0m06:03:29.797576 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 06:03:29.797891 | 1ac13432-3bd7-4175-b92c-524c8ae0af9c ==============================
[0m06:03:29.797891 [info ] [MainThread]: Running with dbt=1.9.2
[0m06:03:29.799170 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m06:03:29.943626 [info ] [MainThread]: dbt version: 1.9.2
[0m06:03:29.944854 [info ] [MainThread]: python version: 3.12.3
[0m06:03:29.945540 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m06:03:29.946182 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m06:03:32.616234 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:03:32.616997 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:03:32.617329 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:03:32.645366 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m06:03:32.646644 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m06:03:32.647456 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m06:03:32.648189 [info ] [MainThread]: adapter type: spark
[0m06:03:32.648883 [info ] [MainThread]: adapter version: 1.9.1
[0m06:03:32.731886 [info ] [MainThread]: Configuration:
[0m06:03:32.733278 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m06:03:32.734214 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m06:03:32.735330 [info ] [MainThread]: Required dependencies:
[0m06:03:32.736369 [debug] [MainThread]: Executing "git --help"
[0m06:03:32.782749 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m06:03:32.783432 [debug] [MainThread]: STDERR: "b''"
[0m06:03:32.783853 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m06:03:32.785306 [info ] [MainThread]: Connection:
[0m06:03:32.786383 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m06:03:32.787430 [info ] [MainThread]:   port: 10000
[0m06:03:32.788386 [info ] [MainThread]:   cluster: None
[0m06:03:32.789284 [info ] [MainThread]:   endpoint: None
[0m06:03:32.789946 [info ] [MainThread]:   schema: s3_database
[0m06:03:32.790868 [info ] [MainThread]:   organization: 0
[0m06:03:32.791877 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m06:03:33.132525 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m06:03:33.133284 [debug] [MainThread]: Using spark connection "debug"
[0m06:03:33.133798 [debug] [MainThread]: On debug: select 1 as id
[0m06:03:33.134261 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:03:54.282799 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10000)]
[0m06:03:54.285715 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m06:03:54.286607 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m06:03:54.287477 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m06:03:54.289821 [info ] [MainThread]: [31m1 check failed:[0m
[0m06:03:54.291260 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m06:03:54.311251 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 24.658934, "process_in_blocks": "0", "process_kernel_time": 2.0625, "process_mem_max_rss": "94584", "process_out_blocks": "0", "process_user_time": 1.765625}
[0m06:03:54.311726 [debug] [MainThread]: Command `dbt debug` failed at 06:03:54.311633 after 24.66 seconds
[0m06:03:54.312009 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m06:03:54.312271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f542717c620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f54271a76e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5427230fe0>]}
[0m06:03:54.312504 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:03:54.312707 [debug] [MainThread]: Flushing usage events
[0m06:04:13.741965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2ae3299d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2ace56960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2ade960f0>]}
[0m06:04:13.746126 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 06:04:13.746443 | 8d905fe7-dce9-443b-94ee-110c8bca8964 ==============================
[0m06:04:13.746443 [info ] [MainThread]: Running with dbt=1.9.2
[0m06:04:13.747821 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m06:04:13.771336 [info ] [MainThread]: dbt version: 1.9.2
[0m06:04:13.772952 [info ] [MainThread]: python version: 3.12.3
[0m06:04:13.774108 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m06:04:13.775251 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m06:04:13.944318 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:04:13.944818 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:04:13.945163 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:04:13.970565 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m06:04:13.972235 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m06:04:13.973459 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m06:04:13.974886 [info ] [MainThread]: adapter type: spark
[0m06:04:13.975969 [info ] [MainThread]: adapter version: 1.9.1
[0m06:04:14.060307 [info ] [MainThread]: Configuration:
[0m06:04:14.062320 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m06:04:14.063303 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m06:04:14.064168 [info ] [MainThread]: Required dependencies:
[0m06:04:14.065670 [debug] [MainThread]: Executing "git --help"
[0m06:04:14.084518 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m06:04:14.085360 [debug] [MainThread]: STDERR: "b''"
[0m06:04:14.085637 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m06:04:14.087617 [info ] [MainThread]: Connection:
[0m06:04:14.088575 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m06:04:14.089526 [info ] [MainThread]:   port: 10001
[0m06:04:14.090441 [info ] [MainThread]:   cluster: None
[0m06:04:14.091920 [info ] [MainThread]:   endpoint: None
[0m06:04:14.092915 [info ] [MainThread]:   schema: s3_database
[0m06:04:14.093663 [info ] [MainThread]:   organization: 0
[0m06:04:14.095012 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m06:04:14.251922 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m06:04:14.252289 [debug] [MainThread]: Using spark connection "debug"
[0m06:04:14.252538 [debug] [MainThread]: On debug: select 1 as id
[0m06:04:14.252755 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:04:35.479367 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m06:04:35.482318 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m06:04:35.483132 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m06:04:35.483941 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m06:04:35.486508 [info ] [MainThread]: [31m1 check failed:[0m
[0m06:04:35.488682 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m06:04:35.495948 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.813879, "process_in_blocks": "0", "process_kernel_time": 0.875, "process_mem_max_rss": "94564", "process_out_blocks": "0", "process_user_time": 1.25}
[0m06:04:35.496361 [debug] [MainThread]: Command `dbt debug` failed at 06:04:35.496272 after 21.81 seconds
[0m06:04:35.496630 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m06:04:35.496886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2abf4d2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2ab74f6e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2ab858fe0>]}
[0m06:04:35.497115 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:04:35.497385 [debug] [MainThread]: Flushing usage events
[0m06:04:52.052179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26472ec5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2647b431a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26475a5d60>]}
[0m06:04:52.056005 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 06:04:52.056317 | c0822bca-cd36-4936-839a-f876313fc0a9 ==============================
[0m06:04:52.056317 [info ] [MainThread]: Running with dbt=1.9.2
[0m06:04:52.057758 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m06:04:52.078971 [info ] [MainThread]: dbt version: 1.9.2
[0m06:04:52.080429 [info ] [MainThread]: python version: 3.12.3
[0m06:04:52.081435 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m06:04:52.082459 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m06:04:52.254496 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:04:52.254921 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:04:52.255211 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:04:52.277962 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m06:04:52.279926 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m06:04:52.281007 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m06:04:52.363572 [info ] [MainThread]: Configuration:
[0m06:04:52.365175 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m06:04:52.366314 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m06:04:52.367374 [info ] [MainThread]: Required dependencies:
[0m06:04:52.368396 [debug] [MainThread]: Executing "git --help"
[0m06:04:52.390583 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m06:04:52.391147 [debug] [MainThread]: STDERR: "b''"
[0m06:04:52.391462 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m06:04:52.392692 [info ] [MainThread]: Connection test skipped since no profile was found
[0m06:04:52.393498 [info ] [MainThread]: [31m1 check failed:[0m
[0m06:04:52.394523 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "dbt_spark_emr_app", target "dev" invalid: Runtime Error
        schema: s3_database 
        database: default 
    On Spark, database must be omitted or have the same value as schema.


[0m06:04:52.399824 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.4055753, "process_in_blocks": "0", "process_kernel_time": 0.6875, "process_mem_max_rss": "93780", "process_out_blocks": "0", "process_user_time": 1.265625}
[0m06:04:52.400920 [debug] [MainThread]: Command `dbt debug` failed at 06:04:52.400729 after 0.41 seconds
[0m06:04:52.401735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2648afe030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f264688dd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2646880b30>]}
[0m06:04:52.402026 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:04:52.402279 [debug] [MainThread]: Flushing usage events
[0m06:05:13.143975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e3f77e8a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e3f77dfa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e3f77dee0>]}
[0m06:05:13.147719 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 06:05:13.148029 | 4645eced-e56d-42c1-bf66-09f7a505a9fd ==============================
[0m06:05:13.148029 [info ] [MainThread]: Running with dbt=1.9.2
[0m06:05:13.149284 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:05:13.171915 [info ] [MainThread]: dbt version: 1.9.2
[0m06:05:13.173158 [info ] [MainThread]: python version: 3.12.3
[0m06:05:13.174084 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m06:05:13.175063 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m06:05:13.342247 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:05:13.342661 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:05:13.342934 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:05:13.369505 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m06:05:13.370865 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m06:05:13.371933 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m06:05:13.373765 [info ] [MainThread]: adapter type: spark
[0m06:05:13.374782 [info ] [MainThread]: adapter version: 1.9.1
[0m06:05:13.455038 [info ] [MainThread]: Configuration:
[0m06:05:13.456950 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m06:05:13.457918 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m06:05:13.458965 [info ] [MainThread]: Required dependencies:
[0m06:05:13.459957 [debug] [MainThread]: Executing "git --help"
[0m06:05:13.481688 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m06:05:13.482269 [debug] [MainThread]: STDERR: "b''"
[0m06:05:13.482561 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m06:05:13.483998 [info ] [MainThread]: Connection:
[0m06:05:13.484604 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m06:05:13.485357 [info ] [MainThread]:   port: 10001
[0m06:05:13.486247 [info ] [MainThread]:   cluster: None
[0m06:05:13.487409 [info ] [MainThread]:   endpoint: None
[0m06:05:13.488527 [info ] [MainThread]:   schema: s3_database
[0m06:05:13.489622 [info ] [MainThread]:   organization: 0
[0m06:05:13.490775 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m06:05:13.650837 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m06:05:13.651359 [debug] [MainThread]: Using spark connection "debug"
[0m06:05:13.651745 [debug] [MainThread]: On debug: select 1 as id
[0m06:05:13.652029 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:05:34.803576 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m06:05:34.806703 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m06:05:34.807586 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m06:05:34.808463 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m06:05:34.811299 [info ] [MainThread]: [31m1 check failed:[0m
[0m06:05:34.813561 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m06:05:34.819811 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.732052, "process_in_blocks": "0", "process_kernel_time": 0.96875, "process_mem_max_rss": "94540", "process_out_blocks": "0", "process_user_time": 1.09375}
[0m06:05:34.820284 [debug] [MainThread]: Command `dbt debug` failed at 06:05:34.820190 after 21.73 seconds
[0m06:05:34.820644 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m06:05:34.820905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e3f77e210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e3f869df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e3fd65100>]}
[0m06:05:34.821125 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:05:34.821317 [debug] [MainThread]: Flushing usage events
[0m06:05:47.994978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82f701d4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82f72d59a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82f701fd10>]}
[0m06:05:47.998819 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 06:05:47.999192 | 9172174c-dd5b-480f-8ef2-e57a5eea6b02 ==============================
[0m06:05:47.999192 [info ] [MainThread]: Running with dbt=1.9.2
[0m06:05:48.000526 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:05:48.022133 [info ] [MainThread]: dbt version: 1.9.2
[0m06:05:48.023394 [info ] [MainThread]: python version: 3.12.3
[0m06:05:48.024215 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m06:05:48.025328 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m06:05:48.221959 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:05:48.222669 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:05:48.222957 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:05:48.251605 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m06:05:48.252779 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m06:05:48.253772 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m06:05:48.255185 [info ] [MainThread]: adapter type: spark
[0m06:05:48.256260 [info ] [MainThread]: adapter version: 1.9.1
[0m06:05:48.342749 [info ] [MainThread]: Configuration:
[0m06:05:48.344167 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m06:05:48.345346 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m06:05:48.346521 [info ] [MainThread]: Required dependencies:
[0m06:05:48.347443 [debug] [MainThread]: Executing "git --help"
[0m06:05:48.368309 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m06:05:48.368834 [debug] [MainThread]: STDERR: "b''"
[0m06:05:48.369081 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m06:05:48.370491 [info ] [MainThread]: Connection:
[0m06:05:48.371496 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m06:05:48.372447 [info ] [MainThread]:   port: 10001
[0m06:05:48.373372 [info ] [MainThread]:   cluster: None
[0m06:05:48.374344 [info ] [MainThread]:   endpoint: None
[0m06:05:48.375267 [info ] [MainThread]:   schema: default
[0m06:05:48.375965 [info ] [MainThread]:   organization: 0
[0m06:05:48.377294 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m06:05:48.551974 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m06:05:48.552790 [debug] [MainThread]: Using spark connection "debug"
[0m06:05:48.553286 [debug] [MainThread]: On debug: select 1 as id
[0m06:05:48.553727 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:06:09.704267 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m06:06:09.706193 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m06:06:09.706749 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m06:06:09.707323 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m06:06:09.709256 [info ] [MainThread]: [31m1 check failed:[0m
[0m06:06:09.710665 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m06:06:09.718266 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.786875, "process_in_blocks": "0", "process_kernel_time": 1.0, "process_mem_max_rss": "94552", "process_out_blocks": "0", "process_user_time": 1.296875}
[0m06:06:09.718809 [debug] [MainThread]: Command `dbt debug` failed at 06:06:09.718700 after 21.79 seconds
[0m06:06:09.719136 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m06:06:09.719471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82f74a6c30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82f6739df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82f70d5760>]}
[0m06:06:09.719738 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:06:09.719969 [debug] [MainThread]: Flushing usage events
[0m06:06:23.811330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa22422df70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2248bd6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa22418e3f0>]}
[0m06:06:23.814951 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 06:06:23.815286 | 7777a2cb-6964-4780-836c-bcb358000a96 ==============================
[0m06:06:23.815286 [info ] [MainThread]: Running with dbt=1.9.2
[0m06:06:23.816338 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:06:23.836057 [info ] [MainThread]: dbt version: 1.9.2
[0m06:06:23.837112 [info ] [MainThread]: python version: 3.12.3
[0m06:06:23.838033 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m06:06:23.838852 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m06:06:24.006294 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:06:24.006700 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:06:24.006987 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:06:24.029771 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m06:06:24.031336 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m06:06:24.032393 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m06:06:24.115468 [info ] [MainThread]: Configuration:
[0m06:06:24.119455 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m06:06:24.120663 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m06:06:24.121590 [info ] [MainThread]: Required dependencies:
[0m06:06:24.122683 [debug] [MainThread]: Executing "git --help"
[0m06:06:24.142940 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m06:06:24.143962 [debug] [MainThread]: STDERR: "b''"
[0m06:06:24.144448 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m06:06:24.145959 [info ] [MainThread]: Connection test skipped since no profile was found
[0m06:06:24.147059 [info ] [MainThread]: [31m1 check failed:[0m
[0m06:06:24.148244 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "dbt_spark_emr_app", target "dev" invalid: Runtime Error
    Must specify `schema` in profile


[0m06:06:24.151995 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.3992891, "process_in_blocks": "0", "process_kernel_time": 0.671875, "process_mem_max_rss": "93744", "process_out_blocks": "0", "process_user_time": 1.28125}
[0m06:06:24.152421 [debug] [MainThread]: Command `dbt debug` failed at 06:06:24.152329 after 0.40 seconds
[0m06:06:24.152722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa22422df70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa224252180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa223d4fbf0>]}
[0m06:06:24.153015 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:06:24.153258 [debug] [MainThread]: Flushing usage events
[0m06:06:45.600363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a6157e5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a5f9c1670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a5f9c0c20>]}
[0m06:06:45.604481 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 06:06:45.604802 | 149b13c9-2c3d-48c5-a04a-4065a01b5c41 ==============================
[0m06:06:45.604802 [info ] [MainThread]: Running with dbt=1.9.2
[0m06:06:45.606097 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m06:06:45.624377 [info ] [MainThread]: dbt version: 1.9.2
[0m06:06:45.626077 [info ] [MainThread]: python version: 3.12.3
[0m06:06:45.627089 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m06:06:45.628219 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m06:06:45.796168 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:06:45.796574 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:06:45.796847 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:06:45.822238 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m06:06:45.823667 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m06:06:45.824864 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m06:06:45.826186 [info ] [MainThread]: adapter type: spark
[0m06:06:45.827213 [info ] [MainThread]: adapter version: 1.9.1
[0m06:06:45.910666 [info ] [MainThread]: Configuration:
[0m06:06:45.912233 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m06:06:45.913354 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m06:06:45.914426 [info ] [MainThread]: Required dependencies:
[0m06:06:45.915481 [debug] [MainThread]: Executing "git --help"
[0m06:06:45.934765 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m06:06:45.935386 [debug] [MainThread]: STDERR: "b''"
[0m06:06:45.935843 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m06:06:45.937382 [info ] [MainThread]: Connection:
[0m06:06:45.938233 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m06:06:45.939318 [info ] [MainThread]:   port: 10001
[0m06:06:45.940415 [info ] [MainThread]:   cluster: None
[0m06:06:45.941463 [info ] [MainThread]:   endpoint: None
[0m06:06:45.942306 [info ] [MainThread]:   schema: default
[0m06:06:45.943294 [info ] [MainThread]:   organization: 0
[0m06:06:45.944649 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m06:06:46.102668 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m06:06:46.103049 [debug] [MainThread]: Using spark connection "debug"
[0m06:06:46.103379 [debug] [MainThread]: On debug: select 1 as id
[0m06:06:46.103638 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:07:07.373272 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m06:07:07.376126 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m06:07:07.376942 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m06:07:07.377755 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m06:07:07.380279 [info ] [MainThread]: [31m1 check failed:[0m
[0m06:07:07.382566 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m06:07:07.389492 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.844933, "process_in_blocks": "0", "process_kernel_time": 0.578125, "process_mem_max_rss": "94572", "process_out_blocks": "0", "process_user_time": 1.484375}
[0m06:07:07.389995 [debug] [MainThread]: Command `dbt debug` failed at 06:07:07.389904 after 21.85 seconds
[0m06:07:07.390370 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m06:07:07.390742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a62af3140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a5ed429c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a5ed28fe0>]}
[0m06:07:07.391001 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:07:07.391500 [debug] [MainThread]: Flushing usage events
[0m06:07:48.260994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa994d9e270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9949afbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa994d9f9b0>]}
[0m06:07:48.265077 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 06:07:48.265391 | ed690944-7bde-42bf-b161-9cc09f197d39 ==============================
[0m06:07:48.265391 [info ] [MainThread]: Running with dbt=1.9.2
[0m06:07:48.266384 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:07:48.291020 [info ] [MainThread]: dbt version: 1.9.2
[0m06:07:48.292256 [info ] [MainThread]: python version: 3.12.3
[0m06:07:48.293036 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m06:07:48.294544 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m06:07:48.469478 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:07:48.469885 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:07:48.470167 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:07:48.495513 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m06:07:48.497350 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m06:07:48.498596 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m06:07:48.500017 [info ] [MainThread]: adapter type: spark
[0m06:07:48.501147 [info ] [MainThread]: adapter version: 1.9.1
[0m06:07:48.591143 [info ] [MainThread]: Configuration:
[0m06:07:48.592641 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m06:07:48.593810 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m06:07:48.594898 [info ] [MainThread]: Required dependencies:
[0m06:07:48.596118 [debug] [MainThread]: Executing "git --help"
[0m06:07:48.615692 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m06:07:48.616183 [debug] [MainThread]: STDERR: "b''"
[0m06:07:48.616533 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m06:07:48.617921 [info ] [MainThread]: Connection:
[0m06:07:48.618787 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m06:07:48.619615 [info ] [MainThread]:   port: 10001
[0m06:07:48.620546 [info ] [MainThread]:   cluster: None
[0m06:07:48.621408 [info ] [MainThread]:   endpoint: None
[0m06:07:48.622058 [info ] [MainThread]:   schema: default
[0m06:07:48.622959 [info ] [MainThread]:   organization: 0
[0m06:07:48.624406 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m06:07:48.807644 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m06:07:48.808047 [debug] [MainThread]: Using spark connection "debug"
[0m06:07:48.808303 [debug] [MainThread]: On debug: select 1 as id
[0m06:07:48.808526 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:08:09.958679 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m06:08:09.961839 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m06:08:09.962720 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m06:08:09.963597 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m06:08:09.966359 [info ] [MainThread]: [31m1 check failed:[0m
[0m06:08:09.968631 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m06:08:09.974787 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.768805, "process_in_blocks": "0", "process_kernel_time": 0.734375, "process_mem_max_rss": "94568", "process_out_blocks": "0", "process_user_time": 1.515625}
[0m06:08:09.975251 [debug] [MainThread]: Command `dbt debug` failed at 06:08:09.975150 after 21.77 seconds
[0m06:08:09.975555 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m06:08:09.975839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa994de30e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9942cf200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa993ab3a10>]}
[0m06:08:09.976095 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:08:09.976323 [debug] [MainThread]: Flushing usage events
[0m12:04:18.769990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24da07c470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24dbab6150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24da696f30>]}
[0m12:04:18.776315 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:04:18.776651 | 2da0c3c5-fb6f-4a54-8547-433e5e4d2311 ==============================
[0m12:04:18.776651 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:04:18.778017 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:04:18.816548 [info ] [MainThread]: dbt version: 1.9.2
[0m12:04:18.817977 [info ] [MainThread]: python version: 3.12.3
[0m12:04:18.818893 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m12:04:18.819707 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m12:04:19.059712 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:04:19.060253 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:04:19.060645 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:04:19.090539 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m12:04:19.091946 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m12:04:19.092971 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m12:04:19.094326 [info ] [MainThread]: adapter type: spark
[0m12:04:19.095200 [info ] [MainThread]: adapter version: 1.9.1
[0m12:04:19.178262 [info ] [MainThread]: Configuration:
[0m12:04:19.179879 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:04:19.180969 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:04:19.181948 [info ] [MainThread]: Required dependencies:
[0m12:04:19.182852 [debug] [MainThread]: Executing "git --help"
[0m12:04:19.725129 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:04:19.725792 [debug] [MainThread]: STDERR: "b''"
[0m12:04:19.726163 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:04:19.727610 [info ] [MainThread]: Connection:
[0m12:04:19.728528 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m12:04:19.729493 [info ] [MainThread]:   port: 10001
[0m12:04:19.730451 [info ] [MainThread]:   cluster: None
[0m12:04:19.731381 [info ] [MainThread]:   endpoint: None
[0m12:04:19.732265 [info ] [MainThread]:   schema: default
[0m12:04:19.732934 [info ] [MainThread]:   organization: 0
[0m12:04:19.734373 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:04:19.916862 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:04:19.917337 [debug] [MainThread]: Using spark connection "debug"
[0m12:04:19.917688 [debug] [MainThread]: On debug: select 1 as id
[0m12:04:19.918000 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:04:41.077117 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m12:04:41.080127 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:04:41.081003 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:04:41.081899 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:04:41.084605 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:04:41.086790 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:04:41.094868 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 22.387638, "process_in_blocks": "0", "process_kernel_time": 1.296875, "process_mem_max_rss": "94568", "process_out_blocks": "0", "process_user_time": 1.234375}
[0m12:04:41.095381 [debug] [MainThread]: Command `dbt debug` failed at 12:04:41.095267 after 22.39 seconds
[0m12:04:41.095727 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:04:41.096043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24d9e2f380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24d931d7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f24da07f4a0>]}
[0m12:04:41.096278 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:04:41.096486 [debug] [MainThread]: Flushing usage events
[0m12:05:06.307296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9410549c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f941030c140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9410299940>]}
[0m12:05:06.311009 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:05:06.311350 | 40ff60a3-b848-48b6-8ad2-0a67464267e6 ==============================
[0m12:05:06.311350 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:05:06.312411 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt debug', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:05:06.336021 [info ] [MainThread]: dbt version: 1.9.2
[0m12:05:06.337442 [info ] [MainThread]: python version: 3.12.3
[0m12:05:06.338395 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m12:05:06.339206 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m12:05:06.506142 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:05:06.506557 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:05:06.506841 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:05:06.531918 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m12:05:06.533622 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m12:05:06.534921 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m12:05:06.536373 [info ] [MainThread]: adapter type: spark
[0m12:05:06.537441 [info ] [MainThread]: adapter version: 1.9.1
[0m12:05:06.616306 [info ] [MainThread]: Configuration:
[0m12:05:06.617979 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:05:06.619089 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:05:06.620078 [info ] [MainThread]: Required dependencies:
[0m12:05:06.621062 [debug] [MainThread]: Executing "git --help"
[0m12:05:06.640000 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:05:06.640616 [debug] [MainThread]: STDERR: "b''"
[0m12:05:06.640958 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:05:06.642261 [info ] [MainThread]: Connection:
[0m12:05:06.643169 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m12:05:06.644045 [info ] [MainThread]:   port: 10001
[0m12:05:06.644915 [info ] [MainThread]:   cluster: None
[0m12:05:06.645789 [info ] [MainThread]:   endpoint: None
[0m12:05:06.646595 [info ] [MainThread]:   schema: default
[0m12:05:06.647491 [info ] [MainThread]:   organization: 0
[0m12:05:06.648708 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:05:06.801636 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:05:06.802003 [debug] [MainThread]: Using spark connection "debug"
[0m12:05:06.802259 [debug] [MainThread]: On debug: select 1 as id
[0m12:05:06.802491 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:05:27.989888 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m12:05:27.992995 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:05:27.993860 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:05:27.994739 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:05:27.997555 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:05:27.999613 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:05:28.005495 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.75311, "process_in_blocks": "0", "process_kernel_time": 0.84375, "process_mem_max_rss": "94552", "process_out_blocks": "0", "process_user_time": 1.25}
[0m12:05:28.005917 [debug] [MainThread]: Command `dbt debug` failed at 12:05:28.005827 after 21.75 seconds
[0m12:05:28.006218 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:05:28.006480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94104cddc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9410c3f020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f940f7ded80>]}
[0m12:05:28.006727 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:05:28.006927 [debug] [MainThread]: Flushing usage events
[0m12:21:27.586698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f91e20ba4b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f91e25adcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f91e23ee330>]}
[0m12:21:27.590445 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:21:27.590763 | 71a04b32-6f2b-4703-a91c-c34ea1159254 ==============================
[0m12:21:27.590763 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:21:27.592102 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:21:27.612322 [info ] [MainThread]: dbt version: 1.9.2
[0m12:21:27.613469 [info ] [MainThread]: python version: 3.12.3
[0m12:21:27.614492 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m12:21:27.615499 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m12:21:27.792643 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:21:27.793055 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:21:27.793339 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:21:27.818580 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m12:21:27.820295 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m12:21:27.821514 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m12:21:27.822779 [info ] [MainThread]: adapter type: spark
[0m12:21:27.823840 [info ] [MainThread]: adapter version: 1.9.1
[0m12:21:27.905201 [info ] [MainThread]: Configuration:
[0m12:21:27.906794 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:21:27.907865 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:21:27.909184 [info ] [MainThread]: Required dependencies:
[0m12:21:27.910273 [debug] [MainThread]: Executing "git --help"
[0m12:21:27.930666 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:21:27.931205 [debug] [MainThread]: STDERR: "b''"
[0m12:21:27.931587 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:21:27.933148 [info ] [MainThread]: Connection:
[0m12:21:27.934280 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m12:21:27.935329 [info ] [MainThread]:   port: 10001
[0m12:21:27.936289 [info ] [MainThread]:   cluster: None
[0m12:21:27.937017 [info ] [MainThread]:   endpoint: None
[0m12:21:27.938099 [info ] [MainThread]:   schema: default
[0m12:21:27.939667 [info ] [MainThread]:   organization: 0
[0m12:21:27.940868 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:21:28.111635 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:21:28.112026 [debug] [MainThread]: Using spark connection "debug"
[0m12:21:28.112271 [debug] [MainThread]: On debug: select 1 as id
[0m12:21:28.112487 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:21:49.378515 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m12:21:49.380460 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:21:49.381747 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:21:49.383114 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:21:49.384818 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:21:49.385568 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:21:49.389637 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.860163, "process_in_blocks": "0", "process_kernel_time": 0.828125, "process_mem_max_rss": "94668", "process_out_blocks": "0", "process_user_time": 1.3125}
[0m12:21:49.390317 [debug] [MainThread]: Command `dbt debug` failed at 12:21:49.390177 after 21.86 seconds
[0m12:21:49.390767 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:21:49.391198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f91e38b71d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f91e111e870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f91e12681a0>]}
[0m12:21:49.391583 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:21:49.391920 [debug] [MainThread]: Flushing usage events
[0m12:23:53.655572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8dda31e8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8dd8db8740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8dd86fdd60>]}
[0m12:23:53.659144 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:23:53.659459 | 34f2b36a-9e1e-4488-a616-63acec1cda4b ==============================
[0m12:23:53.659459 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:23:53.660802 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:23:53.678361 [info ] [MainThread]: dbt version: 1.9.2
[0m12:23:53.679893 [info ] [MainThread]: python version: 3.12.3
[0m12:23:53.681065 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m12:23:53.683438 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m12:23:53.852066 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:23:53.852502 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:23:53.852791 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:23:53.878172 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m12:23:53.879734 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m12:23:53.881164 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m12:23:53.882846 [info ] [MainThread]: adapter type: spark
[0m12:23:53.883875 [info ] [MainThread]: adapter version: 1.9.1
[0m12:23:53.971714 [info ] [MainThread]: Configuration:
[0m12:23:53.973387 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:23:53.974517 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:23:53.975628 [info ] [MainThread]: Required dependencies:
[0m12:23:53.976746 [debug] [MainThread]: Executing "git --help"
[0m12:23:53.995216 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:23:53.995682 [debug] [MainThread]: STDERR: "b''"
[0m12:23:53.995939 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:23:53.996946 [info ] [MainThread]: Connection:
[0m12:23:53.998704 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m12:23:53.999668 [info ] [MainThread]:   port: 10001
[0m12:23:54.000697 [info ] [MainThread]:   cluster: None
[0m12:23:54.001475 [info ] [MainThread]:   endpoint: None
[0m12:23:54.002335 [info ] [MainThread]:   schema: default
[0m12:23:54.002984 [info ] [MainThread]:   organization: 0
[0m12:23:54.004208 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:23:54.164894 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:23:54.165261 [debug] [MainThread]: Using spark connection "debug"
[0m12:23:54.165513 [debug] [MainThread]: On debug: select 1 as id
[0m12:23:54.165730 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:24:15.286104 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m12:24:15.288908 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:24:15.289718 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:24:15.290537 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:24:15.293134 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:24:15.294956 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:24:15.302664 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.702005, "process_in_blocks": "0", "process_kernel_time": 0.828125, "process_mem_max_rss": "94668", "process_out_blocks": "0", "process_user_time": 1.25}
[0m12:24:15.303244 [debug] [MainThread]: Command `dbt debug` failed at 12:24:15.303125 after 21.70 seconds
[0m12:24:15.303682 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:24:15.304032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8dd85a3800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8dd828d520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8dd7ef6090>]}
[0m12:24:15.304262 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:24:15.304493 [debug] [MainThread]: Flushing usage events
[0m12:42:35.964035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01de17f140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01de8c5a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01de17e810>]}
[0m12:42:35.968709 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:42:35.969050 | 46f9e7d6-a3fb-4df7-b5d5-478c8688aa39 ==============================
[0m12:42:35.969050 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:42:35.970093 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:42:36.015893 [info ] [MainThread]: dbt version: 1.9.2
[0m12:42:36.019155 [info ] [MainThread]: python version: 3.12.3
[0m12:42:36.020233 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m12:42:36.021026 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m12:42:36.227130 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:42:36.227633 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:42:36.227992 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:42:36.257706 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m12:42:36.259835 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m12:42:36.260849 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m12:42:36.262235 [info ] [MainThread]: adapter type: spark
[0m12:42:36.263036 [info ] [MainThread]: adapter version: 1.9.1
[0m12:42:36.350616 [info ] [MainThread]: Configuration:
[0m12:42:36.351946 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:42:36.353281 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:42:36.354402 [info ] [MainThread]: Required dependencies:
[0m12:42:36.355302 [debug] [MainThread]: Executing "git --help"
[0m12:42:36.376794 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:42:36.377418 [debug] [MainThread]: STDERR: "b''"
[0m12:42:36.377815 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:42:36.379537 [info ] [MainThread]: Connection:
[0m12:42:36.380805 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m12:42:36.382022 [info ] [MainThread]:   port: 10001
[0m12:42:36.382972 [info ] [MainThread]:   cluster: None
[0m12:42:36.384018 [info ] [MainThread]:   endpoint: None
[0m12:42:36.385076 [info ] [MainThread]:   schema: default
[0m12:42:36.385990 [info ] [MainThread]:   organization: 0
[0m12:42:36.387043 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:42:36.567097 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:42:36.567510 [debug] [MainThread]: Using spark connection "debug"
[0m12:42:36.567779 [debug] [MainThread]: On debug: select 1 as id
[0m12:42:36.568002 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:42:57.762952 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m12:42:57.765106 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:42:57.765640 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:42:57.766060 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:42:57.767595 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:42:57.768748 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:42:57.773878 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.875692, "process_in_blocks": "0", "process_kernel_time": 1.171875, "process_mem_max_rss": "94680", "process_out_blocks": "0", "process_user_time": 1.3125}
[0m12:42:57.774366 [debug] [MainThread]: Command `dbt debug` failed at 12:42:57.774250 after 21.88 seconds
[0m12:42:57.774758 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:42:57.775121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01de9de810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01ddf29dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01dddc15e0>]}
[0m12:42:57.775709 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:42:57.776276 [debug] [MainThread]: Flushing usage events
[0m12:46:58.203885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a30221b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a47c6240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a2d4fdd0>]}
[0m12:46:58.208007 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:46:58.208312 | 2a6182ff-6afe-454b-b743-e4e7789470d2 ==============================
[0m12:46:58.208312 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:46:58.209603 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:46:58.229529 [info ] [MainThread]: dbt version: 1.9.2
[0m12:46:58.230985 [info ] [MainThread]: python version: 3.12.3
[0m12:46:58.232112 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m12:46:58.233490 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m12:46:58.437090 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:46:58.437804 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:46:58.438376 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:46:58.485816 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m12:46:58.487536 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m12:46:58.488761 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m12:46:58.490083 [info ] [MainThread]: adapter type: spark
[0m12:46:58.491190 [info ] [MainThread]: adapter version: 1.9.1
[0m12:46:58.586832 [info ] [MainThread]: Configuration:
[0m12:46:58.588219 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:46:58.589098 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:46:58.589812 [info ] [MainThread]: Required dependencies:
[0m12:46:58.590597 [debug] [MainThread]: Executing "git --help"
[0m12:46:58.608990 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:46:58.609528 [debug] [MainThread]: STDERR: "b''"
[0m12:46:58.609801 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:46:58.611098 [info ] [MainThread]: Connection:
[0m12:46:58.612239 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m12:46:58.613282 [info ] [MainThread]:   port: 10000
[0m12:46:58.614664 [info ] [MainThread]:   cluster: None
[0m12:46:58.616170 [info ] [MainThread]:   endpoint: None
[0m12:46:58.617008 [info ] [MainThread]:   schema: default
[0m12:46:58.618045 [info ] [MainThread]:   organization: 0
[0m12:46:58.619532 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:46:58.789463 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:46:58.789852 [debug] [MainThread]: Using spark connection "debug"
[0m12:46:58.790146 [debug] [MainThread]: On debug: select 1 as id
[0m12:46:58.790400 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:47:19.935315 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10000)]
[0m12:47:19.938503 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:47:19.939492 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:47:19.940552 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:47:19.942549 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:47:19.943747 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:47:19.949217 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.806686, "process_in_blocks": "0", "process_kernel_time": 0.984375, "process_mem_max_rss": "94672", "process_out_blocks": "0", "process_user_time": 1.375}
[0m12:47:19.949989 [debug] [MainThread]: Command `dbt debug` failed at 12:47:19.949769 after 21.81 seconds
[0m12:47:19.950426 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:47:19.950976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a2d4e870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a32964e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a3085a00>]}
[0m12:47:19.951294 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:47:19.951647 [debug] [MainThread]: Flushing usage events
[0m13:00:47.985792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4551bcef90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4551bce660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4551bce600>]}
[0m13:00:47.990170 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 13:00:47.990741 | d2c665ea-ebbf-4de0-8c99-0c642ccc92d9 ==============================
[0m13:00:47.990741 [info ] [MainThread]: Running with dbt=1.9.2
[0m13:00:47.991838 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:00:48.015416 [info ] [MainThread]: dbt version: 1.9.2
[0m13:00:48.016158 [info ] [MainThread]: python version: 3.12.3
[0m13:00:48.016723 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m13:00:48.017151 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m13:00:48.189959 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:00:48.190446 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:00:48.190743 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:00:48.216050 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m13:00:48.217540 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m13:00:48.218217 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m13:00:48.218820 [info ] [MainThread]: adapter type: spark
[0m13:00:48.219376 [info ] [MainThread]: adapter version: 1.9.1
[0m13:00:48.303105 [info ] [MainThread]: Configuration:
[0m13:00:48.304577 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:00:48.305313 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:00:48.306010 [info ] [MainThread]: Required dependencies:
[0m13:00:48.306967 [debug] [MainThread]: Executing "git --help"
[0m13:00:48.327510 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:00:48.328044 [debug] [MainThread]: STDERR: "b''"
[0m13:00:48.328345 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:00:48.329649 [info ] [MainThread]: Connection:
[0m13:00:48.331248 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m13:00:48.332276 [info ] [MainThread]:   port: 10001
[0m13:00:48.333296 [info ] [MainThread]:   cluster: None
[0m13:00:48.334231 [info ] [MainThread]:   endpoint: None
[0m13:00:48.335083 [info ] [MainThread]:   schema: default
[0m13:00:48.336765 [info ] [MainThread]:   organization: 0
[0m13:00:48.337919 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m13:00:48.499494 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m13:00:48.499895 [debug] [MainThread]: Using spark connection "debug"
[0m13:00:48.500220 [debug] [MainThread]: On debug: select 1 as id
[0m13:00:48.500573 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:01:10.084415 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m13:01:10.087568 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m13:01:10.088450 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m13:01:10.089280 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:01:10.091693 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:01:10.093633 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:01:10.101744 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 22.1735, "process_in_blocks": "0", "process_kernel_time": 0.90625, "process_mem_max_rss": "94656", "process_out_blocks": "0", "process_user_time": 1.25}
[0m13:01:10.102187 [debug] [MainThread]: Command `dbt debug` failed at 13:01:10.102091 after 22.17 seconds
[0m13:01:10.102541 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:01:10.102875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45523b4fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4551bccf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45513d3a10>]}
[0m13:01:10.103112 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:01:10.103345 [debug] [MainThread]: Flushing usage events
[0m13:01:28.581372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa30b9e5f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa30b3903b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa30b392c00>]}
[0m13:01:28.585211 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 13:01:28.585644 | c1a49c21-d3ca-47f4-822d-926830eae55e ==============================
[0m13:01:28.585644 [info ] [MainThread]: Running with dbt=1.9.2
[0m13:01:28.586964 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:01:28.608765 [info ] [MainThread]: dbt version: 1.9.2
[0m13:01:28.609842 [info ] [MainThread]: python version: 3.12.3
[0m13:01:28.610852 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m13:01:28.611851 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m13:01:28.783068 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:01:28.783480 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:01:28.783761 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:01:28.810574 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m13:01:28.812404 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m13:01:28.813653 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m13:01:28.815161 [info ] [MainThread]: adapter type: spark
[0m13:01:28.816187 [info ] [MainThread]: adapter version: 1.9.1
[0m13:01:28.897649 [info ] [MainThread]: Configuration:
[0m13:01:28.899331 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:01:28.900465 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:01:28.901600 [info ] [MainThread]: Required dependencies:
[0m13:01:28.902619 [debug] [MainThread]: Executing "git --help"
[0m13:01:28.921642 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:01:28.922434 [debug] [MainThread]: STDERR: "b''"
[0m13:01:28.922852 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:01:28.924004 [info ] [MainThread]: Connection:
[0m13:01:28.924938 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m13:01:28.925763 [info ] [MainThread]:   port: 10001
[0m13:01:28.926462 [info ] [MainThread]:   cluster: None
[0m13:01:28.927430 [info ] [MainThread]:   endpoint: None
[0m13:01:28.928410 [info ] [MainThread]:   schema: default
[0m13:01:28.929323 [info ] [MainThread]:   organization: 0
[0m13:01:28.930474 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m13:01:29.093331 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m13:01:29.093697 [debug] [MainThread]: Using spark connection "debug"
[0m13:01:29.093954 [debug] [MainThread]: On debug: select 1 as id
[0m13:01:29.094174 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:01:50.210300 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m13:01:50.213025 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m13:01:50.213836 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m13:01:50.214664 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:01:50.217192 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:01:50.219486 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:01:50.226477 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.704231, "process_in_blocks": "0", "process_kernel_time": 0.765625, "process_mem_max_rss": "94564", "process_out_blocks": "0", "process_user_time": 1.34375}
[0m13:01:50.226921 [debug] [MainThread]: Command `dbt debug` failed at 13:01:50.226833 after 21.70 seconds
[0m13:01:50.227203 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:01:50.227457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa30e4f3140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa30a22a060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa30a3dcfb0>]}
[0m13:01:50.227700 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:01:50.227896 [debug] [MainThread]: Flushing usage events
[0m13:02:39.527928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0222fe4d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0221746480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0221e69dc0>]}
[0m13:02:39.533683 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 13:02:39.534111 | 09d10d4a-8302-4b58-b935-ddf0fdb8981c ==============================
[0m13:02:39.534111 [info ] [MainThread]: Running with dbt=1.9.2
[0m13:02:39.535589 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:02:39.555115 [info ] [MainThread]: dbt version: 1.9.2
[0m13:02:39.556259 [info ] [MainThread]: python version: 3.12.3
[0m13:02:39.557563 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m13:02:39.559109 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m13:02:39.733441 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:02:39.733844 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:02:39.734128 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:02:39.760659 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m13:02:39.762573 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m13:02:39.763794 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m13:02:39.765253 [info ] [MainThread]: adapter type: spark
[0m13:02:39.766445 [info ] [MainThread]: adapter version: 1.9.1
[0m13:02:39.847958 [info ] [MainThread]: Configuration:
[0m13:02:39.849580 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:02:39.850704 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:02:39.851776 [info ] [MainThread]: Required dependencies:
[0m13:02:39.853007 [debug] [MainThread]: Executing "git --help"
[0m13:02:39.872786 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:02:39.873367 [debug] [MainThread]: STDERR: "b''"
[0m13:02:39.873699 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:02:39.874850 [info ] [MainThread]: Connection:
[0m13:02:39.875633 [info ] [MainThread]:   host: 10.0.1.231
[0m13:02:39.876575 [info ] [MainThread]:   port: 10001
[0m13:02:39.877682 [info ] [MainThread]:   cluster: None
[0m13:02:39.878597 [info ] [MainThread]:   endpoint: None
[0m13:02:39.879547 [info ] [MainThread]:   schema: default
[0m13:02:39.880556 [info ] [MainThread]:   organization: 0
[0m13:02:39.881855 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m13:02:40.049773 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m13:02:40.050157 [debug] [MainThread]: Using spark connection "debug"
[0m13:02:40.050413 [debug] [MainThread]: On debug: select 1 as id
[0m13:02:40.050631 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:03:01.094576 [error] [MainThread]: Could not connect to any of [('10.0.1.231', 10001)]
[0m13:03:01.097846 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m13:03:01.098732 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m13:03:01.099626 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:03:01.102295 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:03:01.104787 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:03:01.113564 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.646519, "process_in_blocks": "0", "process_kernel_time": 0.6875, "process_mem_max_rss": "94464", "process_out_blocks": "0", "process_user_time": 1.4375}
[0m13:03:01.114112 [debug] [MainThread]: Command `dbt debug` failed at 13:03:01.113993 after 21.65 seconds
[0m13:03:01.114477 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:03:01.114831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0222fe4d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f022098a9c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02211d3da0>]}
[0m13:03:01.115163 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:03:01.115433 [debug] [MainThread]: Flushing usage events
[0m13:14:59.088405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23c1059040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23bfe7cf20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23bfee2ba0>]}
[0m13:14:59.092208 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 13:14:59.092583 | f4557c6f-9dec-4a4d-be00-af021d396bde ==============================
[0m13:14:59.092583 [info ] [MainThread]: Running with dbt=1.9.2
[0m13:14:59.093929 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:14:59.112609 [info ] [MainThread]: dbt version: 1.9.2
[0m13:14:59.113817 [info ] [MainThread]: python version: 3.12.3
[0m13:14:59.114910 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m13:14:59.116010 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m13:14:59.296023 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:14:59.296454 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:14:59.296739 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:14:59.324724 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m13:14:59.326316 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m13:14:59.327183 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m13:14:59.328391 [info ] [MainThread]: adapter type: spark
[0m13:14:59.329371 [info ] [MainThread]: adapter version: 1.9.1
[0m13:14:59.411724 [info ] [MainThread]: Configuration:
[0m13:14:59.413268 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:14:59.414221 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:14:59.415018 [info ] [MainThread]: Required dependencies:
[0m13:14:59.415895 [debug] [MainThread]: Executing "git --help"
[0m13:14:59.434484 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:14:59.435488 [debug] [MainThread]: STDERR: "b''"
[0m13:14:59.435925 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:14:59.437413 [info ] [MainThread]: Connection:
[0m13:14:59.438594 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m13:14:59.439535 [info ] [MainThread]:   port: 10001
[0m13:14:59.440437 [info ] [MainThread]:   cluster: None
[0m13:14:59.441282 [info ] [MainThread]:   endpoint: None
[0m13:14:59.442279 [info ] [MainThread]:   schema: default
[0m13:14:59.443405 [info ] [MainThread]:   organization: 0
[0m13:14:59.444743 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m13:14:59.610492 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m13:14:59.610861 [debug] [MainThread]: Using spark connection "debug"
[0m13:14:59.611145 [debug] [MainThread]: On debug: select 1 as id
[0m13:14:59.611368 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:15:20.762183 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m13:15:20.765131 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m13:15:20.765949 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m13:15:20.766779 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:15:20.769382 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:15:20.771450 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:15:20.779408 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.748922, "process_in_blocks": "0", "process_kernel_time": 0.859375, "process_mem_max_rss": "94564", "process_out_blocks": "0", "process_user_time": 1.328125}
[0m13:15:20.780006 [debug] [MainThread]: Command `dbt debug` failed at 13:15:20.779875 after 21.75 seconds
[0m13:15:20.780457 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:15:20.780838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23c0270380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23c067ad20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23bfc582f0>]}
[0m13:15:20.781218 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:15:20.781536 [debug] [MainThread]: Flushing usage events
[0m13:15:29.307818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7372b47f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7372c10fe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7372b46930>]}
[0m13:15:29.313132 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 13:15:29.313565 | ec7aa618-7ef8-4dcd-8ccc-192b07a0f220 ==============================
[0m13:15:29.313565 [info ] [MainThread]: Running with dbt=1.9.2
[0m13:15:29.314966 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m13:15:29.336813 [info ] [MainThread]: dbt version: 1.9.2
[0m13:15:29.338255 [info ] [MainThread]: python version: 3.12.3
[0m13:15:29.339418 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m13:15:29.340584 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m13:15:29.507304 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:15:29.507709 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:15:29.507997 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:15:29.532962 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m13:15:29.534711 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m13:15:29.535765 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m13:15:29.537108 [info ] [MainThread]: adapter type: spark
[0m13:15:29.538217 [info ] [MainThread]: adapter version: 1.9.1
[0m13:15:29.615732 [info ] [MainThread]: Configuration:
[0m13:15:29.617043 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:15:29.618074 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:15:29.619175 [info ] [MainThread]: Required dependencies:
[0m13:15:29.620220 [debug] [MainThread]: Executing "git --help"
[0m13:15:29.642135 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:15:29.642821 [debug] [MainThread]: STDERR: "b''"
[0m13:15:29.643094 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:15:29.644456 [info ] [MainThread]: Connection:
[0m13:15:29.645438 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m13:15:29.646442 [info ] [MainThread]:   port: 10000
[0m13:15:29.647437 [info ] [MainThread]:   cluster: None
[0m13:15:29.648392 [info ] [MainThread]:   endpoint: None
[0m13:15:29.649319 [info ] [MainThread]:   schema: default
[0m13:15:29.650461 [info ] [MainThread]:   organization: 0
[0m13:15:29.651993 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m13:15:29.826017 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m13:15:29.826391 [debug] [MainThread]: Using spark connection "debug"
[0m13:15:29.826649 [debug] [MainThread]: On debug: select 1 as id
[0m13:15:29.826868 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:15:50.959540 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10000)]
[0m13:15:50.962363 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m13:15:50.963329 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m13:15:50.964061 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:15:50.965937 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:15:50.967290 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:15:50.974099 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 21.721148, "process_in_blocks": "0", "process_kernel_time": 0.84375, "process_mem_max_rss": "94560", "process_out_blocks": "0", "process_user_time": 1.296875}
[0m13:15:50.975023 [debug] [MainThread]: Command `dbt debug` failed at 13:15:50.974771 after 21.72 seconds
[0m13:15:50.975487 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:15:50.975856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7372edac00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f737235f6e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7372628fe0>]}
[0m13:15:50.976194 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:15:50.976474 [debug] [MainThread]: Flushing usage events
[0m11:42:48.494876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ef1ae9d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ef247ee70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ef247dbb0>]}
[0m11:42:48.501484 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:42:48.501824 | a6803180-edd0-41a4-9861-4ab80e51722a ==============================
[0m11:42:48.501824 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:42:48.503019 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:42:48.527853 [info ] [MainThread]: dbt version: 1.9.2
[0m11:42:48.529152 [info ] [MainThread]: python version: 3.12.3
[0m11:42:48.530246 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m11:42:48.531344 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m11:42:48.703226 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:42:48.703630 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:42:48.703904 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:42:48.728855 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m11:42:48.730773 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m11:42:48.732186 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m11:42:48.733650 [info ] [MainThread]: adapter type: spark
[0m11:42:48.734692 [info ] [MainThread]: adapter version: 1.9.1
[0m11:42:48.813398 [info ] [MainThread]: Configuration:
[0m11:42:48.815161 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:42:48.816313 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:42:48.817315 [info ] [MainThread]: Required dependencies:
[0m11:42:48.818316 [debug] [MainThread]: Executing "git --help"
[0m11:42:48.839833 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:42:48.840629 [debug] [MainThread]: STDERR: "b''"
[0m11:42:48.840901 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:42:48.842245 [info ] [MainThread]: Connection:
[0m11:42:48.843188 [info ] [MainThread]:   host: localhost
[0m11:42:48.844012 [info ] [MainThread]:   port: 10001
[0m11:42:48.844793 [info ] [MainThread]:   cluster: None
[0m11:42:48.845586 [info ] [MainThread]:   endpoint: None
[0m11:42:48.846436 [info ] [MainThread]:   schema: default
[0m11:42:48.847352 [info ] [MainThread]:   organization: 0
[0m11:42:48.848723 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:42:49.252145 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m11:42:49.252530 [debug] [MainThread]: Using spark connection "debug"
[0m11:42:49.252785 [debug] [MainThread]: On debug: select 1 as id
[0m11:42:49.253001 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:42:49.254983 [error] [MainThread]: Could not connect to any of [('127.0.0.1', 10001)]
[0m11:42:49.256480 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m11:42:49.256824 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m11:42:49.257126 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m11:42:49.257943 [info ] [MainThread]: [31m1 check failed:[0m
[0m11:42:49.259111 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m11:42:49.264546 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.8283958, "process_in_blocks": "0", "process_kernel_time": 0.640625, "process_mem_max_rss": "94536", "process_out_blocks": "0", "process_user_time": 1.4375}
[0m11:42:49.265421 [debug] [MainThread]: Command `dbt debug` failed at 11:42:49.265239 after 0.83 seconds
[0m11:42:49.266041 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:42:49.266657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ef181dd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ef0fc1430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ef143fdd0>]}
[0m11:42:49.267010 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:42:49.267233 [debug] [MainThread]: Flushing usage events
[0m13:51:19.099089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f52339790d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5232f4b5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f52338c8ad0>]}
[0m13:51:19.103109 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 13:51:19.103555 | 427ae5cf-04e3-42e4-bda5-44bb2ca9daf6 ==============================
[0m13:51:19.103555 [info ] [MainThread]: Running with dbt=1.9.2
[0m13:51:19.104569 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt deps', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:51:19.251293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '427ae5cf-04e3-42e4-bda5-44bb2ca9daf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5232d07ce0>]}
[0m13:51:19.251723 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:51:19.383185 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-ef2vqv5p'
[0m13:51:19.383977 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m13:51:19.947079 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m13:51:19.948005 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json
[0m13:51:20.607989 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json 200
[0m13:51:20.614254 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m13:51:21.258759 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m13:51:21.275124 [info ] [MainThread]: Updating lock file in file path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/package-lock.yml
[0m13:51:21.287213 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-dj4jeoga'
[0m13:51:21.299490 [info ] [MainThread]: Installing dbt-labs/dbt_external_tables
[0m13:51:23.156714 [info ] [MainThread]: Installed from version 0.8.2
[0m13:51:23.157496 [info ] [MainThread]: Updated version available: 0.11.1
[0m13:51:23.157930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '427ae5cf-04e3-42e4-bda5-44bb2ca9daf6', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f52328a2810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f52328a2a20>]}
[0m13:51:23.158213 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:51:23.158447 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m13:51:25.466105 [info ] [MainThread]: Installed from version 0.9.2
[0m13:51:25.467062 [info ] [MainThread]: Updated version available: 1.3.0
[0m13:51:25.467736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '427ae5cf-04e3-42e4-bda5-44bb2ca9daf6', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f523397a510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5232d74500>]}
[0m13:51:25.468184 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:51:25.468594 [info ] [MainThread]: 
[0m13:51:25.469267 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_external_tables', 'dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m13:51:25.475728 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 6.4299936, "process_in_blocks": "0", "process_kernel_time": 1.9375, "process_mem_max_rss": "87452", "process_out_blocks": "0", "process_user_time": 1.1875}
[0m13:51:25.476208 [debug] [MainThread]: Command `dbt deps` succeeded at 13:51:25.476106 after 6.43 seconds
[0m13:51:25.476521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5232eab800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f52337c70b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f523301e870>]}
[0m13:51:25.476772 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:51:25.476996 [debug] [MainThread]: Flushing usage events
[0m13:52:23.789685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f755077dd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7551135a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f755077dd30>]}
[0m13:52:23.793208 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 13:52:23.793524 | b646801c-01ec-4c15-b275-df4a9fdd5f0d ==============================
[0m13:52:23.793524 [info ] [MainThread]: Running with dbt=1.9.2
[0m13:52:23.794643 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt deps', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:52:23.913469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b646801c-01ec-4c15-b275-df4a9fdd5f0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7550841550>]}
[0m13:52:23.913917 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:52:23.940240 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-bg1ve00m'
[0m13:52:23.941110 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m13:53:06.442640 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m13:53:06.444455 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json
[0m13:53:28.018003 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json 200
[0m13:53:28.023608 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m13:53:28.463122 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m13:53:28.489579 [info ] [MainThread]: Updating lock file in file path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/package-lock.yml
[0m13:53:28.675812 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-pa58503c'
[0m13:53:28.680469 [info ] [MainThread]: Installing dbt-labs/dbt_external_tables
[0m13:53:30.595451 [info ] [MainThread]: Installed from version 0.11.1
[0m13:53:30.596840 [info ] [MainThread]: Up to date!
[0m13:53:30.598416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'b646801c-01ec-4c15-b275-df4a9fdd5f0d', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7550698b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75506d1940>]}
[0m13:53:30.599165 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:53:30.599818 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m13:53:32.386473 [info ] [MainThread]: Installed from version 1.3.0
[0m13:53:32.387464 [info ] [MainThread]: Up to date!
[0m13:53:32.388200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'b646801c-01ec-4c15-b275-df4a9fdd5f0d', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75507f49b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75507f55b0>]}
[0m13:53:32.388485 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:53:32.391593 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 68.655754, "process_in_blocks": "0", "process_kernel_time": 1.5625, "process_mem_max_rss": "87440", "process_out_blocks": "0", "process_user_time": 1.421875}
[0m13:53:32.392032 [debug] [MainThread]: Command `dbt deps` succeeded at 13:53:32.391935 after 68.66 seconds
[0m13:53:32.392340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7550a5f620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7550ae3fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f755069a840>]}
[0m13:53:32.392589 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:53:32.392810 [debug] [MainThread]: Flushing usage events
[0m13:54:00.510273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f62873126c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6287311340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6287afefc0>]}
[0m13:54:00.513844 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 13:54:00.514168 | 9f97272d-854f-4d6c-ba40-a61a977e80d0 ==============================
[0m13:54:00.514168 [info ] [MainThread]: Running with dbt=1.9.2
[0m13:54:00.515265 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:54:00.537048 [info ] [MainThread]: dbt version: 1.9.2
[0m13:54:00.538190 [info ] [MainThread]: python version: 3.12.3
[0m13:54:00.539064 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m13:54:00.539828 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m13:54:00.816513 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:54:00.816919 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:54:00.817195 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:54:00.842166 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m13:54:00.843619 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m13:54:00.844559 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m13:54:00.845454 [info ] [MainThread]: adapter type: spark
[0m13:54:00.846219 [info ] [MainThread]: adapter version: 1.9.1
[0m13:54:00.925760 [info ] [MainThread]: Configuration:
[0m13:54:00.927110 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:54:00.927998 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:54:00.928836 [info ] [MainThread]: Required dependencies:
[0m13:54:00.929620 [debug] [MainThread]: Executing "git --help"
[0m13:54:01.388913 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:54:01.389730 [debug] [MainThread]: STDERR: "b''"
[0m13:54:01.390099 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:54:01.391994 [info ] [MainThread]: Connection:
[0m13:54:01.392907 [info ] [MainThread]:   host: ec2-44-214-118-143.compute-1.amazonaws.com
[0m13:54:01.393673 [info ] [MainThread]:   port: 10001
[0m13:54:01.394451 [info ] [MainThread]:   cluster: None
[0m13:54:01.395213 [info ] [MainThread]:   endpoint: None
[0m13:54:01.395832 [info ] [MainThread]:   schema: s3_database
[0m13:54:01.396363 [info ] [MainThread]:   organization: 0
[0m13:54:01.397219 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m13:54:01.552843 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m13:54:01.553212 [debug] [MainThread]: Using spark connection "debug"
[0m13:54:01.553460 [debug] [MainThread]: On debug: select 1 as id
[0m13:54:01.553681 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:54:22.697050 [error] [MainThread]: Could not connect to any of [('44.214.118.143', 10001)]
[0m13:54:22.698504 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m13:54:22.698831 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m13:54:22.699157 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m13:54:22.700356 [info ] [MainThread]: [31m1 check failed:[0m
[0m13:54:22.701192 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m13:54:22.704973 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 22.246729, "process_in_blocks": "0", "process_kernel_time": 0.984375, "process_mem_max_rss": "94592", "process_out_blocks": "0", "process_user_time": 1.171875}
[0m13:54:22.705414 [debug] [MainThread]: Command `dbt debug` failed at 13:54:22.705320 after 22.25 seconds
[0m13:54:22.705745 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:54:22.706084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f628622b800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6285896d80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f628639b3b0>]}
[0m13:54:22.706480 [debug] [MainThread]: An error was encountered while trying to send an event
[0m13:54:22.706736 [debug] [MainThread]: Flushing usage events
[0m12:33:42.659688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6859e99f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6859c32c30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6859744d10>]}
[0m12:33:42.703721 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:33:42.705317 | 1449c0c1-247f-4890-bb0c-661d7eb854f6 ==============================
[0m12:33:42.705317 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:33:42.706993 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt debug', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:33:43.080777 [info ] [MainThread]: dbt version: 1.9.2
[0m12:33:43.082606 [info ] [MainThread]: python version: 3.12.3
[0m12:33:43.083527 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m12:33:43.084436 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m12:33:48.290105 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:33:48.291180 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:33:48.292052 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:33:48.352227 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m12:33:48.354046 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m12:33:48.354922 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m12:33:48.355919 [info ] [MainThread]: adapter type: spark
[0m12:33:48.356930 [info ] [MainThread]: adapter version: 1.9.1
[0m12:33:48.523292 [info ] [MainThread]: Configuration:
[0m12:33:48.525095 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:33:48.525985 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:33:48.526834 [info ] [MainThread]: Required dependencies:
[0m12:33:48.527582 [debug] [MainThread]: Executing "git --help"
[0m12:33:48.948150 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:33:48.948628 [debug] [MainThread]: STDERR: "b''"
[0m12:33:48.948902 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:33:48.951035 [info ] [MainThread]: Connection:
[0m12:33:48.952637 [info ] [MainThread]:   host: localhost
[0m12:33:48.954112 [info ] [MainThread]:   port: 10000
[0m12:33:48.955578 [info ] [MainThread]:   cluster: None
[0m12:33:48.956885 [info ] [MainThread]:   endpoint: None
[0m12:33:48.957662 [info ] [MainThread]:   schema: default
[0m12:33:48.958319 [info ] [MainThread]:   organization: 0
[0m12:33:48.959277 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:33:49.447157 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:33:49.448189 [debug] [MainThread]: Using spark connection "debug"
[0m12:33:49.449013 [debug] [MainThread]: On debug: select 1 as id
[0m12:33:49.449756 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:36:22.239799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf4745a9c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf475ef380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf475ee960>]}
[0m12:36:22.243159 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:36:22.243464 | 07522451-0661-450f-9509-3e3e76a9dab6 ==============================
[0m12:36:22.243464 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:36:22.244693 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m12:36:22.262024 [info ] [MainThread]: dbt version: 1.9.2
[0m12:36:22.262985 [info ] [MainThread]: python version: 3.12.3
[0m12:36:22.263635 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m12:36:22.264133 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m12:36:22.454168 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:36:22.454575 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:36:22.454857 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:36:22.480142 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m12:36:22.481264 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m12:36:22.481876 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m12:36:22.482491 [info ] [MainThread]: adapter type: spark
[0m12:36:22.483049 [info ] [MainThread]: adapter version: 1.9.1
[0m12:36:22.563685 [info ] [MainThread]: Configuration:
[0m12:36:22.564932 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:36:22.565842 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:36:22.566757 [info ] [MainThread]: Required dependencies:
[0m12:36:22.567618 [debug] [MainThread]: Executing "git --help"
[0m12:36:22.583340 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:36:22.583867 [debug] [MainThread]: STDERR: "b''"
[0m12:36:22.584168 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:36:22.585505 [info ] [MainThread]: Connection:
[0m12:36:22.586458 [info ] [MainThread]:   host: localhost
[0m12:36:22.587312 [info ] [MainThread]:   port: 10000
[0m12:36:22.588178 [info ] [MainThread]:   cluster: None
[0m12:36:22.588936 [info ] [MainThread]:   endpoint: None
[0m12:36:22.589710 [info ] [MainThread]:   schema: default
[0m12:36:22.590497 [info ] [MainThread]:   organization: 0
[0m12:36:22.591470 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:36:22.752595 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:36:22.752989 [debug] [MainThread]: Using spark connection "debug"
[0m12:36:22.753265 [debug] [MainThread]: On debug: select 1 as id
[0m12:36:22.753506 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:39:33.480608 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m12:39:33.481356 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m12:39:33.483040 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m12:39:33.484620 [info ] [MainThread]: [31m1 check failed:[0m
[0m12:39:33.485392 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m12:39:33.500353 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 191.31537, "process_in_blocks": "0", "process_kernel_time": 0.796875, "process_mem_max_rss": "94552", "process_out_blocks": "0", "process_user_time": 1.203125}
[0m12:39:33.500910 [debug] [MainThread]: Command `dbt debug` failed at 12:39:33.500758 after 191.32 seconds
[0m12:39:33.501368 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:39:33.501804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf471d2ba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf470d38c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf466b9580>]}
[0m12:39:33.502207 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:39:33.502569 [debug] [MainThread]: Flushing usage events
[0m12:41:09.975563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1462fe4e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1461a5f440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1463116270>]}
[0m12:41:09.978970 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:41:09.979287 | 581743e1-2060-4c3e-b08b-ded5405693c3 ==============================
[0m12:41:09.979287 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:41:09.980350 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:41:09.997466 [info ] [MainThread]: dbt version: 1.9.2
[0m12:41:09.998240 [info ] [MainThread]: python version: 3.12.3
[0m12:41:09.998968 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m12:41:09.999598 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m12:41:10.157610 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:41:10.158035 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:41:10.158314 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:41:10.182986 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m12:41:10.184386 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m12:41:10.185077 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m12:41:10.185628 [info ] [MainThread]: adapter type: spark
[0m12:41:10.186151 [info ] [MainThread]: adapter version: 1.9.1
[0m12:41:10.266478 [info ] [MainThread]: Configuration:
[0m12:41:10.267744 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:41:10.268613 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:41:10.269521 [info ] [MainThread]: Required dependencies:
[0m12:41:10.270462 [debug] [MainThread]: Executing "git --help"
[0m12:41:10.286039 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:41:10.286604 [debug] [MainThread]: STDERR: "b''"
[0m12:41:10.286964 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:41:10.288236 [info ] [MainThread]: Connection:
[0m12:41:10.288951 [info ] [MainThread]:   host: localhost
[0m12:41:10.289854 [info ] [MainThread]:   port: 10000
[0m12:41:10.290694 [info ] [MainThread]:   cluster: None
[0m12:41:10.291482 [info ] [MainThread]:   endpoint: None
[0m12:41:10.292344 [info ] [MainThread]:   schema: default
[0m12:41:10.292997 [info ] [MainThread]:   organization: 0
[0m12:41:10.294094 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:41:10.447266 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m12:41:10.447816 [debug] [MainThread]: Using spark connection "debug"
[0m12:41:10.448193 [debug] [MainThread]: On debug: select 1 as id
[0m12:41:10.448531 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:41:13.021167 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m12:41:13.022330 [debug] [MainThread]: SQL status: OK in 2.574 seconds
[0m12:41:13.024501 [debug] [MainThread]: On debug: Close
[0m12:41:13.665695 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m12:41:13.668930 [info ] [MainThread]: [32mAll checks passed![0m
[0m12:41:13.673221 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 3.748613, "process_in_blocks": "0", "process_kernel_time": 0.609375, "process_mem_max_rss": "94580", "process_out_blocks": "0", "process_user_time": 1.375}
[0m12:41:13.674359 [debug] [MainThread]: Command `dbt debug` succeeded at 12:41:13.674099 after 3.75 seconds
[0m12:41:13.675189 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:41:13.676061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1460a55c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1460a566c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1460ac2330>]}
[0m12:41:13.676893 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:41:13.677653 [debug] [MainThread]: Flushing usage events
[0m14:21:13.570874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7643fdbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7644d9250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7650e39b0>]}
[0m14:21:13.575095 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 14:21:13.575422 | e71c2cac-2752-428b-b218-575774df7854 ==============================
[0m14:21:13.575422 [info ] [MainThread]: Running with dbt=1.9.2
[0m14:21:13.576645 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt compile --select vehicle_ext', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:21:13.873109 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:21:13.873517 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:21:13.873791 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:21:14.008826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e71c2cac-2752-428b-b218-575774df7854', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7637eef60>]}
[0m14:21:14.009256 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:21:14.052961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e71c2cac-2752-428b-b218-575774df7854', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7636f6420>]}
[0m14:21:14.053365 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:21:14.053775 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m14:21:14.312108 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m14:21:14.313957 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:21:14.315069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e71c2cac-2752-428b-b218-575774df7854', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7637a7470>]}
[0m14:21:14.315306 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:21:19.875832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e71c2cac-2752-428b-b218-575774df7854', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff762c6b140>]}
[0m14:21:19.876796 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:21:20.004790 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m14:21:20.014956 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m14:21:20.182300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e71c2cac-2752-428b-b218-575774df7854', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7618d7080>]}
[0m14:21:20.182962 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:21:20.183402 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 639 macros
[0m14:21:20.184391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e71c2cac-2752-428b-b218-575774df7854', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff761a91010>]}
[0m14:21:20.184722 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:21:20.186866 [info ] [MainThread]: 
[0m14:21:20.188345 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:21:20.189142 [info ] [MainThread]: 
[0m14:21:20.190038 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:21:20.196808 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m14:21:20.208412 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:21:20.208928 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:21:20.209226 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:21:20.209491 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:21:20.220855 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:21:20.222158 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:21:20.222620 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:21:20.223409 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_relations_without_caching
[0m14:21:20.224088 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:21:20.224851 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Runtime Error
  Database Error
    failed to connect
[0m14:21:20.226132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e71c2cac-2752-428b-b218-575774df7854', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff762c90110>]}
[0m14:21:20.226938 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:21:20.253042 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m14:21:20.253688 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m14:21:20.254220 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m14:21:20.261413 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m14:21:20.265699 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m14:21:20.267475 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m14:21:20.271175 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:21:20.271622 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m14:21:20.272272 [debug] [MainThread]: Command end result
[0m14:21:20.301617 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m14:21:20.306876 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m14:21:20.320817 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m14:21:20.321576 [info ] [MainThread]: Compiled node 'vehicle_ext' is:
CREATE EXTERNAL TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'
STORED AS CSV
LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';
[0m14:21:20.326453 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 6.8089457, "process_in_blocks": "0", "process_kernel_time": 1.578125, "process_mem_max_rss": "105160", "process_out_blocks": "0", "process_user_time": 3.640625}
[0m14:21:20.326968 [debug] [MainThread]: Command `dbt compile` succeeded at 14:21:20.326858 after 6.81 seconds
[0m14:21:20.327354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7643fdbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7644af320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7644adfd0>]}
[0m14:21:20.327650 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:21:20.327908 [debug] [MainThread]: Flushing usage events
[0m14:22:33.763168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2e878c8a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2e86095760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2e86044cb0>]}
[0m14:22:33.766538 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 14:22:33.766851 | 1e60f956-9d6d-435e-b729-a64063fe8a54 ==============================
[0m14:22:33.766851 [info ] [MainThread]: Running with dbt=1.9.2
[0m14:22:33.767870 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select vehicle_ext', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:22:33.929324 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:22:33.929787 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:22:33.930071 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:22:34.056063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1e60f956-9d6d-435e-b729-a64063fe8a54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2e859ba450>]}
[0m14:22:34.056478 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:22:34.099248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1e60f956-9d6d-435e-b729-a64063fe8a54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2e8533a330>]}
[0m14:22:34.099648 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:22:34.100043 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m14:22:34.263993 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m14:22:34.430760 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:22:34.431115 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:22:34.462114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1e60f956-9d6d-435e-b729-a64063fe8a54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2e852720c0>]}
[0m14:22:34.462618 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:22:34.526097 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m14:22:34.531427 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m14:22:34.565109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1e60f956-9d6d-435e-b729-a64063fe8a54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2e851cd670>]}
[0m14:22:34.565497 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:22:34.565804 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 639 macros
[0m14:22:34.566958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1e60f956-9d6d-435e-b729-a64063fe8a54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2e85d8e0f0>]}
[0m14:22:34.567444 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:22:34.568655 [info ] [MainThread]: 
[0m14:22:34.569620 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:22:34.570484 [info ] [MainThread]: 
[0m14:22:34.571450 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:22:34.572947 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:22:34.581749 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:22:34.582134 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:22:34.582375 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:22:34.584375 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:22:34.585539 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:22:34.585951 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:22:34.586415 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:22:34.586828 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:22:34.587611 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:22:34.587912 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:22:34.588186 [info ] [MainThread]: 
[0m14:22:34.589171 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:22:34.590118 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:22:34.593032 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8885297, "process_in_blocks": "0", "process_kernel_time": 0.78125, "process_mem_max_rss": "100048", "process_out_blocks": "0", "process_user_time": 1.515625}
[0m14:22:34.593400 [debug] [MainThread]: Command `dbt run` failed at 14:22:34.593319 after 0.89 seconds
[0m14:22:34.593686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2e8676b290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2e84e692b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2e852901a0>]}
[0m14:22:34.593957 [debug] [MainThread]: An error was encountered while trying to send an event
[0m14:22:34.594171 [debug] [MainThread]: Flushing usage events
[0m01:32:03.878398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbd998df70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbd998d160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbd989b5c0>]}
[0m01:32:03.937968 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:32:03.939168 | 5d727d1d-2674-4574-a868-54108db022c0 ==============================
[0m01:32:03.939168 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:32:03.942221 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m01:32:04.424393 [info ] [MainThread]: dbt version: 1.9.2
[0m01:32:04.426073 [info ] [MainThread]: python version: 3.12.3
[0m01:32:04.427316 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m01:32:04.428451 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m01:32:13.948323 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:32:13.950488 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:32:13.952294 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:32:14.079062 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m01:32:14.081139 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m01:32:14.083330 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m01:32:14.085043 [info ] [MainThread]: adapter type: spark
[0m01:32:14.086607 [info ] [MainThread]: adapter version: 1.9.1
[0m01:32:14.434635 [info ] [MainThread]: Configuration:
[0m01:32:14.437106 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:32:14.438207 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:32:14.439160 [info ] [MainThread]: Required dependencies:
[0m01:32:14.440151 [debug] [MainThread]: Executing "git --help"
[0m01:32:14.907361 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:32:14.910442 [debug] [MainThread]: STDERR: "b''"
[0m01:32:14.911410 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:32:14.914297 [info ] [MainThread]: Connection:
[0m01:32:14.918468 [info ] [MainThread]:   host: localhost
[0m01:32:14.920544 [info ] [MainThread]:   port: 10000
[0m01:32:14.923761 [info ] [MainThread]:   cluster: None
[0m01:32:14.925573 [info ] [MainThread]:   endpoint: None
[0m01:32:14.928241 [info ] [MainThread]:   schema: default
[0m01:32:14.929496 [info ] [MainThread]:   organization: 0
[0m01:32:14.932784 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:32:16.058724 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m01:32:16.060356 [debug] [MainThread]: Using spark connection "debug"
[0m01:32:16.061520 [debug] [MainThread]: On debug: select 1 as id
[0m01:32:16.062627 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:32:18.448331 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m01:32:18.449684 [debug] [MainThread]: SQL status: OK in 2.387 seconds
[0m01:32:18.453491 [debug] [MainThread]: On debug: Close
[0m01:32:19.061800 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m01:32:19.065005 [info ] [MainThread]: [32mAll checks passed![0m
[0m01:32:19.090747 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 15.5587015, "process_in_blocks": "0", "process_kernel_time": 4.75, "process_mem_max_rss": "94548", "process_out_blocks": "0", "process_user_time": 5.015625}
[0m01:32:19.092353 [debug] [MainThread]: Command `dbt debug` succeeded at 01:32:19.092021 after 15.56 seconds
[0m01:32:19.093395 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m01:32:19.094472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbd9743290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbd96436e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbd90ca690>]}
[0m01:32:19.095373 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:32:19.096172 [debug] [MainThread]: Flushing usage events
[0m01:32:44.721171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3445b3eae0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f344538d6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f344522fad0>]}
[0m01:32:44.726124 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:32:44.726576 | 5771bfcd-f39d-4f25-94dd-1a32e34e040c ==============================
[0m01:32:44.726576 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:32:44.728087 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select vehicle_ext', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:32:44.951500 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:32:44.952033 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:32:44.952414 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:32:45.117463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5771bfcd-f39d-4f25-94dd-1a32e34e040c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3444afe1e0>]}
[0m01:32:45.117993 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:32:45.178438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5771bfcd-f39d-4f25-94dd-1a32e34e040c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3444b8db20>]}
[0m01:32:45.179284 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:32:45.179983 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:32:45.477325 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:32:49.477902 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:32:49.479256 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:32:49.610231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5771bfcd-f39d-4f25-94dd-1a32e34e040c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3443ef7a40>]}
[0m01:32:49.611662 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:32:49.875178 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:32:49.920596 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:32:50.091694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5771bfcd-f39d-4f25-94dd-1a32e34e040c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3443f462d0>]}
[0m01:32:50.092856 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:32:50.093877 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 639 macros
[0m01:32:50.096061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5771bfcd-f39d-4f25-94dd-1a32e34e040c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3443f1d700>]}
[0m01:32:50.097583 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:32:50.103682 [info ] [MainThread]: 
[0m01:32:50.106275 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:32:50.108283 [info ] [MainThread]: 
[0m01:32:50.110798 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:32:50.114111 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:32:50.155670 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:32:50.157141 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:32:50.158416 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:32:52.137881 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:32:52.139229 [debug] [ThreadPool]: SQL status: OK in 1.981 seconds
[0m01:32:53.062280 [debug] [ThreadPool]: On list_schemas: Close
[0m01:32:53.691119 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m01:32:53.706447 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:32:53.707645 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:32:53.708646 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:32:53.709577 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:32:56.028569 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:32:56.029937 [debug] [ThreadPool]: SQL status: OK in 2.320 seconds
[0m01:32:56.922594 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:32:56.923928 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:32:56.924639 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:32:56.925585 [debug] [ThreadPool]: On list_None_default: Close
[0m01:32:57.469586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5771bfcd-f39d-4f25-94dd-1a32e34e040c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3444106f60>]}
[0m01:32:57.470887 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:32:57.471995 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:32:57.473303 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:32:57.502072 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:32:57.504157 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_ext ............................... [RUN]
[0m01:32:57.507278 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:32:57.509339 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:32:57.537242 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:32:57.542175 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:32:57.605060 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:32:57.606321 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
drop table if exists default.vehicle_ext
[0m01:32:57.607494 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:32:59.714789 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:32:59.716350 [debug] [Thread-1 (]: SQL status: OK in 2.109 seconds
[0m01:32:59.850685 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:32:59.861892 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:32:59.862784 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:32:59.863614 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE EXTERNAL TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'
STORED AS CSV
LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';
  
[0m01:33:00.124308 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE EXTERNAL TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'
STORED AS CSV
LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';
  
[0m01:33:00.126143 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement"), operationHandle=None)
[0m01:33:00.127956 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: ROLLBACK
[0m01:33:00.129233 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:33:00.129928 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m01:33:00.729996 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement
[0m01:33:00.755245 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5771bfcd-f39d-4f25-94dd-1a32e34e040c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34458489b0>]}
[0m01:33:00.756276 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m01:33:00.757889 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_ext ...................... [[31mERROR[0m in 3.22s]
[0m01:33:00.760501 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:33:00.762243 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement.
[0m01:33:00.773413 [debug] [MainThread]: On master: ROLLBACK
[0m01:33:00.775005 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:33:02.070275 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:33:02.071604 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:33:02.072651 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:33:02.073543 [debug] [MainThread]: On master: ROLLBACK
[0m01:33:02.074409 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:33:02.075246 [debug] [MainThread]: On master: Close
[0m01:33:02.320203 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:33:02.321418 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:33:02.322398 [info ] [MainThread]: 
[0m01:33:02.324915 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.21 seconds (12.21s).
[0m01:33:02.327733 [debug] [MainThread]: Command end result
[0m01:33:02.405063 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:33:02.412387 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:33:02.430466 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:33:02.432434 [info ] [MainThread]: 
[0m01:33:02.434818 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m01:33:02.436637 [info ] [MainThread]: 
[0m01:33:02.438670 [error] [MainThread]:   Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement
[0m01:33:02.441143 [info ] [MainThread]: 
[0m01:33:02.443079 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:33:02.447616 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 17.79278, "process_in_blocks": "0", "process_kernel_time": 2.015625, "process_mem_max_rss": "102360", "process_out_blocks": "0", "process_user_time": 2.765625}
[0m01:33:02.449082 [debug] [MainThread]: Command `dbt run` failed at 01:33:02.448751 after 17.79 seconds
[0m01:33:02.450325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f344562fda0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3448583170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34453ef440>]}
[0m01:33:02.451416 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:33:02.452669 [debug] [MainThread]: Flushing usage events
[0m01:33:24.452426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9004529c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9003dd66f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90057a6720>]}
[0m01:33:24.457316 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:33:24.457747 | 18c5a268-447d-4fbd-a8c7-69d016eb998e ==============================
[0m01:33:24.457747 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:33:24.460307 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt compile --select vehicle_ext', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m01:33:24.671250 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:33:24.671764 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:33:24.672126 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:33:24.840411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '18c5a268-447d-4fbd-a8c7-69d016eb998e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9003a23e60>]}
[0m01:33:24.840960 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:33:24.894133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '18c5a268-447d-4fbd-a8c7-69d016eb998e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f900383da90>]}
[0m01:33:24.894604 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:33:24.895102 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:33:25.115265 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:33:25.441834 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:33:25.443112 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:33:25.572776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '18c5a268-447d-4fbd-a8c7-69d016eb998e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9002fb6930>]}
[0m01:33:25.574187 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:33:25.833545 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:33:25.847167 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:33:25.915729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '18c5a268-447d-4fbd-a8c7-69d016eb998e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9003042ed0>]}
[0m01:33:25.917478 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:33:25.919107 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 639 macros
[0m01:33:25.921689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18c5a268-447d-4fbd-a8c7-69d016eb998e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9003c796a0>]}
[0m01:33:25.922679 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:33:25.926944 [info ] [MainThread]: 
[0m01:33:25.929014 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:33:25.930976 [info ] [MainThread]: 
[0m01:33:25.933156 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:33:25.953402 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m01:33:25.988694 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:33:25.989962 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:33:25.991012 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:33:25.991896 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:33:28.079746 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:33:28.081176 [debug] [ThreadPool]: SQL status: OK in 2.089 seconds
[0m01:33:28.901764 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:33:28.903092 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:33:28.904174 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:33:28.905115 [debug] [ThreadPool]: On list_None_default: Close
[0m01:33:29.514379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18c5a268-447d-4fbd-a8c7-69d016eb998e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f900386b950>]}
[0m01:33:29.515825 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:33:29.548191 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:33:29.549769 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:33:29.551049 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:33:29.573912 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:33:29.577960 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:33:29.579449 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:33:29.586774 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:33:29.587991 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:33:29.590055 [debug] [MainThread]: Command end result
[0m01:33:29.667831 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:33:29.677901 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:33:29.705897 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:33:29.707184 [info ] [MainThread]: Compiled node 'vehicle_ext' is:
CREATE EXTERNAL TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'
STORED AS CSV
LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';
[0m01:33:29.717951 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 5.3313203, "process_in_blocks": "0", "process_kernel_time": 1.28125, "process_mem_max_rss": "100844", "process_out_blocks": "0", "process_user_time": 2.546875}
[0m01:33:29.720291 [debug] [MainThread]: Command `dbt compile` succeeded at 01:33:29.719745 after 5.33 seconds
[0m01:33:29.721733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90041f8140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90031680b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90030c93a0>]}
[0m01:33:29.722514 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:33:29.723375 [debug] [MainThread]: Flushing usage events
[0m01:35:06.441456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09299365d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09297e6480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09280c5d60>]}
[0m01:35:06.446749 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:35:06.447191 | dd11ee19-887b-43d9-bc2d-617130749cd4 ==============================
[0m01:35:06.447191 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:35:06.449278 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run --select vehicle_ext', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:35:06.669971 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:35:06.670487 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:35:06.670856 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:35:06.824943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dd11ee19-887b-43d9-bc2d-617130749cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09280c5d60>]}
[0m01:35:06.825403 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:35:06.880520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dd11ee19-887b-43d9-bc2d-617130749cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0927112720>]}
[0m01:35:06.881131 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:35:06.881644 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:35:07.109029 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:35:07.336726 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:35:07.337460 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m01:35:08.113463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dd11ee19-887b-43d9-bc2d-617130749cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0926b2b6b0>]}
[0m01:35:08.114862 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:35:08.387331 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:35:08.400128 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:35:08.484498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dd11ee19-887b-43d9-bc2d-617130749cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0926e17a70>]}
[0m01:35:08.485182 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:35:08.486037 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 639 macros
[0m01:35:08.488460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dd11ee19-887b-43d9-bc2d-617130749cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0926b30fb0>]}
[0m01:35:08.489439 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:35:08.493630 [info ] [MainThread]: 
[0m01:35:08.495843 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:35:08.497486 [info ] [MainThread]: 
[0m01:35:08.499095 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:35:08.502168 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:35:08.534801 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:35:08.536205 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:35:08.537222 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:35:10.685970 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:35:10.687327 [debug] [ThreadPool]: SQL status: OK in 2.150 seconds
[0m01:35:11.609762 [debug] [ThreadPool]: On list_schemas: Close
[0m01:35:12.137239 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m01:35:12.152311 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:35:12.153598 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:35:12.154646 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:35:12.155609 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:35:14.474363 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:35:14.476133 [debug] [ThreadPool]: SQL status: OK in 2.320 seconds
[0m01:35:15.399339 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:35:15.400607 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:35:15.401733 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:35:15.402622 [debug] [ThreadPool]: On list_None_default: Close
[0m01:35:16.011164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dd11ee19-887b-43d9-bc2d-617130749cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09271522d0>]}
[0m01:35:16.012447 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:35:16.013584 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:35:16.014599 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:35:16.044001 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:35:16.045805 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_ext ............................... [RUN]
[0m01:35:16.048546 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:35:16.049913 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:35:16.068909 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:35:16.073015 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:35:16.132141 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:35:16.134344 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
drop table if exists default.vehicle_ext
[0m01:35:16.136267 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:35:18.365731 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:35:18.367207 [debug] [Thread-1 (]: SQL status: OK in 2.231 seconds
[0m01:35:18.496512 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:35:18.501051 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:35:18.502260 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:35:18.503399 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE EXTERNAL TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'
STORED AS CSV
LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';
  
[0m01:35:18.776528 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE EXTERNAL TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'
STORED AS CSV
LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';
  
[0m01:35:18.778469 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement"), operationHandle=None)
[0m01:35:18.780360 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: ROLLBACK
[0m01:35:18.781585 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:35:18.782674 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m01:35:19.100160 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement
[0m01:35:19.106257 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dd11ee19-887b-43d9-bc2d-617130749cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0926cc1310>]}
[0m01:35:19.107406 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m01:35:19.109255 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_ext ...................... [[31mERROR[0m in 3.05s]
[0m01:35:19.112927 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:35:19.114823 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement.
[0m01:35:19.125094 [debug] [MainThread]: On master: ROLLBACK
[0m01:35:19.126263 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:35:20.720521 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:35:20.721889 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:35:20.722905 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:35:20.723856 [debug] [MainThread]: On master: ROLLBACK
[0m01:35:20.724770 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:35:20.725626 [debug] [MainThread]: On master: Close
[0m01:35:21.027937 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:35:21.029208 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:35:21.030227 [info ] [MainThread]: 
[0m01:35:21.032826 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.53 seconds (12.53s).
[0m01:35:21.035619 [debug] [MainThread]: Command end result
[0m01:35:21.114336 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:35:21.121875 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:35:21.146152 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:35:21.147292 [info ] [MainThread]: 
[0m01:35:21.149687 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m01:35:21.151382 [info ] [MainThread]: 
[0m01:35:21.152879 [error] [MainThread]:   Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement
[0m01:35:21.154737 [info ] [MainThread]: 
[0m01:35:21.156661 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:35:21.159939 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 14.78625, "process_in_blocks": "0", "process_kernel_time": 1.0625, "process_mem_max_rss": "106456", "process_out_blocks": "0", "process_user_time": 3.46875}
[0m01:35:21.160620 [debug] [MainThread]: Command `dbt run` failed at 01:35:21.160473 after 14.79 seconds
[0m01:35:21.161177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0927d8ce60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0927915730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f09284ff1a0>]}
[0m01:35:21.161733 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:35:21.162189 [debug] [MainThread]: Flushing usage events
[0m01:36:37.847635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60f26c4fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60f0dbd250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60f0dbdf40>]}
[0m01:36:37.858681 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:36:37.859391 | 9b5c7da8-63ed-432c-9309-5a421783f13c ==============================
[0m01:36:37.859391 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:36:37.861584 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile --select vehicle_ext', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m01:36:38.130895 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:36:38.131473 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:36:38.131853 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:36:38.341838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b5c7da8-63ed-432c-9309-5a421783f13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60f0fd2300>]}
[0m01:36:38.342318 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:38.404722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9b5c7da8-63ed-432c-9309-5a421783f13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60f04c1520>]}
[0m01:36:38.405282 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:38.405863 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:36:38.684684 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:36:39.138477 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:36:39.141280 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m01:36:40.236121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b5c7da8-63ed-432c-9309-5a421783f13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60efb32660>]}
[0m01:36:40.237566 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:40.506567 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:36:40.519605 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:36:40.585894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b5c7da8-63ed-432c-9309-5a421783f13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60efb31c70>]}
[0m01:36:40.587025 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:40.588043 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 639 macros
[0m01:36:40.590586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b5c7da8-63ed-432c-9309-5a421783f13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60efb20770>]}
[0m01:36:40.591554 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:40.595641 [info ] [MainThread]: 
[0m01:36:40.597815 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:36:40.599161 [info ] [MainThread]: 
[0m01:36:40.601640 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:36:40.623761 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m01:36:40.658764 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:36:40.660214 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:36:40.661203 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:36:40.662178 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:36:42.562827 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:36:42.564268 [debug] [ThreadPool]: SQL status: OK in 1.902 seconds
[0m01:36:43.462583 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:36:43.463926 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:36:43.464992 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:36:43.466078 [debug] [ThreadPool]: On list_None_default: Close
[0m01:36:44.074937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b5c7da8-63ed-432c-9309-5a421783f13c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60f010c050>]}
[0m01:36:44.076240 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:44.110656 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:36:44.112849 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:36:44.114294 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:36:44.132344 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:36:44.136319 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:36:44.137501 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:36:44.144102 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:36:44.145823 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:36:44.148881 [debug] [MainThread]: Command end result
[0m01:36:44.227091 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:36:44.234150 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:36:44.253688 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:36:44.254433 [info ] [MainThread]: Compiled node 'vehicle_ext' is:
CREATE EXTERNAL TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3';

LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
[0m01:36:44.261153 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 6.489917, "process_in_blocks": "0", "process_kernel_time": 1.390625, "process_mem_max_rss": "104920", "process_out_blocks": "0", "process_user_time": 3.578125}
[0m01:36:44.262676 [debug] [MainThread]: Command `dbt compile` succeeded at 01:36:44.262357 after 6.49 seconds
[0m01:36:44.263795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60f0b59ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60f04c1820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60f154b290>]}
[0m01:36:44.264780 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:44.265614 [debug] [MainThread]: Flushing usage events
[0m01:36:50.394843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1baaefbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1baaefbf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1baaefe60>]}
[0m01:36:50.399811 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:36:50.400244 | f420abc8-58b0-4323-b6b5-ea793ce56ce5 ==============================
[0m01:36:50.400244 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:36:50.402211 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select vehicle_ext', 'send_anonymous_usage_stats': 'True'}
[0m01:36:50.617391 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:36:50.617909 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:36:50.618264 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:36:50.769509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f420abc8-58b0-4323-b6b5-ea793ce56ce5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1ba18c710>]}
[0m01:36:50.770005 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:50.822980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f420abc8-58b0-4323-b6b5-ea793ce56ce5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1ba1806b0>]}
[0m01:36:50.823467 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:50.823974 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:36:51.051567 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:36:51.273061 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:36:51.273497 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:36:51.312909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f420abc8-58b0-4323-b6b5-ea793ce56ce5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1ba1128d0>]}
[0m01:36:51.313393 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:51.396807 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:36:51.401612 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:36:51.437565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f420abc8-58b0-4323-b6b5-ea793ce56ce5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1ba0351c0>]}
[0m01:36:51.438264 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:51.438829 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 639 macros
[0m01:36:51.440758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f420abc8-58b0-4323-b6b5-ea793ce56ce5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1ba816300>]}
[0m01:36:51.441772 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:51.445962 [info ] [MainThread]: 
[0m01:36:51.447880 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:36:51.449632 [info ] [MainThread]: 
[0m01:36:51.451229 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:36:51.455499 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:36:51.491178 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:36:51.492668 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:36:51.493755 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:36:53.417122 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:36:53.418637 [debug] [ThreadPool]: SQL status: OK in 1.925 seconds
[0m01:36:54.316685 [debug] [ThreadPool]: On list_schemas: Close
[0m01:36:54.845683 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m01:36:54.861455 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:36:54.862703 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:36:54.863726 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:36:54.864699 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:36:57.079514 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:36:57.080875 [debug] [ThreadPool]: SQL status: OK in 2.216 seconds
[0m01:36:57.986844 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:36:57.988288 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:36:57.989349 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:36:57.990293 [debug] [ThreadPool]: On list_None_default: Close
[0m01:36:58.616515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f420abc8-58b0-4323-b6b5-ea793ce56ce5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1ba0d2ae0>]}
[0m01:36:58.617807 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:36:58.618939 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:36:58.619951 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:36:58.648743 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:36:58.650513 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_ext ............................... [RUN]
[0m01:36:58.652941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:36:58.654171 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:36:58.677320 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:36:58.681019 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:36:58.742564 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:36:58.744167 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
drop table if exists default.vehicle_ext
[0m01:36:58.745288 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:37:00.868144 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:37:00.869589 [debug] [Thread-1 (]: SQL status: OK in 2.124 seconds
[0m01:37:01.001525 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:37:01.006262 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:01.008560 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:37:01.010609 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE EXTERNAL TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3';

LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:37:01.278836 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE EXTERNAL TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3';

LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:37:01.280806 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement"), operationHandle=None)
[0m01:37:01.282694 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: ROLLBACK
[0m01:37:01.283860 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:37:01.284853 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m01:37:01.548104 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement
[0m01:37:01.554231 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f420abc8-58b0-4323-b6b5-ea793ce56ce5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1bb5c89b0>]}
[0m01:37:01.555303 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m01:37:01.556942 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_ext ...................... [[31mERROR[0m in 2.90s]
[0m01:37:01.560403 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:37:01.562090 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement.
[0m01:37:01.572052 [debug] [MainThread]: On master: ROLLBACK
[0m01:37:01.573428 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:37:03.227209 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:37:03.228513 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:03.229474 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:37:03.230380 [debug] [MainThread]: On master: ROLLBACK
[0m01:37:03.231271 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:37:03.232114 [debug] [MainThread]: On master: Close
[0m01:37:03.531327 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:37:03.532590 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:37:03.533575 [info ] [MainThread]: 
[0m01:37:03.536302 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.08 seconds (12.08s).
[0m01:37:03.540148 [debug] [MainThread]: Command end result
[0m01:37:03.637716 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:37:03.644553 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:37:03.669491 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:37:03.671358 [info ] [MainThread]: 
[0m01:37:03.674011 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m01:37:03.676096 [info ] [MainThread]: 
[0m01:37:03.678218 [error] [MainThread]:   Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement
[0m01:37:03.680258 [info ] [MainThread]: 
[0m01:37:03.681904 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:37:03.686267 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 13.357368, "process_in_blocks": "0", "process_kernel_time": 1.078125, "process_mem_max_rss": "102424", "process_out_blocks": "0", "process_user_time": 2.453125}
[0m01:37:03.687989 [debug] [MainThread]: Command `dbt run` failed at 01:37:03.687594 after 13.36 seconds
[0m01:37:03.689196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1bac95d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1b9cc6990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa1ba27af90>]}
[0m01:37:03.690125 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:03.690944 [debug] [MainThread]: Flushing usage events
[0m01:37:12.840777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f69dd8fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f6a9e2a20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f6930f3e0>]}
[0m01:37:12.846000 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:37:12.846417 | ff0093eb-5178-4705-b174-86cbd0818d04 ==============================
[0m01:37:12.846417 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:37:12.848359 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select vehicle_ext', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:37:13.078541 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:37:13.079122 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:37:13.079520 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:37:13.237405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ff0093eb-5178-4705-b174-86cbd0818d04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f69757f80>]}
[0m01:37:13.237899 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:13.294099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ff0093eb-5178-4705-b174-86cbd0818d04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f68d20aa0>]}
[0m01:37:13.295087 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:13.295670 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:37:13.533763 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:37:13.763134 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:37:13.763825 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m01:37:14.570176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ff0093eb-5178-4705-b174-86cbd0818d04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f68378380>]}
[0m01:37:14.571550 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:14.823264 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:37:14.836239 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:37:14.915416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ff0093eb-5178-4705-b174-86cbd0818d04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f686f5af0>]}
[0m01:37:14.916553 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:14.917578 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 639 macros
[0m01:37:14.919960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff0093eb-5178-4705-b174-86cbd0818d04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f6880d820>]}
[0m01:37:14.920931 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:14.925123 [info ] [MainThread]: 
[0m01:37:14.927166 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:37:14.929477 [info ] [MainThread]: 
[0m01:37:14.932804 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:37:14.936343 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:37:14.968564 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:37:14.970005 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:37:14.971073 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:37:17.047680 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:37:17.049040 [debug] [ThreadPool]: SQL status: OK in 2.078 seconds
[0m01:37:17.972784 [debug] [ThreadPool]: On list_schemas: Close
[0m01:37:18.537142 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m01:37:18.552618 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:18.554006 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:37:18.555079 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:37:18.556072 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:37:20.733831 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:37:20.735260 [debug] [ThreadPool]: SQL status: OK in 2.179 seconds
[0m01:37:21.658342 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:37:21.659763 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:37:21.660747 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:37:21.661660 [debug] [ThreadPool]: On list_None_default: Close
[0m01:37:22.168486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff0093eb-5178-4705-b174-86cbd0818d04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f68352840>]}
[0m01:37:22.169776 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:22.170915 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:22.171931 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:37:22.201678 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:37:22.203501 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_ext ............................... [RUN]
[0m01:37:22.206008 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:37:22.207240 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:37:22.225870 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:37:22.230364 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:37:22.297670 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:37:22.299207 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
drop table if exists default.vehicle_ext
[0m01:37:22.300436 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:37:24.523050 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:37:24.524542 [debug] [Thread-1 (]: SQL status: OK in 2.224 seconds
[0m01:37:24.646199 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:37:24.650676 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:24.651890 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:37:24.653025 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3';

LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:37:24.932727 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3';

LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:37:24.934633 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement"), operationHandle=None)
[0m01:37:24.936363 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: ROLLBACK
[0m01:37:24.937513 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:37:24.938541 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m01:37:25.202394 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement
[0m01:37:25.208507 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ff0093eb-5178-4705-b174-86cbd0818d04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f684efad0>]}
[0m01:37:25.209628 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m01:37:25.211280 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_ext ...................... [[31mERROR[0m in 3.00s]
[0m01:37:25.214499 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:37:25.216355 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement.
[0m01:37:25.226233 [debug] [MainThread]: On master: ROLLBACK
[0m01:37:25.227454 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:37:26.861244 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:37:26.862605 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:26.863622 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:37:26.864587 [debug] [MainThread]: On master: ROLLBACK
[0m01:37:26.865711 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:37:26.866573 [debug] [MainThread]: On master: Close
[0m01:37:27.185350 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:37:27.186616 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:37:27.187648 [info ] [MainThread]: 
[0m01:37:27.189898 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.26 seconds (12.26s).
[0m01:37:27.192765 [debug] [MainThread]: Command end result
[0m01:37:27.272535 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:37:27.281184 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:37:27.303653 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:37:27.304845 [info ] [MainThread]: 
[0m01:37:27.307543 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m01:37:27.309962 [info ] [MainThread]: 
[0m01:37:27.312635 [error] [MainThread]:   Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement
[0m01:37:27.315719 [info ] [MainThread]: 
[0m01:37:27.317754 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:37:27.322701 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 14.549219, "process_in_blocks": "0", "process_kernel_time": 1.4375, "process_mem_max_rss": "106460", "process_out_blocks": "0", "process_user_time": 3.09375}
[0m01:37:27.324188 [debug] [MainThread]: Command `dbt run` failed at 01:37:27.323856 after 14.55 seconds
[0m01:37:27.325391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f69642a80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f69641dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3f69642ae0>]}
[0m01:37:27.326332 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:27.327268 [debug] [MainThread]: Flushing usage events
[0m01:37:50.273618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c2174da0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c28cbad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c209b5c0>]}
[0m01:37:50.278224 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:37:50.278646 | 0bd53a27-2cc0-4b66-9f57-08a1eeba1ad5 ==============================
[0m01:37:50.278646 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:37:50.280039 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select vehicle_ext', 'send_anonymous_usage_stats': 'True'}
[0m01:37:50.503420 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:37:50.503938 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:37:50.504318 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:37:50.657714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0bd53a27-2cc0-4b66-9f57-08a1eeba1ad5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c1f42300>]}
[0m01:37:50.658216 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:50.712473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0bd53a27-2cc0-4b66-9f57-08a1eeba1ad5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c37177a0>]}
[0m01:37:50.712967 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:50.713487 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:37:50.950277 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:37:51.180426 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:37:51.180865 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:37:51.220238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0bd53a27-2cc0-4b66-9f57-08a1eeba1ad5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c1812b70>]}
[0m01:37:51.220697 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:51.303056 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:37:51.307742 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:37:51.373931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0bd53a27-2cc0-4b66-9f57-08a1eeba1ad5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c0f96e70>]}
[0m01:37:51.375662 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:51.377395 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 639 macros
[0m01:37:51.379798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0bd53a27-2cc0-4b66-9f57-08a1eeba1ad5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c1b7e5d0>]}
[0m01:37:51.380773 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:51.384981 [info ] [MainThread]: 
[0m01:37:51.387136 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:37:51.389239 [info ] [MainThread]: 
[0m01:37:51.392522 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:37:51.396740 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:37:51.432611 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:37:51.433926 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:37:51.435034 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:37:53.297642 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:37:53.299007 [debug] [ThreadPool]: SQL status: OK in 1.864 seconds
[0m01:37:54.119226 [debug] [ThreadPool]: On list_schemas: Close
[0m01:37:54.749227 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m01:37:54.764733 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:54.765986 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:37:54.767018 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:37:54.767974 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:37:56.983349 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:37:56.984727 [debug] [ThreadPool]: SQL status: OK in 2.217 seconds
[0m01:37:57.708552 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:37:57.709849 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:37:57.710894 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:37:57.711810 [debug] [ThreadPool]: On list_None_default: Close
[0m01:37:58.215994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0bd53a27-2cc0-4b66-9f57-08a1eeba1ad5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c1155a90>]}
[0m01:37:58.217314 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:37:58.218497 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:37:58.219594 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:37:58.248840 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:37:58.251454 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_ext ............................... [RUN]
[0m01:37:58.254110 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:37:58.255279 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:37:58.274576 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:37:58.279148 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:37:58.339660 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:37:58.341201 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
drop table if exists default.vehicle_ext
[0m01:37:58.342655 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:38:00.465743 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:38:00.467186 [debug] [Thread-1 (]: SQL status: OK in 2.125 seconds
[0m01:38:00.589786 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:38:00.594090 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:38:00.595863 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:38:00.597202 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3';

LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:38:00.876077 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE TABLE IF NOT EXISTS vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3';

LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:38:00.877956 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement"), operationHandle=None)
[0m01:38:00.879667 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: ROLLBACK
[0m01:38:00.880955 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:38:00.882014 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m01:38:01.205879 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement
[0m01:38:01.213183 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bd53a27-2cc0-4b66-9f57-08a1eeba1ad5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c289ca10>]}
[0m01:38:01.214655 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m01:38:01.217156 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_ext ...................... [[31mERROR[0m in 2.95s]
[0m01:38:01.220639 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:38:01.222504 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement.
[0m01:38:01.231363 [debug] [MainThread]: On master: ROLLBACK
[0m01:38:01.232404 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:38:02.706804 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:38:02.708889 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:38:02.710369 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:38:02.712045 [debug] [MainThread]: On master: ROLLBACK
[0m01:38:02.713496 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:38:02.714815 [debug] [MainThread]: On master: Close
[0m01:38:02.975692 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:38:02.977539 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:38:02.979082 [info ] [MainThread]: 
[0m01:38:02.981794 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 11.59 seconds (11.59s).
[0m01:38:02.985508 [debug] [MainThread]: Command end result
[0m01:38:03.063952 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:38:03.071438 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:38:03.096469 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:38:03.097462 [info ] [MainThread]: 
[0m01:38:03.099758 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m01:38:03.101804 [info ] [MainThread]: 
[0m01:38:03.103789 [error] [MainThread]:   Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'IF' in create table statement
[0m01:38:03.106122 [info ] [MainThread]: 
[0m01:38:03.108464 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:38:03.113228 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 12.907994, "process_in_blocks": "0", "process_kernel_time": 1.1875, "process_mem_max_rss": "102408", "process_out_blocks": "0", "process_user_time": 2.4375}
[0m01:38:03.114828 [debug] [MainThread]: Command `dbt run` failed at 01:38:03.114449 after 12.91 seconds
[0m01:38:03.116245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c259a060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c55f3170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39c1811940>]}
[0m01:38:03.117226 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:38:03.118094 [debug] [MainThread]: Flushing usage events
[0m01:38:16.831897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac39977ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac3990fe00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac39974cb0>]}
[0m01:38:16.836749 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:38:16.837156 | 3430fd1f-4e05-4716-9dd6-44bd4fadfd53 ==============================
[0m01:38:16.837156 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:38:16.839192 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select vehicle_ext', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:38:17.100587 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:38:17.101109 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:38:17.101491 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:38:17.290990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3430fd1f-4e05-4716-9dd6-44bd4fadfd53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac398cfb60>]}
[0m01:38:17.291862 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:38:17.352592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3430fd1f-4e05-4716-9dd6-44bd4fadfd53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac39429a90>]}
[0m01:38:17.353165 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:38:17.353763 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:38:17.583371 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:38:17.788798 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:38:17.789466 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m01:38:18.727918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3430fd1f-4e05-4716-9dd6-44bd4fadfd53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac38678830>]}
[0m01:38:18.729328 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:38:19.004877 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:38:19.017674 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:38:19.094295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3430fd1f-4e05-4716-9dd6-44bd4fadfd53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac389d1a90>]}
[0m01:38:19.095469 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:38:19.096499 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 639 macros
[0m01:38:19.098704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3430fd1f-4e05-4716-9dd6-44bd4fadfd53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac386782f0>]}
[0m01:38:19.099682 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:38:19.104626 [info ] [MainThread]: 
[0m01:38:19.107004 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:38:19.108720 [info ] [MainThread]: 
[0m01:38:19.110767 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:38:19.114192 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:38:19.149927 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:38:19.151264 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:38:19.152266 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:38:21.251984 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:38:21.253341 [debug] [ThreadPool]: SQL status: OK in 2.101 seconds
[0m01:38:22.074018 [debug] [ThreadPool]: On list_schemas: Close
[0m01:38:22.703847 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m01:38:22.719376 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:38:22.720739 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:38:22.721779 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:38:22.722742 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:38:24.836249 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:38:24.837615 [debug] [ThreadPool]: SQL status: OK in 2.115 seconds
[0m01:38:25.734733 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:38:25.736139 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:38:25.737180 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:38:25.738132 [debug] [ThreadPool]: On list_None_default: Close
[0m01:38:26.372755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3430fd1f-4e05-4716-9dd6-44bd4fadfd53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac38660950>]}
[0m01:38:26.374237 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:38:26.375372 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:38:26.376363 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:38:26.404149 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:38:26.405955 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_ext ............................... [RUN]
[0m01:38:26.408509 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:38:26.409717 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:38:26.427034 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:38:26.431474 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:38:26.498934 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:38:26.500468 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
drop table if exists default.vehicle_ext
[0m01:38:26.501663 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:38:28.625151 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:38:28.626611 [debug] [Thread-1 (]: SQL status: OK in 2.125 seconds
[0m01:38:28.748164 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:38:28.752608 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:38:28.753818 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:38:28.754961 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3';

LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:38:29.035474 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3';

LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:38:29.037354 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement"), operationHandle=None)
[0m01:38:29.039271 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: ROLLBACK
[0m01:38:29.040415 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:38:29.041410 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m01:38:29.359691 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement
[0m01:38:29.365616 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3430fd1f-4e05-4716-9dd6-44bd4fadfd53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac387deae0>]}
[0m01:38:29.366690 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m01:38:29.368357 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_ext ...................... [[31mERROR[0m in 2.95s]
[0m01:38:29.371641 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:38:29.373632 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement.
[0m01:38:29.384070 [debug] [MainThread]: On master: ROLLBACK
[0m01:38:29.385273 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:38:31.014778 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:38:31.016144 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:38:31.017144 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:38:31.018085 [debug] [MainThread]: On master: ROLLBACK
[0m01:38:31.018983 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:38:31.019831 [debug] [MainThread]: On master: Close
[0m01:38:31.287613 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:38:31.288837 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:38:31.289832 [info ] [MainThread]: 
[0m01:38:31.292498 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.18 seconds (12.18s).
[0m01:38:31.295394 [debug] [MainThread]: Command end result
[0m01:38:31.369556 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:38:31.374456 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:38:31.392272 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:38:31.393390 [info ] [MainThread]: 
[0m01:38:31.395661 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m01:38:31.397560 [info ] [MainThread]: 
[0m01:38:31.400010 [error] [MainThread]:   Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement
[0m01:38:31.402699 [info ] [MainThread]: 
[0m01:38:31.404612 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:38:31.409271 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 14.652225, "process_in_blocks": "0", "process_kernel_time": 1.25, "process_mem_max_rss": "106428", "process_out_blocks": "0", "process_user_time": 3.390625}
[0m01:38:31.411328 [debug] [MainThread]: Command `dbt run` failed at 01:38:31.410844 after 14.65 seconds
[0m01:38:31.412671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac3a0d44a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac3a0c7050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac38005ca0>]}
[0m01:38:31.413627 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:38:31.414439 [debug] [MainThread]: Flushing usage events
[0m01:40:52.495950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cce005f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cdbd33e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cc3daf30>]}
[0m01:40:52.502467 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:40:52.503008 | f8a5990d-bb7f-4b60-af65-1ab0682ab027 ==============================
[0m01:40:52.503008 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:40:52.504685 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select vehicle_ext', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m01:40:52.722130 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:40:52.722658 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:40:52.723020 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:40:52.876427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f8a5990d-bb7f-4b60-af65-1ab0682ab027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cba30ce0>]}
[0m01:40:52.876895 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:40:52.930218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f8a5990d-bb7f-4b60-af65-1ab0682ab027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cc04b3b0>]}
[0m01:40:52.930711 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:40:52.931210 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:40:53.158063 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:40:53.368730 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m01:40:53.369417 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m01:40:54.104071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f8a5990d-bb7f-4b60-af65-1ab0682ab027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cb2bea50>]}
[0m01:40:54.105464 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:40:54.358969 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:40:54.373602 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:40:54.450289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f8a5990d-bb7f-4b60-af65-1ab0682ab027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cb47cec0>]}
[0m01:40:54.451682 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:40:54.453281 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 639 macros
[0m01:40:54.455640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8a5990d-bb7f-4b60-af65-1ab0682ab027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cb2b7dd0>]}
[0m01:40:54.456643 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:40:54.460718 [info ] [MainThread]: 
[0m01:40:54.462767 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:40:54.465291 [info ] [MainThread]: 
[0m01:40:54.468402 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:40:54.472055 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:40:54.504669 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:40:54.506145 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:40:54.507260 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:40:56.389249 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:40:56.390601 [debug] [ThreadPool]: SQL status: OK in 1.883 seconds
[0m01:40:57.313055 [debug] [ThreadPool]: On list_schemas: Close
[0m01:40:57.879431 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m01:40:57.894174 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:40:57.895414 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:40:57.896459 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:40:57.897434 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:40:59.904137 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:40:59.905504 [debug] [ThreadPool]: SQL status: OK in 2.008 seconds
[0m01:41:00.795955 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:41:00.797383 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:41:00.798500 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:41:00.799399 [debug] [ThreadPool]: On list_None_default: Close
[0m01:41:01.343070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8a5990d-bb7f-4b60-af65-1ab0682ab027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cb2d30e0>]}
[0m01:41:01.344358 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:41:01.345495 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:41:01.346519 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:41:01.377160 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:41:01.378967 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_ext ............................... [RUN]
[0m01:41:01.381556 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:41:01.382791 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:41:01.401718 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:41:01.406270 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:41:01.477778 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:41:01.479310 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
drop table if exists default.vehicle_ext
[0m01:41:01.480498 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:41:03.476658 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:41:03.478144 [debug] [Thread-1 (]: SQL status: OK in 1.998 seconds
[0m01:41:03.635503 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:41:03.640162 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:41:03.641623 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:41:03.642825 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      %sql
CREATE TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3';

LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:41:03.888724 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      %sql
CREATE TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3';

LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:41:03.890631 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near '%' 'sql' 'CREATE' in create table statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 16:6 cannot recognize input near '%' 'sql' 'CREATE' in create table statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near '%' 'sql' 'CREATE' in create table statement"), operationHandle=None)
[0m01:41:03.892514 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: ROLLBACK
[0m01:41:03.893639 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:41:03.894714 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m01:41:04.190057 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near '%' 'sql' 'CREATE' in create table statement
[0m01:41:04.196203 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8a5990d-bb7f-4b60-af65-1ab0682ab027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cb41f710>]}
[0m01:41:04.197524 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m01:41:04.199287 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_ext ...................... [[31mERROR[0m in 2.81s]
[0m01:41:04.202653 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:41:04.204483 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near '%' 'sql' 'CREATE' in create table statement.
[0m01:41:04.214514 [debug] [MainThread]: On master: ROLLBACK
[0m01:41:04.215471 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:41:05.830941 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:41:05.832330 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:41:05.833336 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:41:05.834227 [debug] [MainThread]: On master: ROLLBACK
[0m01:41:05.835838 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:41:05.837110 [debug] [MainThread]: On master: Close
[0m01:41:06.117640 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:41:06.119195 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:41:06.120564 [info ] [MainThread]: 
[0m01:41:06.123245 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 11.65 seconds (11.65s).
[0m01:41:06.126351 [debug] [MainThread]: Command end result
[0m01:41:06.208740 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:41:06.216461 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:41:06.240346 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:41:06.241521 [info ] [MainThread]: 
[0m01:41:06.243847 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m01:41:06.245929 [info ] [MainThread]: 
[0m01:41:06.247755 [error] [MainThread]:   Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near '%' 'sql' 'CREATE' in create table statement
[0m01:41:06.250869 [info ] [MainThread]: 
[0m01:41:06.253366 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:41:06.258247 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 13.835441, "process_in_blocks": "0", "process_kernel_time": 1.015625, "process_mem_max_rss": "106480", "process_out_blocks": "0", "process_user_time": 3.53125}
[0m01:41:06.259709 [debug] [MainThread]: Command `dbt run` failed at 01:41:06.259458 after 13.84 seconds
[0m01:41:06.260914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cc01e120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cccdb380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28cccda150>]}
[0m01:41:06.261877 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:41:06.262739 [debug] [MainThread]: Flushing usage events
[0m01:45:44.347198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb00bb04e60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb00a57e3f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb00aff1d60>]}
[0m01:45:44.351873 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:45:44.352341 | be62df34-12e4-4bda-ac3d-5a6147c24e92 ==============================
[0m01:45:44.352341 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:45:44.353992 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile --select vehicle_ext', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:45:44.580377 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:45:44.580947 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:45:44.581383 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:45:44.747702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'be62df34-12e4-4bda-ac3d-5a6147c24e92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb009d1b4d0>]}
[0m01:45:44.748188 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:45:44.805468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'be62df34-12e4-4bda-ac3d-5a6147c24e92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb009ca6e40>]}
[0m01:45:44.805972 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:45:44.806488 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:45:45.031577 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:45:45.265144 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m01:45:45.265759 [debug] [MainThread]: Partial parsing: added file: dbt_spark_emr_app://macros/factory.sql
[0m01:45:45.266214 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m01:45:45.290802 [error] [MainThread]: Encountered an error:
Compilation Error
  expected token '(', got 'end of statement block'
    line 1
      {% macro create_ext_table %}
[0m01:45:45.296574 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": false, "command_wall_clock_time": 1.0256113, "process_in_blocks": "0", "process_kernel_time": 1.109375, "process_mem_max_rss": "98700", "process_out_blocks": "0", "process_user_time": 1.78125}
[0m01:45:45.297439 [debug] [MainThread]: Command `dbt compile` failed at 01:45:45.297208 after 1.03 seconds
[0m01:45:45.298034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb00a6161e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb00919b320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb00919b3e0>]}
[0m01:45:45.298522 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:45:45.298938 [debug] [MainThread]: Flushing usage events
[0m01:46:32.368532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b8c86dac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b8cf10140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b8c86dc70>]}
[0m01:46:32.373344 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:46:32.373787 | b0f4448b-ab3c-4039-93e9-dd33c440171e ==============================
[0m01:46:32.373787 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:46:32.375568 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt compile --select vehicle_ext', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:46:32.592197 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:46:32.592734 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:46:32.593106 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:46:32.745427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b0f4448b-ab3c-4039-93e9-dd33c440171e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b8c132240>]}
[0m01:46:32.745907 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:46:32.799447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b0f4448b-ab3c-4039-93e9-dd33c440171e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b8c776420>]}
[0m01:46:32.799929 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:46:32.800494 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:46:33.031708 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:46:33.245528 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m01:46:33.246127 [debug] [MainThread]: Partial parsing: added file: dbt_spark_emr_app://macros/factory.sql
[0m01:46:33.246570 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m01:46:34.006646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b0f4448b-ab3c-4039-93e9-dd33c440171e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b8bb676b0>]}
[0m01:46:34.008107 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:46:34.276438 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:46:34.294271 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:46:34.367211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b0f4448b-ab3c-4039-93e9-dd33c440171e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b8be70980>]}
[0m01:46:34.368436 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:46:34.369703 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 640 macros
[0m01:46:34.372239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0f4448b-ab3c-4039-93e9-dd33c440171e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b8a9c6180>]}
[0m01:46:34.373255 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:46:34.377501 [info ] [MainThread]: 
[0m01:46:34.379710 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:46:34.381571 [info ] [MainThread]: 
[0m01:46:34.383655 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:46:34.402160 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m01:46:34.448927 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:46:34.451192 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:46:34.452985 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:46:34.454562 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:46:36.564016 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:46:36.565387 [debug] [ThreadPool]: SQL status: OK in 2.111 seconds
[0m01:46:37.363016 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:46:37.364484 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:46:37.365568 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:46:37.366573 [debug] [ThreadPool]: On list_None_default: Close
[0m01:46:37.932516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0f4448b-ab3c-4039-93e9-dd33c440171e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b8c4a17f0>]}
[0m01:46:37.934020 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:46:37.965952 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:46:37.968126 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:46:37.969461 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:46:37.990070 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:46:37.994533 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:46:37.996669 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:46:38.005013 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:46:38.006696 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:46:38.009904 [debug] [MainThread]: Command end result
[0m01:46:38.098993 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:46:38.106522 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:46:38.126017 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:46:38.126763 [info ] [MainThread]: Compiled node 'vehicle_ext' is:


CREATE TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'



-- LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
[0m01:46:38.137523 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 5.836388, "process_in_blocks": "0", "process_kernel_time": 1.28125, "process_mem_max_rss": "105472", "process_out_blocks": "0", "process_user_time": 3.015625}
[0m01:46:38.139752 [debug] [MainThread]: Command `dbt compile` succeeded at 01:46:38.139231 after 5.84 seconds
[0m01:46:38.141418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b8c86cb60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b90283170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b8bb7cbc0>]}
[0m01:46:38.142811 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:46:38.144170 [debug] [MainThread]: Flushing usage events
[0m01:46:58.692290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28bdf820f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28be234f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28be1945f0>]}
[0m01:46:58.697933 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:46:58.698343 | d49d267e-2df7-4ce6-addd-593805ea5848 ==============================
[0m01:46:58.698343 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:46:58.700036 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select vehicle_ext', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:46:58.955002 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:46:58.955538 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:46:58.955906 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:46:59.108109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd49d267e-2df7-4ce6-addd-593805ea5848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28be20aa20>]}
[0m01:46:59.108589 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:46:59.163039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd49d267e-2df7-4ce6-addd-593805ea5848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28be15c5c0>]}
[0m01:46:59.163546 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:46:59.164064 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:46:59.428181 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:46:59.657727 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:46:59.658169 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:46:59.700430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd49d267e-2df7-4ce6-addd-593805ea5848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28bd3bf140>]}
[0m01:46:59.700918 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:46:59.947085 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:46:59.962671 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:47:00.049537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd49d267e-2df7-4ce6-addd-593805ea5848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28bd3bf3b0>]}
[0m01:47:00.051229 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:47:00.053126 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 640 macros
[0m01:47:00.055381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd49d267e-2df7-4ce6-addd-593805ea5848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28bd461be0>]}
[0m01:47:00.056801 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:47:00.061631 [info ] [MainThread]: 
[0m01:47:00.063211 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:47:00.065059 [info ] [MainThread]: 
[0m01:47:00.067691 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:47:00.070441 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:47:00.107863 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:47:00.109837 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:47:00.111192 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:47:02.060983 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:47:02.062314 [debug] [ThreadPool]: SQL status: OK in 1.951 seconds
[0m01:47:02.987025 [debug] [ThreadPool]: On list_schemas: Close
[0m01:47:03.615070 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m01:47:03.630053 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:47:03.631250 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:47:03.632270 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:47:03.633231 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:47:05.882587 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:47:05.884128 [debug] [ThreadPool]: SQL status: OK in 2.251 seconds
[0m01:47:06.610003 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:47:06.611423 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:47:06.612479 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:47:06.613423 [debug] [ThreadPool]: On list_None_default: Close
[0m01:47:07.181870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd49d267e-2df7-4ce6-addd-593805ea5848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28bfa02d20>]}
[0m01:47:07.183135 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:47:07.184309 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:47:07.185341 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:47:07.214199 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:47:07.216047 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_ext ............................... [RUN]
[0m01:47:07.218428 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:47:07.219646 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:47:07.243027 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:47:07.247535 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:47:07.308875 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:47:07.310463 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
drop table if exists default.vehicle_ext
[0m01:47:07.311676 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:47:09.536998 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:47:09.538471 [debug] [Thread-1 (]: SQL status: OK in 2.227 seconds
[0m01:47:09.660005 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:47:09.664131 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:47:09.665417 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:47:09.666578 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      

CREATE TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'



-- LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:47:09.946245 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      

CREATE TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'



-- LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:47:09.948155 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 18:0 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 18:0 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 18:0 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement"), operationHandle=None)
[0m01:47:09.950059 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: ROLLBACK
[0m01:47:09.951175 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:47:09.952216 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m01:47:10.271061 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 18:0 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement
[0m01:47:10.277204 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd49d267e-2df7-4ce6-addd-593805ea5848', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28be95c7a0>]}
[0m01:47:10.278448 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m01:47:10.280371 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_ext ...................... [[31mERROR[0m in 3.05s]
[0m01:47:10.283564 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:47:10.285439 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 18:0 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement.
[0m01:47:10.295236 [debug] [MainThread]: On master: ROLLBACK
[0m01:47:10.296426 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:47:11.925391 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:47:11.926735 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:47:11.927676 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:47:11.928569 [debug] [MainThread]: On master: ROLLBACK
[0m01:47:11.929553 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:47:11.930376 [debug] [MainThread]: On master: Close
[0m01:47:12.199224 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:47:12.200455 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:47:12.201521 [info ] [MainThread]: 
[0m01:47:12.203915 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.13 seconds (12.13s).
[0m01:47:12.206804 [debug] [MainThread]: Command end result
[0m01:47:12.281588 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:47:12.291828 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:47:12.310154 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:47:12.311321 [info ] [MainThread]: 
[0m01:47:12.313957 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m01:47:12.316550 [info ] [MainThread]: 
[0m01:47:12.319241 [error] [MainThread]:   Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 18:0 cannot recognize input near 'CREATE' 'TABLE' 'vehicle_poc' in create table statement
[0m01:47:12.322013 [info ] [MainThread]: 
[0m01:47:12.323985 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:47:12.328780 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 13.704955, "process_in_blocks": "0", "process_kernel_time": 1.3125, "process_mem_max_rss": "102392", "process_out_blocks": "0", "process_user_time": 2.5}
[0m01:47:12.330332 [debug] [MainThread]: Command `dbt run` failed at 01:47:12.329949 after 13.71 seconds
[0m01:47:12.331533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28bfcd9580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28bdc0fb00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28be3e08f0>]}
[0m01:47:12.332584 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:47:12.333481 [debug] [MainThread]: Flushing usage events
[0m01:57:10.530619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82ab1e7c80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82ab557f80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82ab61a600>]}
[0m01:57:10.535340 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:57:10.535766 | 4aaecdc3-2638-4587-9bf7-9d76b92d03cc ==============================
[0m01:57:10.535766 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:57:10.536858 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile --select vehicle_ext', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m01:57:10.756521 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:57:10.757038 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:57:10.757398 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:57:10.919931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4aaecdc3-2638-4587-9bf7-9d76b92d03cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82aabd2240>]}
[0m01:57:10.920452 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:10.974152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4aaecdc3-2638-4587-9bf7-9d76b92d03cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82aabf8e60>]}
[0m01:57:10.974667 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:10.975327 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:57:11.204098 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:57:11.425413 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m01:57:11.426069 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/schema.yml
[0m01:57:11.426529 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m01:57:12.251913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4aaecdc3-2638-4587-9bf7-9d76b92d03cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82a9e24950>]}
[0m01:57:12.253200 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:12.505158 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:57:12.518150 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:57:12.588319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4aaecdc3-2638-4587-9bf7-9d76b92d03cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82a9fe7980>]}
[0m01:57:12.589537 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:12.590617 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 640 macros
[0m01:57:12.592297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4aaecdc3-2638-4587-9bf7-9d76b92d03cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82aa3861e0>]}
[0m01:57:12.593286 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:12.597304 [info ] [MainThread]: 
[0m01:57:12.599012 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:57:12.600385 [info ] [MainThread]: 
[0m01:57:12.602011 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:57:12.624735 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m01:57:12.660136 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:57:12.661642 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:57:12.662672 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:57:12.663608 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:57:14.721336 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:57:14.722827 [debug] [ThreadPool]: SQL status: OK in 2.059 seconds
[0m01:57:15.577940 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:57:15.579360 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:57:15.580468 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:57:15.581467 [debug] [ThreadPool]: On list_None_default: Close
[0m01:57:16.157560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4aaecdc3-2638-4587-9bf7-9d76b92d03cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82ac902db0>]}
[0m01:57:16.158834 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:16.189938 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:57:16.192106 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:57:16.193494 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:57:16.211093 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:57:16.215706 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:57:16.217833 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:57:16.224824 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:57:16.226071 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:57:16.228047 [debug] [MainThread]: Command end result
[0m01:57:16.304294 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:57:16.311577 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:57:16.337035 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:57:16.338453 [info ] [MainThread]: Compiled node 'vehicle_ext' is:
CREATE EXTERNAL TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
   'separatorChar' = ','
   )
STORED AS TEXTFILE
LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';

-- LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
[0m01:57:16.349550 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 5.891096, "process_in_blocks": "0", "process_kernel_time": 1.3125, "process_mem_max_rss": "104948", "process_out_blocks": "0", "process_user_time": 2.859375}
[0m01:57:16.351057 [debug] [MainThread]: Command `dbt compile` succeeded at 01:57:16.350728 after 5.89 seconds
[0m01:57:16.352171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82aab7d160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82ab03e1e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82aa3848c0>]}
[0m01:57:16.353396 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:16.354634 [debug] [MainThread]: Flushing usage events
[0m01:57:28.093591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8e380560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8cbe6990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8d05f4a0>]}
[0m01:57:28.098236 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 01:57:28.098650 | 5c86c246-c543-47f5-9418-27eab605b1be ==============================
[0m01:57:28.098650 [info ] [MainThread]: Running with dbt=1.9.2
[0m01:57:28.100114 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select vehicle_ext', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:57:28.309872 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:57:28.310408 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:57:28.310775 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:57:28.467770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5c86c246-c543-47f5-9418-27eab605b1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8bb78ce0>]}
[0m01:57:28.468255 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:28.522086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5c86c246-c543-47f5-9418-27eab605b1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8bdf4ec0>]}
[0m01:57:28.522573 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:28.523083 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m01:57:28.743056 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m01:57:28.975693 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:57:28.976180 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:57:29.019863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5c86c246-c543-47f5-9418-27eab605b1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8bb20a70>]}
[0m01:57:29.020354 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:29.120375 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:57:29.140450 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:57:29.220281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5c86c246-c543-47f5-9418-27eab605b1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8bb21370>]}
[0m01:57:29.222004 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:29.223621 [info ] [MainThread]: Found 3 models, 4 data tests, 1 source, 640 macros
[0m01:57:29.226116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c86c246-c543-47f5-9418-27eab605b1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8bc03710>]}
[0m01:57:29.227139 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:29.231419 [info ] [MainThread]: 
[0m01:57:29.233345 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:57:29.234804 [info ] [MainThread]: 
[0m01:57:29.236371 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m01:57:29.239834 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m01:57:29.275898 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m01:57:29.277397 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m01:57:29.278508 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:57:31.140946 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:57:31.142290 [debug] [ThreadPool]: SQL status: OK in 1.864 seconds
[0m01:57:31.963008 [debug] [ThreadPool]: On list_schemas: Close
[0m01:57:32.559767 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m01:57:32.576054 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m01:57:32.577399 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m01:57:32.578456 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m01:57:32.579361 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m01:57:34.792633 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m01:57:34.794191 [debug] [ThreadPool]: SQL status: OK in 2.215 seconds
[0m01:57:35.646730 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m01:57:35.648158 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m01:57:35.649212 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m01:57:35.650164 [debug] [ThreadPool]: On list_None_default: Close
[0m01:57:36.134418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c86c246-c543-47f5-9418-27eab605b1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8bcc20c0>]}
[0m01:57:36.135981 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:36.137161 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:57:36.138171 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:57:36.165966 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m01:57:36.167725 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_ext ............................... [RUN]
[0m01:57:36.170261 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m01:57:36.171467 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m01:57:36.190423 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:57:36.194406 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m01:57:36.256878 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:57:36.258368 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
drop table if exists default.vehicle_ext
[0m01:57:36.259576 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:57:38.250498 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m01:57:38.251968 [debug] [Thread-1 (]: SQL status: OK in 1.992 seconds
[0m01:57:38.377265 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_ext"
[0m01:57:38.381697 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m01:57:38.382927 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m01:57:38.384094 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE EXTERNAL TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
   'separatorChar' = ','
   )
STORED AS TEXTFILE
LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';

-- LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:57:38.685280 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
    
        create table default.vehicle_ext
      
      
      
      
      
      
      
      

      as
      CREATE EXTERNAL TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
   'separatorChar' = ','
   )
STORED AS TEXTFILE
LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';

-- LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
  
[0m01:57:38.688083 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement"), operationHandle=None)
[0m01:57:38.689815 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: ROLLBACK
[0m01:57:38.691238 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m01:57:38.692829 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m01:57:39.011140 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement
[0m01:57:39.017533 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5c86c246-c543-47f5-9418-27eab605b1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8ae9e840>]}
[0m01:57:39.018594 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m01:57:39.020334 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_ext ...................... [[31mERROR[0m in 2.84s]
[0m01:57:39.023127 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m01:57:39.025036 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement.
[0m01:57:39.035885 [debug] [MainThread]: On master: ROLLBACK
[0m01:57:39.037266 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:57:40.663480 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:57:40.664920 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m01:57:40.665884 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m01:57:40.666803 [debug] [MainThread]: On master: ROLLBACK
[0m01:57:40.667723 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m01:57:40.668542 [debug] [MainThread]: On master: Close
[0m01:57:40.936508 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:57:40.937535 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m01:57:40.938622 [info ] [MainThread]: 
[0m01:57:40.940448 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 11.70 seconds (11.70s).
[0m01:57:40.943675 [debug] [MainThread]: Command end result
[0m01:57:41.020818 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m01:57:41.027898 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m01:57:41.046915 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m01:57:41.048036 [info ] [MainThread]: 
[0m01:57:41.050125 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m01:57:41.052044 [info ] [MainThread]: 
[0m01:57:41.054024 [error] [MainThread]:   Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create table statement
[0m01:57:41.057251 [info ] [MainThread]: 
[0m01:57:41.059697 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m01:57:41.065682 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 13.040827, "process_in_blocks": "0", "process_kernel_time": 1.234375, "process_mem_max_rss": "102400", "process_out_blocks": "0", "process_user_time": 2.40625}
[0m01:57:41.067923 [debug] [MainThread]: Command `dbt run` failed at 01:57:41.067391 after 13.04 seconds
[0m01:57:41.069840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8bcae240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8dcd3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e8ae42a50>]}
[0m01:57:41.071280 [debug] [MainThread]: An error was encountered while trying to send an event
[0m01:57:41.072103 [debug] [MainThread]: Flushing usage events
[0m02:07:27.892108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f907ec6c410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f907d03ac00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f907d91ad20>]}
[0m02:07:27.901125 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 02:07:27.901629 | 068cef7d-bd92-47ea-8ef0-0e5085e19e19 ==============================
[0m02:07:27.901629 [info ] [MainThread]: Running with dbt=1.9.2
[0m02:07:27.903199 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt deps', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m02:07:28.069343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '068cef7d-bd92-47ea-8ef0-0e5085e19e19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f907ccc7710>]}
[0m02:07:28.070018 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:07:28.303147 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m02:07:28.307624 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m02:07:28.311020 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.5225749, "process_in_blocks": "0", "process_kernel_time": 0.90625, "process_mem_max_rss": "86444", "process_out_blocks": "0", "process_user_time": 1.59375}
[0m02:07:28.311676 [debug] [MainThread]: Command `dbt deps` succeeded at 02:07:28.311541 after 0.52 seconds
[0m02:07:28.312140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f907da61b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f907cbcc770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f907cd41820>]}
[0m02:07:28.312685 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:07:28.313010 [debug] [MainThread]: Flushing usage events
[0m02:16:56.256876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3eb810830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3ed156150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3ebea9c40>]}
[0m02:16:56.266957 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 02:16:56.268340 | fe69f31e-6c3c-4f53-856c-176bf6916770 ==============================
[0m02:16:56.268340 [info ] [MainThread]: Running with dbt=1.9.2
[0m02:16:56.270705 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt deps', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m02:16:56.718470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fe69f31e-6c3c-4f53-856c-176bf6916770', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3eb810830>]}
[0m02:16:56.719774 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:16:56.806777 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-flib8c0i'
[0m02:16:56.809281 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m02:16:57.518658 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m02:16:57.520998 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json
[0m02:16:58.322887 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json 200
[0m02:16:58.353939 [info ] [MainThread]: Updating lock file in file path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/package-lock.yml
[0m02:16:58.377921 [debug] [MainThread]: Set downloads directory='/tmp/dbt-downloads-28liznxg'
[0m02:16:58.384878 [info ] [MainThread]: Installing dbt-labs/dbt_external_tables
[0m02:17:00.269251 [info ] [MainThread]: Installed from version 0.11.1
[0m02:17:00.271412 [info ] [MainThread]: Up to date!
[0m02:17:00.273231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'fe69f31e-6c3c-4f53-856c-176bf6916770', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3eb550800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3eb552300>]}
[0m02:17:00.274220 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:17:00.283776 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 4.218084, "process_in_blocks": "0", "process_kernel_time": 2.265625, "process_mem_max_rss": "87172", "process_out_blocks": "0", "process_user_time": 2.671875}
[0m02:17:00.285411 [debug] [MainThread]: Command `dbt deps` succeeded at 02:17:00.285039 after 4.22 seconds
[0m02:17:00.286613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3eb1ed040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3ebd27290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3eb550800>]}
[0m02:17:00.287616 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:17:00.288535 [debug] [MainThread]: Flushing usage events
[0m02:17:33.510846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd035017e00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd034fac290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd034fae3f0>]}
[0m02:17:33.516845 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 02:17:33.517262 | 4d109943-fac7-4611-96ea-3a59c1089051 ==============================
[0m02:17:33.517262 [info ] [MainThread]: Running with dbt=1.9.2
[0m02:17:33.518942 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select vehicle_ext', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m02:17:33.758864 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:17:33.759400 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:17:33.759823 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:17:33.964714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4d109943-fac7-4611-96ea-3a59c1089051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd034a64ec0>]}
[0m02:17:33.965269 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:17:34.021072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4d109943-fac7-4611-96ea-3a59c1089051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd034ad2810>]}
[0m02:17:34.021606 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:17:34.022141 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m02:17:34.266463 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m02:17:34.524390 [debug] [MainThread]: Partial parsing enabled: 54 files deleted, 0 files added, 2 files changed.
[0m02:17:34.527559 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/schema.yml
[0m02:17:34.529690 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/mutually_exclusive_ranges.sql
[0m02:17:34.531336 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_emr_app://models/example/my_second_dbt_model.sql
[0m02:17:34.532954 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/union.sql
[0m02:17:34.534505 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/nullcheck.sql
[0m02:17:34.536040 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/at_least_one.sql
[0m02:17:34.537568 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/get_tables_by_pattern_sql.sql
[0m02:17:34.538623 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/jinja_helpers/pretty_time.sql
[0m02:17:34.539624 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/get_table_types_sql.sql
[0m02:17:34.540510 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/web/get_url_host.sql
[0m02:17:34.541400 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/get_relations_by_prefix.sql
[0m02:17:34.542332 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/generate_series.sql
[0m02:17:34.543257 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/safe_divide.sql
[0m02:17:34.544158 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/web/get_url_path.sql
[0m02:17:34.545055 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/surrogate_key.sql
[0m02:17:34.546121 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/unpivot.sql
[0m02:17:34.546986 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/get_single_value.sql
[0m02:17:34.547834 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/get_query_results_as_dict.sql
[0m02:17:34.548671 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/jinja_helpers/log_info.sql
[0m02:17:34.549772 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/recency.sql
[0m02:17:34.550796 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/jinja_helpers/_is_relation.sql
[0m02:17:34.552116 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/pivot.sql
[0m02:17:34.553490 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/cardinality_equality.sql
[0m02:17:34.554989 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/safe_add.sql
[0m02:17:34.556504 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/get_column_values.sql
[0m02:17:34.557873 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/fewer_rows_than.sql
[0m02:17:34.559191 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/haversine_distance.sql
[0m02:17:34.560161 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/sequential_values.sql
[0m02:17:34.561097 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/deduplicate.sql
[0m02:17:34.562012 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/width_bucket.sql
[0m02:17:34.562907 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/get_filtered_columns_in_relation.sql
[0m02:17:34.563767 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/get_tables_by_prefix_sql.sql
[0m02:17:34.564569 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/safe_subtract.sql
[0m02:17:34.565399 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/accepted_range.sql
[0m02:17:34.566221 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/relationships_where.sql
[0m02:17:34.567065 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/date_spine.sql
[0m02:17:34.567895 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/unique_combination_of_columns.sql
[0m02:17:34.568723 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/not_accepted_values.sql
[0m02:17:34.569547 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/equal_rowcount.sql
[0m02:17:34.570381 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_emr_app://models/example/my_first_dbt_model.sql
[0m02:17:34.571223 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/jinja_helpers/pretty_log_format.sql
[0m02:17:34.572108 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/not_null_proportion.sql
[0m02:17:34.572965 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/generate_surrogate_key.sql
[0m02:17:34.573815 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/nullcheck_table.sql
[0m02:17:34.574675 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/not_constant.sql
[0m02:17:34.575500 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/get_relations_by_pattern.sql
[0m02:17:34.576326 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/equality.sql
[0m02:17:34.577188 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/jinja_helpers/_is_ephemeral.sql
[0m02:17:34.578013 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/groupby.sql
[0m02:17:34.579005 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/web/get_url_parameter.sql
[0m02:17:34.579865 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/jinja_helpers/slugify.sql
[0m02:17:34.580954 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/expression_is_true.sql
[0m02:17:34.581924 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/generic_tests/not_empty_string.sql
[0m02:17:34.582796 [debug] [MainThread]: Partial parsing: deleted file: dbt_utils://macros/sql/star.sql
[0m02:17:34.583917 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m02:17:35.361029 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m02:17:35.399688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4d109943-fac7-4611-96ea-3a59c1089051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd032c78da0>]}
[0m02:17:35.401357 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:17:35.591915 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m02:17:35.612126 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m02:17:35.687693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4d109943-fac7-4611-96ea-3a59c1089051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd033ff7290>]}
[0m02:17:35.688589 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:17:35.689231 [info ] [MainThread]: Found 1 model, 1 source, 525 macros
[0m02:17:35.691089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d109943-fac7-4611-96ea-3a59c1089051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd033ec6930>]}
[0m02:17:35.692032 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:17:35.696748 [info ] [MainThread]: 
[0m02:17:35.698919 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:17:35.700774 [info ] [MainThread]: 
[0m02:17:35.703500 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m02:17:35.708401 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m02:17:35.738340 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m02:17:35.739804 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m02:17:35.740895 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:17:37.792176 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m02:17:37.793538 [debug] [ThreadPool]: SQL status: OK in 2.053 seconds
[0m02:17:38.716782 [debug] [ThreadPool]: On list_schemas: Close
[0m02:17:39.330067 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m02:17:39.346385 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:17:39.347658 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m02:17:39.348737 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m02:17:39.349701 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:17:41.581154 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m02:17:41.582442 [debug] [ThreadPool]: SQL status: OK in 2.233 seconds
[0m02:17:42.348366 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m02:17:42.349871 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m02:17:42.350921 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m02:17:42.351829 [debug] [ThreadPool]: On list_None_default: Close
[0m02:17:42.914231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d109943-fac7-4611-96ea-3a59c1089051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd032c4d0a0>]}
[0m02:17:42.915449 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:17:42.916621 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:17:42.917593 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:17:42.945632 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m02:17:42.947445 [info ] [Thread-1 (]: 1 of 1 START sql view model default.vehicle_ext ................................ [RUN]
[0m02:17:42.949819 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m02:17:42.951041 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m02:17:42.966934 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m02:17:42.970642 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m02:17:43.061099 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_ext"
[0m02:17:43.065091 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m02:17:43.066295 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m02:17:43.067169 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
create or replace view default.vehicle_ext
  
  
  as
    CREATE EXTERNAL TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
   'separatorChar' = ','
   )
STORED AS TEXTFILE
LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';

-- LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;

[0m02:17:43.067941 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m02:17:44.792469 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
create or replace view default.vehicle_ext
  
  
  as
    CREATE EXTERNAL TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
)
COMMENT 'Vehicle raw data, considered as source from s3'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
   'separatorChar' = ','
   )
STORED AS TEXTFILE
LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';

-- LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;

[0m02:17:44.794252 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 6:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 6:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 6:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement"), operationHandle=None)
[0m02:17:44.796741 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: ROLLBACK
[0m02:17:44.798052 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m02:17:44.798919 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m02:17:45.065744 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 6:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement
[0m02:17:45.069939 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4d109943-fac7-4611-96ea-3a59c1089051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0323524b0>]}
[0m02:17:45.070832 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m02:17:45.072004 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.vehicle_ext ....................... [[31mERROR[0m in 2.12s]
[0m02:17:45.075421 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m02:17:45.077230 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 6:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement.
[0m02:17:45.084963 [debug] [MainThread]: On master: ROLLBACK
[0m02:17:45.086076 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:17:46.588366 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m02:17:46.589393 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:17:46.590156 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:17:46.590828 [debug] [MainThread]: On master: ROLLBACK
[0m02:17:46.591449 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m02:17:46.592078 [debug] [MainThread]: On master: Close
[0m02:17:46.840055 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:17:46.840991 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m02:17:46.841651 [info ] [MainThread]: 
[0m02:17:46.843119 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 11.14 seconds (11.14s).
[0m02:17:46.844879 [debug] [MainThread]: Command end result
[0m02:17:46.924801 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m02:17:46.933696 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m02:17:46.947938 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m02:17:46.949063 [info ] [MainThread]: 
[0m02:17:46.950791 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m02:17:46.952500 [info ] [MainThread]: 
[0m02:17:46.954413 [error] [MainThread]:   Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 6:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement
[0m02:17:46.956813 [info ] [MainThread]: 
[0m02:17:46.958278 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m02:17:46.962827 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 13.520777, "process_in_blocks": "0", "process_kernel_time": 1.53125, "process_mem_max_rss": "104376", "process_out_blocks": "0", "process_user_time": 3.015625}
[0m02:17:46.963997 [debug] [MainThread]: Command `dbt run` failed at 02:17:46.963760 after 13.52 seconds
[0m02:17:46.965158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd032c87bf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0341d2a80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd03235e180>]}
[0m02:17:46.966106 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:17:46.966917 [debug] [MainThread]: Flushing usage events
[0m02:34:10.080811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2346c4ade0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23481815e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2346b6e000>]}
[0m02:34:10.084801 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 02:34:10.085146 | 2bd40d32-2f65-437e-8bae-fda3f45e4091 ==============================
[0m02:34:10.085146 [info ] [MainThread]: Running with dbt=1.9.2
[0m02:34:10.085957 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run-operation create_external_table', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m02:34:10.268421 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:34:10.268798 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:34:10.269073 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:34:10.401907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2bd40d32-2f65-437e-8bae-fda3f45e4091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23468e17c0>]}
[0m02:34:10.402296 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:34:10.445594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2bd40d32-2f65-437e-8bae-fda3f45e4091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23464c4860>]}
[0m02:34:10.446016 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:34:10.446409 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m02:34:10.611047 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m02:34:10.752739 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m02:34:10.753247 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m02:34:10.753573 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m02:34:10.930859 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m02:34:10.941217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2bd40d32-2f65-437e-8bae-fda3f45e4091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2346056db0>]}
[0m02:34:10.941642 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:34:10.995509 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m02:34:11.000513 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m02:34:11.034641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2bd40d32-2f65-437e-8bae-fda3f45e4091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2345fc45f0>]}
[0m02:34:11.035034 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:34:11.035349 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m02:34:11.036272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2bd40d32-2f65-437e-8bae-fda3f45e4091', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23460e8920>]}
[0m02:34:11.036566 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:34:11.036962 [debug] [MainThread]: Acquiring new spark connection 'macro_create_external_table'
[0m02:34:11.037233 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:34:11.037461 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:34:11.043909 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m02:34:11.045260 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 1.019252, "process_in_blocks": "0", "process_kernel_time": 0.9375, "process_mem_max_rss": "102092", "process_out_blocks": "0", "process_user_time": 1.515625}
[0m02:34:11.045955 [debug] [MainThread]: Command `dbt run-operation` succeeded at 02:34:11.045826 after 1.02 seconds
[0m02:34:11.046395 [debug] [MainThread]: Connection 'macro_create_external_table' was properly closed.
[0m02:34:11.046804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2346a73f80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23461bb8f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2345fc6f90>]}
[0m02:34:11.047074 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:34:11.047306 [debug] [MainThread]: Flushing usage events
[0m02:39:09.927619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc414dafd70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4150cb5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc414dafcb0>]}
[0m02:39:09.949123 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 02:39:09.949455 | 6c4ae6bf-9051-46d3-a6a2-cee3016b5071 ==============================
[0m02:39:09.949455 [info ] [MainThread]: Running with dbt=1.9.2
[0m02:39:09.950481 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile --select vehicle_ext', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m02:39:15.308127 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:39:15.309177 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:39:15.310038 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:39:15.562803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6c4ae6bf-9051-46d3-a6a2-cee3016b5071', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc414bf82f0>]}
[0m02:39:15.563290 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:15.616042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6c4ae6bf-9051-46d3-a6a2-cee3016b5071', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4168df740>]}
[0m02:39:15.616560 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:15.617026 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m02:39:16.102619 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m02:39:18.001936 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m02:39:18.003356 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m02:39:18.004479 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m02:39:18.273672 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m02:39:18.285530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6c4ae6bf-9051-46d3-a6a2-cee3016b5071', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4143ef3e0>]}
[0m02:39:18.286113 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:18.355753 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m02:39:18.384163 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m02:39:18.671464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6c4ae6bf-9051-46d3-a6a2-cee3016b5071', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc414309bb0>]}
[0m02:39:18.671844 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:18.672160 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m02:39:18.673768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c4ae6bf-9051-46d3-a6a2-cee3016b5071', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc414149520>]}
[0m02:39:18.674917 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:18.679240 [info ] [MainThread]: 
[0m02:39:18.681386 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:39:18.682621 [info ] [MainThread]: 
[0m02:39:18.683815 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m02:39:18.685476 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m02:39:18.700598 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:39:18.701322 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m02:39:18.701714 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m02:39:18.702030 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:39:20.441789 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m02:39:20.442898 [debug] [ThreadPool]: SQL status: OK in 1.741 seconds
[0m02:39:21.166698 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m02:39:21.167743 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m02:39:21.168551 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m02:39:21.169272 [debug] [ThreadPool]: On list_None_default: Close
[0m02:39:21.719953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c4ae6bf-9051-46d3-a6a2-cee3016b5071', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc412de00e0>]}
[0m02:39:21.720917 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:21.758669 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m02:39:21.759171 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m02:39:21.759495 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m02:39:21.767936 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m02:39:21.769752 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m02:39:21.770547 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m02:39:21.773233 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:39:21.773621 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m02:39:21.774205 [debug] [MainThread]: Command end result
[0m02:39:21.796056 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m02:39:21.799033 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m02:39:21.805685 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m02:39:21.806187 [info ] [MainThread]: Compiled node 'vehicle_ext' is:



    CREATE EXTERNAL TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
    )
        COMMENT 'Vehicle raw data, considered as source from s3'
        ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
        WITH SERDEPROPERTIES (
        'separatorChar' = ','
    )
    STORED AS TEXTFILE
    LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';



-- LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;
[0m02:39:21.819132 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 11.998076, "process_in_blocks": "0", "process_kernel_time": 1.65625, "process_mem_max_rss": "103228", "process_out_blocks": "0", "process_user_time": 2.109375}
[0m02:39:21.819960 [debug] [MainThread]: Command `dbt compile` succeeded at 02:39:21.819771 after 12.00 seconds
[0m02:39:21.820587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4185c3140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc414dafc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4150cb5c0>]}
[0m02:39:21.821230 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:21.821724 [debug] [MainThread]: Flushing usage events
[0m02:39:42.288897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d20b2dd30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d2055aba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d20af5d60>]}
[0m02:39:42.292388 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 02:39:42.292708 | 6d6aca47-cdbd-4767-a4bf-741094ab9951 ==============================
[0m02:39:42.292708 [info ] [MainThread]: Running with dbt=1.9.2
[0m02:39:42.293792 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select vehicle_ext', 'send_anonymous_usage_stats': 'True'}
[0m02:39:42.454196 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:39:42.454612 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:39:42.454898 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:39:42.588733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d6aca47-cdbd-4767-a4bf-741094ab9951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d2037fbf0>]}
[0m02:39:42.589136 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:42.632842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d6aca47-cdbd-4767-a4bf-741094ab9951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d20af5d60>]}
[0m02:39:42.633236 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:42.633621 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m02:39:42.792146 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m02:39:42.923001 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:39:42.923417 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:39:42.927980 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m02:39:42.951958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6d6aca47-cdbd-4767-a4bf-741094ab9951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d1f9658e0>]}
[0m02:39:42.952463 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:43.006246 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m02:39:43.010026 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m02:39:43.048693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6d6aca47-cdbd-4767-a4bf-741094ab9951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d1f6d8620>]}
[0m02:39:43.049243 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:43.049685 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m02:39:43.050688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d6aca47-cdbd-4767-a4bf-741094ab9951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d1f981a90>]}
[0m02:39:43.050968 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:43.052289 [info ] [MainThread]: 
[0m02:39:43.053183 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:39:43.053932 [info ] [MainThread]: 
[0m02:39:43.054896 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m02:39:43.056095 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m02:39:43.064543 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m02:39:43.064968 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m02:39:43.065236 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:39:44.807287 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m02:39:44.808431 [debug] [ThreadPool]: SQL status: OK in 1.743 seconds
[0m02:39:45.536544 [debug] [ThreadPool]: On list_schemas: Close
[0m02:39:46.036765 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m02:39:46.048340 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:39:46.049330 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m02:39:46.050083 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m02:39:46.050754 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:39:48.047737 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m02:39:48.048837 [debug] [ThreadPool]: SQL status: OK in 1.998 seconds
[0m02:39:48.774889 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m02:39:48.775971 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m02:39:48.776810 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m02:39:48.777589 [debug] [ThreadPool]: On list_None_default: Close
[0m02:39:49.262152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d6aca47-cdbd-4767-a4bf-741094ab9951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d1fa5ec00>]}
[0m02:39:49.263219 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:49.264108 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:39:49.264894 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:39:49.274962 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m02:39:49.275838 [info ] [Thread-1 (]: 1 of 1 START sql view model default.vehicle_ext ................................ [RUN]
[0m02:39:49.277127 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m02:39:49.277570 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m02:39:49.295238 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m02:39:49.298404 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m02:39:49.341092 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_ext"
[0m02:39:49.343029 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m02:39:49.343496 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m02:39:49.343917 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
create or replace view default.vehicle_ext
  
  
  as
    


    CREATE EXTERNAL TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
    )
        COMMENT 'Vehicle raw data, considered as source from s3'
        ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
        WITH SERDEPROPERTIES (
        'separatorChar' = ','
    )
    STORED AS TEXTFILE
    LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';



-- LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;

[0m02:39:49.344273 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m02:39:50.996456 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */
create or replace view default.vehicle_ext
  
  
  as
    


    CREATE EXTERNAL TABLE vehicle_poc.vehicle_ext (
    VIN string, County string, City string, State string, Postal Code integer, 
    Model Year integer, Make string, Model string, Electric_Vehicle_Type string, 
    Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
    Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
    Electric_Utility string, Census_Tract long
    )
        COMMENT 'Vehicle raw data, considered as source from s3'
        ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
        WITH SERDEPROPERTIES (
        'separatorChar' = ','
    )
    STORED AS TEXTFILE
    LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv';



-- LOAD DATA INPATH 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv' OVERWRITE INTO TABLE vehicle_poc.vehicle_ext;

[0m02:39:50.998078 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 9:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 9:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 9:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement"), operationHandle=None)
[0m02:39:50.999585 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: ROLLBACK
[0m02:39:51.000460 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m02:39:51.001214 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m02:39:51.398980 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 9:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement
[0m02:39:51.409274 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6d6aca47-cdbd-4767-a4bf-741094ab9951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d1f6d9a90>]}
[0m02:39:51.410174 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m02:39:51.411441 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.vehicle_ext ....................... [[31mERROR[0m in 2.12s]
[0m02:39:51.413959 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m02:39:51.415282 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 9:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement.
[0m02:39:51.420160 [debug] [MainThread]: On master: ROLLBACK
[0m02:39:51.421060 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:39:52.731390 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m02:39:52.732540 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:39:52.733343 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:39:52.734092 [debug] [MainThread]: On master: ROLLBACK
[0m02:39:52.734838 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m02:39:52.735554 [debug] [MainThread]: On master: Close
[0m02:39:52.978085 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:39:52.978525 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m02:39:52.978798 [info ] [MainThread]: 
[0m02:39:52.979648 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 9.92 seconds (9.92s).
[0m02:39:52.980493 [debug] [MainThread]: Command end result
[0m02:39:53.008313 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m02:39:53.010972 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m02:39:53.018368 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m02:39:53.018962 [info ] [MainThread]: 
[0m02:39:53.019980 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m02:39:53.020936 [info ] [MainThread]: 
[0m02:39:53.021748 [error] [MainThread]:   Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 9:4 cannot recognize input near 'CREATE' 'EXTERNAL' 'TABLE' in create view statement
[0m02:39:53.022825 [info ] [MainThread]: 
[0m02:39:53.023576 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m02:39:53.025386 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 10.795293, "process_in_blocks": "0", "process_kernel_time": 0.953125, "process_mem_max_rss": "101060", "process_out_blocks": "0", "process_user_time": 1.53125}
[0m02:39:53.025826 [debug] [MainThread]: Command `dbt run` failed at 02:39:53.025714 after 10.80 seconds
[0m02:39:53.026225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d2019dfa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d2055ac00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6d1f6c3fb0>]}
[0m02:39:53.026530 [debug] [MainThread]: An error was encountered while trying to send an event
[0m02:39:53.026810 [debug] [MainThread]: Flushing usage events
[0m03:08:36.025096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c53a4d7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c53a4c8c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c53a4dc10>]}
[0m03:08:36.028800 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:08:36.029184 | 1ad337f4-afc8-4863-be61-7068f610afde ==============================
[0m03:08:36.029184 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:08:36.030218 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run-operation stage_external_sources', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m03:08:36.208533 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:08:36.208955 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:08:36.209250 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:08:36.343441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1ad337f4-afc8-4863-be61-7068f610afde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c532d8a10>]}
[0m03:08:36.343874 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:08:36.388978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1ad337f4-afc8-4863-be61-7068f610afde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c53684890>]}
[0m03:08:36.389414 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:08:36.389824 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:08:36.561457 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:08:36.624534 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m03:08:36.626668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1ad337f4-afc8-4863-be61-7068f610afde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c532305c0>]}
[0m03:08:36.627108 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:08:37.729890 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:08:37.747708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ad337f4-afc8-4863-be61-7068f610afde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c53a75eb0>]}
[0m03:08:37.748176 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:08:37.838587 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:08:37.844386 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:08:37.888567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ad337f4-afc8-4863-be61-7068f610afde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c53a92540>]}
[0m03:08:37.889865 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:08:37.891313 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m03:08:37.893225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ad337f4-afc8-4863-be61-7068f610afde', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c5304e000>]}
[0m03:08:37.894373 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:08:37.895334 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:08:37.896049 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:08:37.896574 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:08:37.921002 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:08:37.927406 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:08:37.945789 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:08:37.947316 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:08:37.948687 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:08:39.742431 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:08:39.743871 [debug] [MainThread]: SQL status: OK in 1.795 seconds
[0m03:08:40.225029 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:08:40.252806 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:08:40.256564 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:08:40.257105 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:08:40.756174 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:08:40.756648 [debug] [MainThread]: SQL status: OK in 0.499 seconds
[0m03:08:40.758342 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:08:40.759875 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:08:40.760824 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:08:40.761089 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:08:41.243438 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:08:41.244482 [debug] [MainThread]: SQL status: OK in 0.483 seconds
[0m03:08:41.247120 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:08:41.249332 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN None,            C...  
[0m03:08:41.251752 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:08:41.252661 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN None,
            County None,
            City None,
            State None,
            Postal_Code None,
            Model_Year None,
            Make None,
            Model None,
            Electric_Vehicle_Type None,
            Clean_Alternative_Fuel_Vehicle None,
            Electric_Range None,
            Base_MSRP None,
            Legislative_District None,
            DOL_Vehicle_ID None,
            Vehicle_Location None,
            Electric_Utility None,
            Cnsus_Tract None
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:08:41.611268 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN None,
            County None,
            City None,
            State None,
            Postal_Code None,
            Model_Year None,
            Make None,
            Model None,
            Electric_Vehicle_Type None,
            Clean_Alternative_Fuel_Vehicle None,
            Electric_Range None,
            Base_MSRP None,
            Legislative_District None,
            DOL_Vehicle_ID None,
            Vehicle_Location None,
            Electric_Utility None,
            Cnsus_Tract None
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:08:41.612899 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:16 cannot recognize input near 'None' ',' 'County' in column type:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type"), operationHandle=None)
[0m03:08:41.614637 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:08:41.615350 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type
[0m03:08:41.615690 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:08:41.615974 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:08:41.616239 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:08:41.861445 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type
[0m03:08:41.904691 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:16 cannot recognize input near 'None' ',' 'County' in column type:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type

[0m03:08:41.919996 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:08:41.922484 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 5.953218, "process_in_blocks": "0", "process_kernel_time": 0.984375, "process_mem_max_rss": "102336", "process_out_blocks": "0", "process_user_time": 2.59375}
[0m03:08:41.923515 [debug] [MainThread]: Command `dbt run-operation` failed at 03:08:41.923254 after 5.95 seconds
[0m03:08:41.924157 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:08:41.924566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c53a4c710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c52f5e270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c53379af0>]}
[0m03:08:41.925382 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:08:41.926057 [debug] [MainThread]: Flushing usage events
[0m03:11:47.742057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f806cda30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f80cb5a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f806cd880>]}
[0m03:11:47.745464 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:11:47.745778 | 8048c0d8-ea95-41d7-accd-e38f980e4bc4 ==============================
[0m03:11:47.745778 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:11:47.746863 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run-operation stage_external_sources', 'send_anonymous_usage_stats': 'True'}
[0m03:11:47.909238 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:11:47.909620 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:11:47.909894 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:11:48.043483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8048c0d8-ea95-41d7-accd-e38f980e4bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f813b7aa0>]}
[0m03:11:48.043877 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:11:48.086732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8048c0d8-ea95-41d7-accd-e38f980e4bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7fffa810>]}
[0m03:11:48.087142 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:11:48.087544 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:11:48.246869 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:11:48.386058 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 0 files changed.
[0m03:11:48.386474 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m03:11:48.386744 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_emr_app://macros/factory.sql
[0m03:11:48.420616 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:11:48.439510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8048c0d8-ea95-41d7-accd-e38f980e4bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f806ce420>]}
[0m03:11:48.439916 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:11:48.486801 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:11:48.491099 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:11:48.507443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8048c0d8-ea95-41d7-accd-e38f980e4bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7fc476e0>]}
[0m03:11:48.507882 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:11:48.508208 [info ] [MainThread]: Found 1 source, 523 macros
[0m03:11:48.508921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8048c0d8-ea95-41d7-accd-e38f980e4bc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f8003e9c0>]}
[0m03:11:48.509198 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:11:48.509647 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:11:48.509959 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:11:48.510216 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:11:48.524997 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:11:48.530760 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:11:48.544529 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:11:48.544982 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:11:48.545270 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:11:50.303163 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:11:50.304311 [debug] [MainThread]: SQL status: OK in 1.759 seconds
[0m03:11:50.790286 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:11:50.835472 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:11:50.840793 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:11:50.841698 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:11:51.327795 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:11:51.328886 [debug] [MainThread]: SQL status: OK in 0.487 seconds
[0m03:11:51.331521 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:11:51.333715 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:11:51.336198 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:11:51.337042 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:11:51.824733 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:11:51.825895 [debug] [MainThread]: SQL status: OK in 0.488 seconds
[0m03:11:51.828450 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:11:51.830589 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN None,            C...  
[0m03:11:51.833077 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:11:51.833976 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN None,
            County None,
            City None,
            State None,
            Postal_Code None,
            Model_Year None,
            Make None,
            Model None,
            Electric_Vehicle_Type None,
            Clean_Alternative_Fuel_Vehicle None,
            Electric_Range None,
            Base_MSRP None,
            Legislative_District None,
            DOL_Vehicle_ID None,
            Vehicle_Location None,
            Electric_Utility None,
            Cnsus_Tract None
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:11:52.079816 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN None,
            County None,
            City None,
            State None,
            Postal_Code None,
            Model_Year None,
            Make None,
            Model None,
            Electric_Vehicle_Type None,
            Clean_Alternative_Fuel_Vehicle None,
            Electric_Range None,
            Base_MSRP None,
            Legislative_District None,
            DOL_Vehicle_ID None,
            Vehicle_Location None,
            Electric_Utility None,
            Cnsus_Tract None
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:11:52.081228 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:16 cannot recognize input near 'None' ',' 'County' in column type:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type"), operationHandle=None)
[0m03:11:52.082439 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:11:52.083249 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type
[0m03:11:52.084122 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:11:52.084905 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:11:52.085656 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:11:52.331902 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type
[0m03:11:52.348702 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:16 cannot recognize input near 'None' ',' 'County' in column type:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type

[0m03:11:52.363913 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:11:52.365491 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.6749988, "process_in_blocks": "0", "process_kernel_time": 1.046875, "process_mem_max_rss": "99368", "process_out_blocks": "0", "process_user_time": 1.390625}
[0m03:11:52.366019 [debug] [MainThread]: Command `dbt run-operation` failed at 03:11:52.365904 after 4.68 seconds
[0m03:11:52.366393 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:11:52.366750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f806cd490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f7fc47830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f8003c890>]}
[0m03:11:52.367040 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:11:52.367303 [debug] [MainThread]: Flushing usage events
[0m03:14:06.501302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc9fa94b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc9e8b9940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc9e4dede0>]}
[0m03:14:06.504676 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:14:06.504992 | 36f4c61b-ea3f-470d-9eb0-372c3176c74f ==============================
[0m03:14:06.504992 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:14:06.505976 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run-operation stage_external_sources', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m03:14:06.671493 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:14:06.671875 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:14:06.672155 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:14:06.800646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '36f4c61b-ea3f-470d-9eb0-372c3176c74f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc9d542ae0>]}
[0m03:14:06.801085 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:14:06.846481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '36f4c61b-ea3f-470d-9eb0-372c3176c74f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc9e93a9f0>]}
[0m03:14:06.846948 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:14:06.847346 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:14:07.015302 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:14:07.136539 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:14:07.137114 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/schema.yml
[0m03:14:07.253958 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:14:07.267348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '36f4c61b-ea3f-470d-9eb0-372c3176c74f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc9d1b6690>]}
[0m03:14:07.267819 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:14:07.309939 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:14:07.313820 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:14:07.329913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '36f4c61b-ea3f-470d-9eb0-372c3176c74f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc9d18f050>]}
[0m03:14:07.330304 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:14:07.330608 [info ] [MainThread]: Found 1 source, 523 macros
[0m03:14:07.331486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36f4c61b-ea3f-470d-9eb0-372c3176c74f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc9d0a3380>]}
[0m03:14:07.331769 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:14:07.332168 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:14:07.332433 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:14:07.332677 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:14:07.345391 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:14:07.351432 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:14:07.360483 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:14:07.360987 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:14:07.361314 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:14:09.123183 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:14:09.124372 [debug] [MainThread]: SQL status: OK in 1.763 seconds
[0m03:14:09.623578 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:14:09.667248 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:14:09.673619 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:14:09.674661 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:14:10.162783 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:14:10.163852 [debug] [MainThread]: SQL status: OK in 0.488 seconds
[0m03:14:10.166531 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:14:10.168701 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:14:10.171165 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:14:10.172031 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:14:10.786165 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:14:10.787358 [debug] [MainThread]: SQL status: OK in 0.614 seconds
[0m03:14:10.789977 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:14:10.792130 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN None,            C...  
[0m03:14:10.794582 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:14:10.795474 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN None,
            County None,
            City None,
            State None,
            Postal_Code None,
            Model_Year None,
            Make None,
            Model None,
            Electric_Vehicle_Type None,
            Clean_Alternative_Fuel_Vehicle None,
            Electric_Range None,
            Base_MSRP None,
            Legislative_District None,
            DOL_Vehicle_ID None,
            Vehicle_Location None,
            Electric_Utility None,
            Cnsus_Tract None
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:14:11.044675 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN None,
            County None,
            City None,
            State None,
            Postal_Code None,
            Model_Year None,
            Make None,
            Model None,
            Electric_Vehicle_Type None,
            Clean_Alternative_Fuel_Vehicle None,
            Electric_Range None,
            Base_MSRP None,
            Legislative_District None,
            DOL_Vehicle_ID None,
            Vehicle_Location None,
            Electric_Utility None,
            Cnsus_Tract None
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:14:11.046112 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:16 cannot recognize input near 'None' ',' 'County' in column type:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type"), operationHandle=None)
[0m03:14:11.047850 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:14:11.048715 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type
[0m03:14:11.049629 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:14:11.050344 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:14:11.051016 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:14:11.329431 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type
[0m03:14:11.347507 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:16 cannot recognize input near 'None' ',' 'County' in column type:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type

[0m03:14:11.363419 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:14:11.366290 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.91687, "process_in_blocks": "0", "process_kernel_time": 0.796875, "process_mem_max_rss": "100676", "process_out_blocks": "0", "process_user_time": 1.671875}
[0m03:14:11.367342 [debug] [MainThread]: Command `dbt run-operation` failed at 03:14:11.367183 after 4.92 seconds
[0m03:14:11.367836 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:14:11.368301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc9e03ac00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc9e93b0e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcc9e939eb0>]}
[0m03:14:11.368608 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:14:11.368805 [debug] [MainThread]: Flushing usage events
[0m03:16:19.259261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29032036b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f290340b860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f290340a960>]}
[0m03:16:19.263447 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:16:19.263795 | f5cde6e1-7717-4f2c-9043-760f42a57d17 ==============================
[0m03:16:19.263795 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:16:19.264943 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt run-operation stage_external_sources', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m03:16:19.515297 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:16:19.515771 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:16:19.516067 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:16:19.665923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f5cde6e1-7717-4f2c-9043-760f42a57d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2902ec70e0>]}
[0m03:16:19.666385 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:16:19.710428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f5cde6e1-7717-4f2c-9043-760f42a57d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f290274db80>]}
[0m03:16:19.710915 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:16:19.712248 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:16:19.877948 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:16:19.880117 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m03:16:19.880999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f5cde6e1-7717-4f2c-9043-760f42a57d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29026a2840>]}
[0m03:16:19.881236 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:16:20.908841 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:16:20.992409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f5cde6e1-7717-4f2c-9043-760f42a57d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f290233d430>]}
[0m03:16:20.993675 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:16:21.067196 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:16:21.075367 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:16:21.110464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f5cde6e1-7717-4f2c-9043-760f42a57d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29023989b0>]}
[0m03:16:21.110984 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:16:21.111352 [info ] [MainThread]: Found 1 source, 523 macros
[0m03:16:21.112139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5cde6e1-7717-4f2c-9043-760f42a57d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29023746b0>]}
[0m03:16:21.112433 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:16:21.112876 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:16:21.113188 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:16:21.113618 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:16:21.131038 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:16:21.137624 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:16:21.151122 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:16:21.151983 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:16:21.152675 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:16:22.902807 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:16:22.904091 [debug] [MainThread]: SQL status: OK in 1.751 seconds
[0m03:16:23.390052 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:16:23.433208 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:16:23.439517 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:16:23.440414 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:16:23.926284 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:16:23.926739 [debug] [MainThread]: SQL status: OK in 0.486 seconds
[0m03:16:23.929133 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:16:23.931243 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:16:23.933763 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:16:23.934635 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:16:24.424129 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:16:24.425451 [debug] [MainThread]: SQL status: OK in 0.490 seconds
[0m03:16:24.429312 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:16:24.430845 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN None,            C...  
[0m03:16:24.432394 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:16:24.433308 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN None,
            County None,
            City None,
            State None,
            Postal_Code None,
            Model_Year None,
            Make None,
            Model None,
            Electric_Vehicle_Type None,
            Clean_Alternative_Fuel_Vehicle None,
            Electric_Range None,
            Base_MSRP None,
            Legislative_District None,
            DOL_Vehicle_ID None,
            Vehicle_Location None,
            Electric_Utility None,
            Cnsus_Tract None
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:16:24.796716 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN None,
            County None,
            City None,
            State None,
            Postal_Code None,
            Model_Year None,
            Make None,
            Model None,
            Electric_Vehicle_Type None,
            Clean_Alternative_Fuel_Vehicle None,
            Electric_Range None,
            Base_MSRP None,
            Legislative_District None,
            DOL_Vehicle_ID None,
            Vehicle_Location None,
            Electric_Utility None,
            Cnsus_Tract None
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:16:24.798263 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:16 cannot recognize input near 'None' ',' 'County' in column type:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type"), operationHandle=None)
[0m03:16:24.799796 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:16:24.800884 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type
[0m03:16:24.801801 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:16:24.802781 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:16:24.803573 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:16:25.082003 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type
[0m03:16:25.096120 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:16 cannot recognize input near 'None' ',' 'County' in column type:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'County' in column type

[0m03:16:25.109167 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:16:25.111992 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 5.9060874, "process_in_blocks": "0", "process_kernel_time": 1.1875, "process_mem_max_rss": "99796", "process_out_blocks": "0", "process_user_time": 2.734375}
[0m03:16:25.112773 [debug] [MainThread]: Command `dbt run-operation` failed at 03:16:25.112608 after 5.91 seconds
[0m03:16:25.113341 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:16:25.113836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f290340a960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29022e1b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29024a0500>]}
[0m03:16:25.114311 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:16:25.114769 [debug] [MainThread]: Flushing usage events
[0m03:18:28.886336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec833ea80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec837bdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec83798b0>]}
[0m03:18:28.889948 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:18:28.890286 | 8462608e-c3a9-4310-a793-c1c95622bffa ==============================
[0m03:18:28.890286 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:18:28.891360 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run-operation stage_external_sources', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m03:18:29.051799 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:18:29.052214 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:18:29.052499 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:18:29.179928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8462608e-c3a9-4310-a793-c1c95622bffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec7e0d190>]}
[0m03:18:29.180333 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:18:29.223459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8462608e-c3a9-4310-a793-c1c95622bffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec7dcb830>]}
[0m03:18:29.223868 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:18:29.224271 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:18:29.387157 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:18:29.512617 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:18:29.513226 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/schema.yml
[0m03:18:29.629267 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:18:29.642647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8462608e-c3a9-4310-a793-c1c95622bffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec75f3a70>]}
[0m03:18:29.643037 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:18:29.684657 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:18:29.688511 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:18:29.705227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8462608e-c3a9-4310-a793-c1c95622bffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec91a53d0>]}
[0m03:18:29.705792 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:18:29.706113 [info ] [MainThread]: Found 1 source, 523 macros
[0m03:18:29.706840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8462608e-c3a9-4310-a793-c1c95622bffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec719eab0>]}
[0m03:18:29.707098 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:18:29.707524 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:18:29.707808 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:18:29.708055 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:18:29.721703 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:18:29.727642 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:18:29.735700 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:18:29.736068 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:18:29.736337 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:18:31.509466 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:18:31.510656 [debug] [MainThread]: SQL status: OK in 1.774 seconds
[0m03:18:32.005254 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:18:32.034859 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:18:32.039277 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:18:32.040141 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:18:32.540124 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:18:32.541158 [debug] [MainThread]: SQL status: OK in 0.500 seconds
[0m03:18:32.543567 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:18:32.545565 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:18:32.548943 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:18:32.549746 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:18:33.056591 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:18:33.057577 [debug] [MainThread]: SQL status: OK in 0.507 seconds
[0m03:18:33.059978 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:18:33.062036 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN None,            C...  
[0m03:18:33.064354 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:18:33.065187 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN None,
            Country None,
            City None,
            State None,
            Postal_Code None,
            Model_Year None,
            Make None,
            Model None,
            Electric_Vehicle_Type None,
            Clean_Alternative_Fuel_Vehicle None,
            Electric_Range None,
            Base_MSRP None,
            Legislative_District None,
            DOL_Vehicle_ID None,
            Vehicle_Location None,
            Electric_Utility None,
            Cnsus_Tract None
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:18:33.319886 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN None,
            Country None,
            City None,
            State None,
            Postal_Code None,
            Model_Year None,
            Make None,
            Model None,
            Electric_Vehicle_Type None,
            Clean_Alternative_Fuel_Vehicle None,
            Electric_Range None,
            Base_MSRP None,
            Legislative_District None,
            DOL_Vehicle_ID None,
            Vehicle_Location None,
            Electric_Utility None,
            Cnsus_Tract None
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:18:33.321247 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'Country' in column type:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:16 cannot recognize input near 'None' ',' 'Country' in column type:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'Country' in column type"), operationHandle=None)
[0m03:18:33.322733 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:18:33.323427 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'Country' in column type
[0m03:18:33.323754 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:18:33.324029 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:18:33.324306 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:18:33.576927 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'Country' in column type
[0m03:18:33.594411 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'Country' in column type:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:16 cannot recognize input near 'None' ',' 'Country' in column type:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'Country' in column type"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'Country' in column type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:16 cannot recognize input near 'None' ',' 'Country' in column type

[0m03:18:33.610579 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:18:33.613417 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.778903, "process_in_blocks": "0", "process_kernel_time": 0.890625, "process_mem_max_rss": "100668", "process_out_blocks": "0", "process_user_time": 1.53125}
[0m03:18:33.614441 [debug] [MainThread]: Command `dbt run-operation` failed at 03:18:33.614210 after 4.78 seconds
[0m03:18:33.615205 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:18:33.615976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec8102630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec7e571d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ec8acb290>]}
[0m03:18:33.616665 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:18:33.617270 [debug] [MainThread]: Flushing usage events
[0m03:19:44.628031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00b01f71d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00b04d2030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00b08ae0f0>]}
[0m03:19:44.631455 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:19:44.631801 | 99acfb6d-c563-4ef9-b2a5-04289142f937 ==============================
[0m03:19:44.631801 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:19:44.632910 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run-operation stage_external_sources', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m03:19:44.794749 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:19:44.795126 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:19:44.795399 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:19:44.923526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '99acfb6d-c563-4ef9-b2a5-04289142f937', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00b05a8140>]}
[0m03:19:44.923923 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:19:44.967470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '99acfb6d-c563-4ef9-b2a5-04289142f937', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00af820800>]}
[0m03:19:44.967908 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:19:44.968310 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:19:45.138289 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:19:45.264238 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:19:45.264868 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/schema.yml
[0m03:19:45.383153 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:19:45.394730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '99acfb6d-c563-4ef9-b2a5-04289142f937', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00af367320>]}
[0m03:19:45.395149 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:19:45.438553 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:19:45.442233 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:19:45.458556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '99acfb6d-c563-4ef9-b2a5-04289142f937', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00af30ad50>]}
[0m03:19:45.458954 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:19:45.459272 [info ] [MainThread]: Found 1 source, 523 macros
[0m03:19:45.460198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '99acfb6d-c563-4ef9-b2a5-04289142f937', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00af00f380>]}
[0m03:19:45.460476 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:19:45.460916 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:19:45.461215 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:19:45.461467 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:19:45.475102 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:19:45.480624 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:19:45.489856 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:19:45.490320 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:19:45.490607 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:19:47.265261 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:19:47.266501 [debug] [MainThread]: SQL status: OK in 1.776 seconds
[0m03:19:47.760574 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:19:47.806654 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:19:47.813162 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:19:47.813547 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:19:48.393892 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:19:48.394939 [debug] [MainThread]: SQL status: OK in 0.581 seconds
[0m03:19:48.397383 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:19:48.399431 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:19:48.401909 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:19:48.402713 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:19:48.896944 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:19:48.898037 [debug] [MainThread]: SQL status: OK in 0.495 seconds
[0m03:19:48.900754 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:19:48.902925 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN varchar(255),     ...  
[0m03:19:48.905397 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:19:48.906310 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN varchar(255),
            Country varchar(255),
            City varchar(255),
            State varchar(255),
            Postal_Code int,
            Model_Year int,
            Make varchar(255),
            Model varchar(255),
            Electric_Vehicle_Type varchar(255),
            Clean_Alternative_Fuel_Vehicle varchar(255),
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location varchar(255),
            Electric_Utility varchar(255),
            Cnsus_Tract varchar(255)
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:19:49.159032 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN varchar(255),
            Country varchar(255),
            City varchar(255),
            State varchar(255),
            Postal_Code int,
            Model_Year int,
            Make varchar(255),
            Model varchar(255),
            Electric_Vehicle_Type varchar(255),
            Clean_Alternative_Fuel_Vehicle varchar(255),
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location varchar(255),
            Electric_Utility varchar(255),
            Cnsus_Tract varchar(255)
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:19:49.160487 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)
[0m03:19:49.162216 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:19:49.163084 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:19:49.164000 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:19:49.164826 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:19:49.165612 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:19:49.427176 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:19:49.441978 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

[0m03:19:49.452787 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:19:49.456762 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.8819084, "process_in_blocks": "0", "process_kernel_time": 0.921875, "process_mem_max_rss": "100684", "process_out_blocks": "0", "process_user_time": 1.53125}
[0m03:19:49.458661 [debug] [MainThread]: Command `dbt run-operation` failed at 03:19:49.458214 after 4.88 seconds
[0m03:19:49.460052 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:19:49.461384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00b063da00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00afb8cec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00b08af320>]}
[0m03:19:49.462596 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:19:49.463627 [debug] [MainThread]: Flushing usage events
[0m03:21:01.219724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97adedd8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97adedca10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97adedde80>]}
[0m03:21:01.223572 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:21:01.223899 | 5016e03b-fc5b-4eb1-8784-1f5fb14b9830 ==============================
[0m03:21:01.223899 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:21:01.224935 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run-operation stage_external_sources', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m03:21:01.383959 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:21:01.384338 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:21:01.384603 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:21:01.518409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5016e03b-fc5b-4eb1-8784-1f5fb14b9830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97ade4e150>]}
[0m03:21:01.518792 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:21:01.561903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5016e03b-fc5b-4eb1-8784-1f5fb14b9830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97ae896060>]}
[0m03:21:01.562319 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:21:01.562715 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:21:01.721716 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:21:01.846524 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:21:01.847112 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/schema.yml
[0m03:21:01.890279 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:21:01.908903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5016e03b-fc5b-4eb1-8784-1f5fb14b9830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97ad7a2000>]}
[0m03:21:01.909313 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:21:01.955013 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:21:01.958701 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:21:01.974350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5016e03b-fc5b-4eb1-8784-1f5fb14b9830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97ad3f5a90>]}
[0m03:21:01.974731 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:21:01.975027 [info ] [MainThread]: Found 1 source, 523 macros
[0m03:21:01.976025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5016e03b-fc5b-4eb1-8784-1f5fb14b9830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97ad3d6ff0>]}
[0m03:21:01.976329 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:21:01.976727 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:21:01.976988 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:21:01.977208 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:21:01.990810 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:21:01.996604 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:21:02.009485 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:21:02.009949 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:21:02.010246 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:21:03.777727 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:21:03.779038 [debug] [MainThread]: SQL status: OK in 1.769 seconds
[0m03:21:04.272803 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:21:04.297267 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:21:04.302662 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:21:04.303630 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:21:04.793706 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:21:04.794849 [debug] [MainThread]: SQL status: OK in 0.490 seconds
[0m03:21:04.797523 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:21:04.799671 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:21:04.802130 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:21:04.803002 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:21:05.302056 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:21:05.303163 [debug] [MainThread]: SQL status: OK in 0.499 seconds
[0m03:21:05.305736 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:21:05.307635 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN varchar(255),     ...  
[0m03:21:05.310148 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:21:05.311136 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN varchar(255),
            Country varchar(255),
            City varchar(255),
            State varchar(255),
            Postal_Code int,
            Model_Year int,
            Make varchar(255),
            Model varchar(255),
            Electric_Vehicle_Type varchar(255),
            Clean_Alternative_Fuel_Vehicle varchar(255),
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location varchar(255),
            Electric_Utility varchar(255),
            Cnsus_Tract varchar(255)
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:21:05.573256 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN varchar(255),
            Country varchar(255),
            City varchar(255),
            State varchar(255),
            Postal_Code int,
            Model_Year int,
            Make varchar(255),
            Model varchar(255),
            Electric_Vehicle_Type varchar(255),
            Clean_Alternative_Fuel_Vehicle varchar(255),
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location varchar(255),
            Electric_Utility varchar(255),
            Cnsus_Tract varchar(255)
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:21:05.574703 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)
[0m03:21:05.576073 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:21:05.576938 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:21:05.577863 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:21:05.578679 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:21:05.579443 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:21:05.857208 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:21:05.871834 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

[0m03:21:05.878767 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:21:05.880373 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.712774, "process_in_blocks": "0", "process_kernel_time": 0.921875, "process_mem_max_rss": "99584", "process_out_blocks": "0", "process_user_time": 1.421875}
[0m03:21:05.880951 [debug] [MainThread]: Command `dbt run-operation` failed at 03:21:05.880805 after 4.71 seconds
[0m03:21:05.881283 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:21:05.881643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97adde6510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97ad3f5df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97ad3f5c70>]}
[0m03:21:05.882025 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:21:05.882260 [debug] [MainThread]: Flushing usage events
[0m03:23:44.476404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c9e45ab40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c9ecfef90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c9e5cc350>]}
[0m03:23:44.479778 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:23:44.480097 | b6193abf-1b1e-483b-9658-5066d71ff84f ==============================
[0m03:23:44.480097 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:23:44.481417 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run-operation stage_external_sources', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m03:23:44.644785 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:23:44.645211 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:23:44.645498 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:23:44.773858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b6193abf-1b1e-483b-9658-5066d71ff84f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c9e5ccd10>]}
[0m03:23:44.774247 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:23:44.818153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b6193abf-1b1e-483b-9658-5066d71ff84f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c9da8d160>]}
[0m03:23:44.818582 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:23:44.818981 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:23:44.981004 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:23:45.101497 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:23:45.102101 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/schema.yml
[0m03:23:45.219634 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:23:45.231604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b6193abf-1b1e-483b-9658-5066d71ff84f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c9d882840>]}
[0m03:23:45.232186 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:23:45.276383 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:23:45.280035 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:23:45.295880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b6193abf-1b1e-483b-9658-5066d71ff84f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c9d7d5c10>]}
[0m03:23:45.296271 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:23:45.296578 [info ] [MainThread]: Found 1 source, 523 macros
[0m03:23:45.297417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b6193abf-1b1e-483b-9658-5066d71ff84f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c9d8152b0>]}
[0m03:23:45.297686 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:23:45.298126 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:23:45.298440 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:23:45.298704 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:23:45.312253 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:23:45.318142 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:23:45.327667 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:23:45.328124 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:23:45.328415 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:23:47.108947 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:23:47.110162 [debug] [MainThread]: SQL status: OK in 1.782 seconds
[0m03:23:47.607469 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:23:47.626172 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:23:47.630812 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:23:47.631749 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:23:48.136914 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:23:48.138016 [debug] [MainThread]: SQL status: OK in 0.505 seconds
[0m03:23:48.140712 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:23:48.142838 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:23:48.145311 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:23:48.146183 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:23:48.650742 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:23:48.651840 [debug] [MainThread]: SQL status: OK in 0.505 seconds
[0m03:23:48.654488 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:23:48.657191 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN string,           ...  
[0m03:23:48.660433 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:23:48.661624 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN string,
            Country string,
            City string,
            State string,
            Postal_Code int,
            Model_Year int,
            Make string,
            Model string,
            Electric_Vehicle_Type string,
            Clean_Alternative_Fuel_Vehicle string,
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location string,
            Electric_Utility string,
            Cnsus_Tract string
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:23:49.026325 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN string,
            Country string,
            City string,
            State string,
            Postal_Code int,
            Model_Year int,
            Make string,
            Model string,
            Electric_Vehicle_Type string,
            Clean_Alternative_Fuel_Vehicle string,
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location string,
            Electric_Utility string,
            Cnsus_Tract string
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:23:49.027738 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)
[0m03:23:49.029369 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:23:49.030241 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:23:49.031209 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:23:49.032047 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:23:49.032833 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:23:49.281902 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:23:49.299598 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

[0m03:23:49.313730 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:23:49.315422 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.8914666, "process_in_blocks": "0", "process_kernel_time": 0.9375, "process_mem_max_rss": "100656", "process_out_blocks": "0", "process_user_time": 1.5}
[0m03:23:49.315999 [debug] [MainThread]: Command `dbt run-operation` failed at 03:23:49.315878 after 4.89 seconds
[0m03:23:49.316380 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:23:49.316747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ca1ab30e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c9d867ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0c9ed5b1a0>]}
[0m03:23:49.317068 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:23:49.317332 [debug] [MainThread]: Flushing usage events
[0m03:26:48.061804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6349cd2f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63493bb440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f634a594620>]}
[0m03:26:48.065673 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:26:48.066027 | ac354238-b426-43de-a749-38d55d691246 ==============================
[0m03:26:48.066027 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:26:48.066903 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run-operation create_external_table', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m03:26:48.230327 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:26:48.230745 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:26:48.231046 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:26:48.358315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ac354238-b426-43de-a749-38d55d691246', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6348b94e60>]}
[0m03:26:48.358737 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:26:48.402169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ac354238-b426-43de-a749-38d55d691246', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6349841550>]}
[0m03:26:48.402581 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:26:48.402978 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:26:48.566060 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:26:48.689499 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m03:26:48.689998 [debug] [MainThread]: Partial parsing: added file: dbt_spark_emr_app://macros/factory.sql
[0m03:26:48.725015 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:26:48.738068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ac354238-b426-43de-a749-38d55d691246', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63489673e0>]}
[0m03:26:48.738486 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:26:48.787188 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:26:48.790732 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:26:48.806581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac354238-b426-43de-a749-38d55d691246', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63486c7290>]}
[0m03:26:48.806959 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:26:48.807259 [info ] [MainThread]: Found 1 source, 524 macros
[0m03:26:48.808026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ac354238-b426-43de-a749-38d55d691246', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6348a29a60>]}
[0m03:26:48.808293 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:26:48.808725 [debug] [MainThread]: Acquiring new spark connection 'macro_create_external_table'
[0m03:26:48.808995 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:26:48.809220 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:26:48.818138 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:26:48.820862 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 0.8191974, "process_in_blocks": "0", "process_kernel_time": 0.75, "process_mem_max_rss": "99076", "process_out_blocks": "0", "process_user_time": 1.484375}
[0m03:26:48.821402 [debug] [MainThread]: Command `dbt run-operation` succeeded at 03:26:48.821294 after 0.82 seconds
[0m03:26:48.821709 [debug] [MainThread]: Connection 'macro_create_external_table' was properly closed.
[0m03:26:48.821998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6349528140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63486bb3b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6349785a00>]}
[0m03:26:48.822243 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:26:48.822462 [debug] [MainThread]: Flushing usage events
[0m03:31:13.642965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b39f4ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b3b26000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b2b26720>]}
[0m03:31:13.646337 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:31:13.646656 | aaa306fa-ff94-4025-8418-edfbac38086b ==============================
[0m03:31:13.646656 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:31:13.647804 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --select vehicle_ext', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m03:31:13.809642 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:31:13.810021 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:31:13.810291 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:31:13.937358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'aaa306fa-ff94-4025-8418-edfbac38086b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b38b9eb0>]}
[0m03:31:13.937761 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:31:13.981720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'aaa306fa-ff94-4025-8418-edfbac38086b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b1c12900>]}
[0m03:31:13.982128 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:31:13.982526 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:31:14.142813 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:31:14.266824 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 2 files changed.
[0m03:31:14.267285 [debug] [MainThread]: Partial parsing: added file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m03:31:14.267665 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/schema.yml
[0m03:31:14.267963 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m03:31:14.504651 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:31:14.517265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aaa306fa-ff94-4025-8418-edfbac38086b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b0fd8bc0>]}
[0m03:31:14.517668 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:31:14.575672 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:31:14.579323 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:31:14.601987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aaa306fa-ff94-4025-8418-edfbac38086b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b1136cc0>]}
[0m03:31:14.602405 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:31:14.602713 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m03:31:14.603459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aaa306fa-ff94-4025-8418-edfbac38086b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b0e04950>]}
[0m03:31:14.603717 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:31:14.604852 [info ] [MainThread]: 
[0m03:31:14.605512 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:31:14.606091 [info ] [MainThread]: 
[0m03:31:14.606815 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m03:31:14.607927 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m03:31:14.616206 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m03:31:14.616861 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m03:31:14.617254 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:31:16.394999 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m03:31:16.396046 [debug] [ThreadPool]: SQL status: OK in 1.779 seconds
[0m03:31:17.130495 [debug] [ThreadPool]: On list_schemas: Close
[0m03:31:17.622019 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m03:31:17.630190 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m03:31:17.630668 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m03:31:17.630978 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m03:31:17.631262 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m03:31:19.613453 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m03:31:19.613940 [debug] [ThreadPool]: SQL status: OK in 1.983 seconds
[0m03:31:20.345273 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m03:31:20.346376 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m03:31:20.347217 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m03:31:20.347989 [debug] [ThreadPool]: On list_None_default: Close
[0m03:31:20.842212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aaa306fa-ff94-4025-8418-edfbac38086b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b0eafad0>]}
[0m03:31:20.843352 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:31:20.844321 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:31:20.845121 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:31:20.858401 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m03:31:20.859092 [info ] [Thread-1 (]: 1 of 1 START sql tsable model default.vehicle_ext .............................. [RUN]
[0m03:31:20.860932 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m03:31:20.861919 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m03:31:20.874133 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m03:31:20.881771 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m03:31:20.887694 [debug] [Thread-1 (]: Compilation Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  No materialization 'tsable' was found for adapter spark! (searched types 'default' and 'spark')
[0m03:31:20.890231 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aaa306fa-ff94-4025-8418-edfbac38086b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b10e1b80>]}
[0m03:31:20.890773 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m03:31:20.891503 [error] [Thread-1 (]: 1 of 1 ERROR creating sql tsable model default.vehicle_ext ..................... [[31mERROR[0m in 0.03s]
[0m03:31:20.893869 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m03:31:20.894593 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Compilation Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  No materialization 'tsable' was found for adapter spark! (searched types 'default' and 'spark').
[0m03:31:20.899233 [debug] [MainThread]: On master: ROLLBACK
[0m03:31:20.900231 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:31:22.343363 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:31:22.344462 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:31:22.345267 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:31:22.346037 [debug] [MainThread]: On master: ROLLBACK
[0m03:31:22.346764 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:31:22.347393 [debug] [MainThread]: On master: Close
[0m03:31:22.592860 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:31:22.594418 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m03:31:22.595718 [info ] [MainThread]: 
[0m03:31:22.598100 [info ] [MainThread]: Finished running 1 tsable model in 0 hours 0 minutes and 7.99 seconds (7.99s).
[0m03:31:22.601282 [debug] [MainThread]: Command end result
[0m03:31:22.644974 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:31:22.648073 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:31:22.659298 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:31:22.660180 [info ] [MainThread]: 
[0m03:31:22.661987 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m03:31:22.662925 [info ] [MainThread]: 
[0m03:31:22.663762 [error] [MainThread]:   Compilation Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  No materialization 'tsable' was found for adapter spark! (searched types 'default' and 'spark')
[0m03:31:22.664900 [info ] [MainThread]: 
[0m03:31:22.665619 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m03:31:22.667183 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.075922, "process_in_blocks": "0", "process_kernel_time": 0.921875, "process_mem_max_rss": "104560", "process_out_blocks": "0", "process_user_time": 1.734375}
[0m03:31:22.668346 [debug] [MainThread]: Command `dbt run` failed at 03:31:22.668059 after 9.08 seconds
[0m03:31:22.668784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b2ec2cf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b1d9fbf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd6b0f1d2e0>]}
[0m03:31:22.669732 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:31:22.670228 [debug] [MainThread]: Flushing usage events
[0m03:32:34.263385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b8831c710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b887859a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b885a9a00>]}
[0m03:32:34.266955 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:32:34.267270 | 6f7fac0b-4bc8-4b28-80ac-7c539ce999ed ==============================
[0m03:32:34.267270 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:32:34.268089 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run-operation create_external_table', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m03:32:34.424451 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:32:34.424867 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:32:34.425148 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:32:34.557519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6f7fac0b-4bc8-4b28-80ac-7c539ce999ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b879aa810>]}
[0m03:32:34.557937 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:32:34.601196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6f7fac0b-4bc8-4b28-80ac-7c539ce999ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b880b7200>]}
[0m03:32:34.601612 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:32:34.602002 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:32:34.765936 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:32:34.900466 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:32:34.900997 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m03:32:35.131923 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:32:35.145026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6f7fac0b-4bc8-4b28-80ac-7c539ce999ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b8775e570>]}
[0m03:32:35.145519 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:32:35.202903 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:32:35.206623 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:32:35.222604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6f7fac0b-4bc8-4b28-80ac-7c539ce999ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b893ada90>]}
[0m03:32:35.223013 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:32:35.223326 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m03:32:35.224329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6f7fac0b-4bc8-4b28-80ac-7c539ce999ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b862043e0>]}
[0m03:32:35.224566 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:32:35.224959 [debug] [MainThread]: Acquiring new spark connection 'macro_create_external_table'
[0m03:32:35.225250 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:32:35.225477 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:32:35.233768 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:32:35.235564 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 1.0249705, "process_in_blocks": "0", "process_kernel_time": 0.921875, "process_mem_max_rss": "103292", "process_out_blocks": "0", "process_user_time": 1.546875}
[0m03:32:35.236098 [debug] [MainThread]: Command `dbt run-operation` succeeded at 03:32:35.235969 after 1.03 seconds
[0m03:32:35.236516 [debug] [MainThread]: Connection 'macro_create_external_table' was properly closed.
[0m03:32:35.236811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b887859a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b8763c290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b893ada90>]}
[0m03:32:35.237057 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:32:35.237283 [debug] [MainThread]: Flushing usage events
[0m03:33:26.026298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb361fa1f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb362184bc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb362054b90>]}
[0m03:33:26.029831 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:33:26.030157 | a40127b9-1783-478d-b8de-20c1c1a28569 ==============================
[0m03:33:26.030157 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:33:26.031219 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run-operation stage_external_sources', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m03:33:26.190354 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:33:26.190759 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:33:26.191029 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:33:26.318371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a40127b9-1783-478d-b8de-20c1c1a28569', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb363c330e0>]}
[0m03:33:26.318786 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:33:26.362762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a40127b9-1783-478d-b8de-20c1c1a28569', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb36072ca40>]}
[0m03:33:26.363189 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:33:26.363586 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:33:26.526622 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:33:26.663257 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m03:33:26.663626 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m03:33:26.668258 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:33:26.692666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a40127b9-1783-478d-b8de-20c1c1a28569', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb35fa05ee0>]}
[0m03:33:26.693065 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:33:26.750301 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:33:26.753827 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:33:26.770183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a40127b9-1783-478d-b8de-20c1c1a28569', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb35f7b5880>]}
[0m03:33:26.770571 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:33:26.770877 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m03:33:26.771968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a40127b9-1783-478d-b8de-20c1c1a28569', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb35f776750>]}
[0m03:33:26.772366 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:33:26.772952 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:33:26.773331 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:33:26.773679 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:33:26.790046 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:33:26.796244 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:33:26.805394 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:33:26.805851 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:33:26.806135 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:33:28.586223 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:33:28.587422 [debug] [MainThread]: SQL status: OK in 1.781 seconds
[0m03:33:29.074773 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:33:29.116027 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:33:29.121968 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:33:29.122993 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:33:29.615959 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:33:29.617033 [debug] [MainThread]: SQL status: OK in 0.493 seconds
[0m03:33:29.619092 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:33:29.621056 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:33:29.623381 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:33:29.624237 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:33:30.123400 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:33:30.124463 [debug] [MainThread]: SQL status: OK in 0.499 seconds
[0m03:33:30.127113 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:33:30.129414 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN string,           ...  
[0m03:33:30.131814 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:33:30.132724 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN string,
            Country string,
            City string,
            State string,
            Postal_Code int,
            Model_Year int,
            Make string,
            Model string,
            Electric_Vehicle_Type string,
            Clean_Alternative_Fuel_Vehicle string,
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location string,
            Electric_Utility string,
            Cnsus_Tract string
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:33:30.380343 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN string,
            Country string,
            City string,
            State string,
            Postal_Code int,
            Model_Year int,
            Make string,
            Model string,
            Electric_Vehicle_Type string,
            Clean_Alternative_Fuel_Vehicle string,
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location string,
            Electric_Utility string,
            Cnsus_Tract string
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:33:30.381290 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)
[0m03:33:30.384560 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:33:30.385506 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:33:30.386501 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:33:30.387372 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:33:30.388164 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:33:30.637274 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:33:30.654706 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

[0m03:33:30.668941 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:33:30.671253 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.6988487, "process_in_blocks": "0", "process_kernel_time": 0.796875, "process_mem_max_rss": "99292", "process_out_blocks": "0", "process_user_time": 1.59375}
[0m03:33:30.672277 [debug] [MainThread]: Command `dbt run-operation` failed at 03:33:30.672042 after 4.70 seconds
[0m03:33:30.673040 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:33:30.673781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb361fa1f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb35f68e030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb360efb200>]}
[0m03:33:30.674436 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:33:30.675035 [debug] [MainThread]: Flushing usage events
[0m03:40:15.091011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7b9766e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7b8916540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7b85bf740>]}
[0m03:40:15.094548 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:40:15.094862 | 2ccfc29e-5be5-4b94-b26f-e0076268d582 ==============================
[0m03:40:15.094862 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:40:15.095791 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation stage_external_sources', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m03:40:15.266028 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:40:15.266451 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:40:15.266742 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:40:15.396296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2ccfc29e-5be5-4b94-b26f-e0076268d582', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7b7c51940>]}
[0m03:40:15.396691 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:40:15.439956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2ccfc29e-5be5-4b94-b26f-e0076268d582', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7b876b020>]}
[0m03:40:15.440382 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:40:15.440787 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:40:15.602699 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:40:15.740253 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m03:40:15.740870 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/schema.yml
[0m03:40:15.872235 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:40:15.885239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2ccfc29e-5be5-4b94-b26f-e0076268d582', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7b734e960>]}
[0m03:40:15.885684 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:40:15.940727 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:40:15.944854 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:40:15.961296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2ccfc29e-5be5-4b94-b26f-e0076268d582', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7b71a5430>]}
[0m03:40:15.961659 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:40:15.961954 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m03:40:15.962675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2ccfc29e-5be5-4b94-b26f-e0076268d582', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7b7061b50>]}
[0m03:40:15.962945 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:40:15.963341 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:40:15.963600 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:40:15.963822 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:40:15.976173 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:40:15.981146 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:40:15.989298 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:40:15.989677 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:40:15.989962 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:40:17.746333 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:40:17.747559 [debug] [MainThread]: SQL status: OK in 1.757 seconds
[0m03:40:18.235422 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:40:18.268530 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:40:18.274290 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:40:18.275309 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:40:18.763944 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:40:18.765023 [debug] [MainThread]: SQL status: OK in 0.489 seconds
[0m03:40:18.767114 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:40:18.768610 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:40:18.770832 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:40:18.771745 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:40:19.262494 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:40:19.263592 [debug] [MainThread]: SQL status: OK in 0.491 seconds
[0m03:40:19.266228 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:40:19.268315 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN string,           ...  
[0m03:40:19.270729 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:40:19.271699 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN string,
            Country string,
            City string,
            State string,
            Postal_Code int,
            Model_Year int,
            Make string,
            Model string,
            Electric_Vehicle_Type string,
            Clean_Alternative_Fuel_Vehicle string,
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location string,
            Electric_Utility string,
            Cnsus_Tract string
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:40:19.533047 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN string,
            Country string,
            City string,
            State string,
            Postal_Code int,
            Model_Year int,
            Make string,
            Model string,
            Electric_Vehicle_Type string,
            Clean_Alternative_Fuel_Vehicle string,
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location string,
            Electric_Utility string,
            Cnsus_Tract string
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:40:19.534517 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)
[0m03:40:19.536115 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:40:19.536946 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:40:19.537806 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:40:19.538507 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:40:19.539160 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:40:19.787577 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:40:19.804565 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

[0m03:40:19.814308 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:40:19.817215 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.779121, "process_in_blocks": "0", "process_kernel_time": 0.765625, "process_mem_max_rss": "100940", "process_out_blocks": "0", "process_user_time": 1.75}
[0m03:40:19.818379 [debug] [MainThread]: Command `dbt run-operation` failed at 03:40:19.818115 after 4.78 seconds
[0m03:40:19.819230 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:40:19.820037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7b97ccda0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7b734ca40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7b734ccb0>]}
[0m03:40:19.820732 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:40:19.821363 [debug] [MainThread]: Flushing usage events
[0m03:44:18.228179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6a9f3fe60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6aa99bb60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6abca6570>]}
[0m03:44:18.232010 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:44:18.232380 | c0faa268-d44b-45ec-a56a-30217c25afab ==============================
[0m03:44:18.232380 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:44:18.233458 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation stage_external_sources --args select: vehicle_ext', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m03:44:18.394806 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:44:18.395237 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:44:18.395522 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:44:18.527319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c0faa268-d44b-45ec-a56a-30217c25afab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6a9d838c0>]}
[0m03:44:18.527743 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:44:18.571348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c0faa268-d44b-45ec-a56a-30217c25afab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6a9d82e10>]}
[0m03:44:18.571761 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:44:18.572163 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:44:18.735234 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:44:18.875002 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m03:44:18.875369 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m03:44:18.880055 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:44:18.906488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c0faa268-d44b-45ec-a56a-30217c25afab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6a92c1850>]}
[0m03:44:18.906901 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:44:18.963884 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:44:18.967856 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:44:18.984114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c0faa268-d44b-45ec-a56a-30217c25afab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6a9478d10>]}
[0m03:44:18.984670 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:44:18.985131 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m03:44:18.986082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c0faa268-d44b-45ec-a56a-30217c25afab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6a939c4a0>]}
[0m03:44:18.986457 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:44:18.987044 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:44:18.987446 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:44:18.987797 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:44:19.004136 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:44:19.009640 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:44:19.018724 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:44:19.019197 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:44:19.019506 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:44:20.788532 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:44:20.789774 [debug] [MainThread]: SQL status: OK in 1.770 seconds
[0m03:44:21.278734 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:44:21.320744 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:44:21.325407 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:44:21.326145 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:44:21.813140 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:44:21.814250 [debug] [MainThread]: SQL status: OK in 0.487 seconds
[0m03:44:21.816917 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:44:21.819140 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:44:21.821615 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:44:21.822469 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:44:22.311766 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:44:22.312861 [debug] [MainThread]: SQL status: OK in 0.490 seconds
[0m03:44:22.315474 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:44:22.317658 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN string,           ...  
[0m03:44:22.320029 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:44:22.320937 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN string,
            Country string,
            City string,
            State string,
            Postal_Code int,
            Model_Year int,
            Make string,
            Model string,
            Electric_Vehicle_Type string,
            Clean_Alternative_Fuel_Vehicle string,
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location string,
            Electric_Utility string,
            Cnsus_Tract string
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:44:22.675611 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN string,
            Country string,
            City string,
            State string,
            Postal_Code int,
            Model_Year int,
            Make string,
            Model string,
            Electric_Vehicle_Type string,
            Clean_Alternative_Fuel_Vehicle string,
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location string,
            Electric_Utility string,
            Cnsus_Tract string
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:44:22.677225 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)
[0m03:44:22.679077 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:44:22.679427 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:44:22.679799 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:44:22.680107 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:44:22.680375 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:44:22.926907 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:44:22.943317 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

[0m03:44:22.959930 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:44:22.962867 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 4.7887893, "process_in_blocks": "0", "process_kernel_time": 1.015625, "process_mem_max_rss": "99280", "process_out_blocks": "0", "process_user_time": 1.484375}
[0m03:44:22.963960 [debug] [MainThread]: Command `dbt run-operation` failed at 03:44:22.963726 after 4.79 seconds
[0m03:44:22.964858 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:44:22.965376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6ab1d6540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6aaa1f350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6aaa1dfd0>]}
[0m03:44:22.965787 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:44:22.966167 [debug] [MainThread]: Flushing usage events
[0m03:46:58.012935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d3761ef30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d37a26210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d37f497f0>]}
[0m03:46:58.016468 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 03:46:58.016782 | 881efdfd-b1f2-44e9-8433-2aea914d05c8 ==============================
[0m03:46:58.016782 [info ] [MainThread]: Running with dbt=1.9.2
[0m03:46:58.018000 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation stage_external_sources --args select: vehicle_ext', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m03:46:58.178131 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m03:46:58.178518 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m03:46:58.178798 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m03:46:58.306664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '881efdfd-b1f2-44e9-8433-2aea914d05c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d35d9f920>]}
[0m03:46:58.307123 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:46:58.350219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '881efdfd-b1f2-44e9-8433-2aea914d05c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d35381430>]}
[0m03:46:58.350621 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:46:58.351050 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m03:46:58.515684 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m03:46:58.581723 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m03:46:58.582957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '881efdfd-b1f2-44e9-8433-2aea914d05c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d3602a1e0>]}
[0m03:46:58.583214 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:46:59.596477 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m03:46:59.618897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '881efdfd-b1f2-44e9-8433-2aea914d05c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d352c1c40>]}
[0m03:46:59.619378 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:46:59.690271 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m03:46:59.698000 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m03:46:59.718875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '881efdfd-b1f2-44e9-8433-2aea914d05c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d340b4ce0>]}
[0m03:46:59.719487 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:46:59.720024 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m03:46:59.721200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '881efdfd-b1f2-44e9-8433-2aea914d05c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d353e9a60>]}
[0m03:46:59.721734 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:46:59.722542 [debug] [MainThread]: Acquiring new spark connection 'macro_stage_external_sources'
[0m03:46:59.723074 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m03:46:59.723500 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m03:46:59.754107 [info ] [MainThread]: 1 of 1 START external source vehicle_poc.vehicle_ext
[0m03:46:59.766979 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".vehicle_poc", this is inefficient
[0m03:46:59.779521 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:46:59.780069 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show table extended in vehicle_poc like '*'
  
[0m03:46:59.780450 [debug] [MainThread]: Opening a new connection, currently in state init
[0m03:47:01.525326 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:47:01.526572 [debug] [MainThread]: SQL status: OK in 1.746 seconds
[0m03:47:02.008869 [debug] [MainThread]: While listing relations in database=, schema=vehicle_poc, found: 
[0m03:47:02.050170 [info ] [MainThread]: 1 of 1 (1) create schema if not exists vehicle_poc
[0m03:47:02.057161 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:47:02.057790 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 create schema if not exists vehicle_poc
            
[0m03:47:02.542078 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:47:02.543685 [debug] [MainThread]: SQL status: OK in 0.485 seconds
[0m03:47:02.545909 [info ] [MainThread]: 1 of 1 (1) OK
[0m03:47:02.547847 [info ] [MainThread]: 1 of 1 (2) drop table if exists vehicle_poc.vehicle_ext
[0m03:47:02.549175 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:47:02.549552 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists vehicle_poc.vehicle_ext
    

            
[0m03:47:03.033298 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m03:47:03.034360 [debug] [MainThread]: SQL status: OK in 0.484 seconds
[0m03:47:03.036936 [info ] [MainThread]: 1 of 1 (2) OK
[0m03:47:03.039267 [info ] [MainThread]: 1 of 1 (3) create table vehicle_poc.vehicle_ext (                    VIN string,           ...  
[0m03:47:03.041870 [debug] [MainThread]: Using spark connection "macro_stage_external_sources"
[0m03:47:03.042788 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN string,
            Country string,
            City string,
            State string,
            Postal_Code int,
            Model_Year int,
            Make string,
            Model string,
            Electric_Vehicle_Type string,
            Clean_Alternative_Fuel_Vehicle string,
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location string,
            Electric_Utility string,
            Cnsus_Tract string
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:47:03.286917 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table vehicle_poc.vehicle_ext (
        
            VIN string,
            Country string,
            City string,
            State string,
            Postal_Code int,
            Model_Year int,
            Make string,
            Model string,
            Electric_Vehicle_Type string,
            Clean_Alternative_Fuel_Vehicle string,
            Electric_Range int,
            Base_MSRP int,
            Legislative_District int,
            DOL_Vehicle_ID int,
            Vehicle_Location string,
            Electric_Utility string,
            Cnsus_Tract string
    )  using csv
    options ('sep' = ',', 
'header' = 'true')
    
    
    
    location 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/Electric_Vehicle_Population_Data.csv'
    

            
[0m03:47:03.288388 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)
[0m03:47:03.289988 [debug] [MainThread]: Spark adapter: Error while running:
macro stage_external_sources
[0m03:47:03.290874 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:47:03.291803 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m03:47:03.292578 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m03:47:03.293312 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m03:47:03.539007 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'
[0m03:47:03.558689 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 24:7 missing EOF at 'using' near ')':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 220, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 24:7 missing EOF at 'using' near ')'

[0m03:47:03.567330 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m03:47:03.568741 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 5.6101584, "process_in_blocks": "0", "process_kernel_time": 0.8125, "process_mem_max_rss": "102688", "process_out_blocks": "0", "process_user_time": 2.5625}
[0m03:47:03.569340 [debug] [MainThread]: Command `dbt run-operation` failed at 03:47:03.569224 after 5.61 seconds
[0m03:47:03.569707 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m03:47:03.570081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d3761ef30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d352a3e00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d3677b2f0>]}
[0m03:47:03.570375 [debug] [MainThread]: An error was encountered while trying to send an event
[0m03:47:03.570629 [debug] [MainThread]: Flushing usage events
[0m04:02:03.398304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37cacc2ae0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37cad066f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37cacc1c40>]}
[0m04:02:03.401761 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 04:02:03.402077 | 433c6b3c-41d8-4328-9c80-fba525cd482b ==============================
[0m04:02:03.402077 [info ] [MainThread]: Running with dbt=1.9.2
[0m04:02:03.403392 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt show --select vehicle_ext', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m04:02:03.567534 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m04:02:03.567958 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m04:02:03.568247 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m04:02:03.694705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '433c6b3c-41d8-4328-9c80-fba525cd482b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37ca7b6120>]}
[0m04:02:03.696710 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:03.740049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '433c6b3c-41d8-4328-9c80-fba525cd482b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37cb3d25a0>]}
[0m04:02:03.740466 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:03.783160 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m04:02:03.955962 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m04:02:04.022213 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m04:02:04.023434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '433c6b3c-41d8-4328-9c80-fba525cd482b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37cacc23c0>]}
[0m04:02:04.023684 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:05.088067 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m04:02:05.128843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '433c6b3c-41d8-4328-9c80-fba525cd482b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37c9b1ca70>]}
[0m04:02:05.129793 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:05.212986 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m04:02:05.225507 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m04:02:05.234437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '433c6b3c-41d8-4328-9c80-fba525cd482b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37c9a2ba10>]}
[0m04:02:05.235738 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:05.236432 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m04:02:05.237759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '433c6b3c-41d8-4328-9c80-fba525cd482b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37c9f33620>]}
[0m04:02:05.238826 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:05.242315 [info ] [MainThread]: 
[0m04:02:05.244065 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:02:05.245017 [info ] [MainThread]: 
[0m04:02:05.246601 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m04:02:05.248487 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m04:02:05.260886 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m04:02:05.261420 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m04:02:05.261787 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m04:02:05.262113 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:02:07.026917 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:02:07.028020 [debug] [ThreadPool]: SQL status: OK in 1.766 seconds
[0m04:02:07.760069 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m04:02:07.761263 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m04:02:07.762128 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m04:02:07.762911 [debug] [ThreadPool]: On list_None_default: Close
[0m04:02:08.251251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '433c6b3c-41d8-4328-9c80-fba525cd482b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37c9f30b90>]}
[0m04:02:08.252356 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:08.262495 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m04:02:08.263794 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m04:02:08.264635 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m04:02:08.278709 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m04:02:08.281611 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m04:02:08.293446 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m04:02:08.294147 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
  select * from s3_database.example_s3_table limit 100;
  
  limit 5

[0m04:02:08.294642 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:02:10.314185 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
  select * from s3_database.example_s3_table limit 100;
  
  limit 5

[0m04:02:10.315881 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 4:54 cannot recognize input near 'limit' '100' ';' in limit clause:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 4:54 cannot recognize input near 'limit' '100' ';' in limit clause:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 4:54 cannot recognize input near 'limit' '100' ';' in limit clause"), operationHandle=None)
[0m04:02:10.317487 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m04:02:10.606152 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 4:54 cannot recognize input near 'limit' '100' ';' in limit clause
[0m04:02:10.607832 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m04:02:10.609272 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 4:54 cannot recognize input near 'limit' '100' ';' in limit clause.
[0m04:02:10.611414 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:02:10.612464 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m04:02:10.613813 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
    Error while compiling statement: FAILED: ParseException line 4:54 cannot recognize input near 'limit' '100' ';' in limit clause
[0m04:02:10.617673 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 7.2712064, "process_in_blocks": "0", "process_kernel_time": 0.90625, "process_mem_max_rss": "103864", "process_out_blocks": "0", "process_user_time": 2.609375}
[0m04:02:10.618734 [debug] [MainThread]: Command `dbt show` failed at 04:02:10.618488 after 7.27 seconds
[0m04:02:10.619596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37ca76e3c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37c9f33c80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37c9a88830>]}
[0m04:02:10.620337 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:10.621023 [debug] [MainThread]: Flushing usage events
[0m04:02:24.864774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f194558fe30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1945682870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f194558e120>]}
[0m04:02:24.868498 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 04:02:24.868824 | b8fcd1ec-57b0-49d7-9463-57bc445900db ==============================
[0m04:02:24.868824 [info ] [MainThread]: Running with dbt=1.9.2
[0m04:02:24.869846 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt show --select vehicle_ext', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m04:02:25.032926 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m04:02:25.033670 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m04:02:25.034121 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m04:02:25.164841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b8fcd1ec-57b0-49d7-9463-57bc445900db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19449ca900>]}
[0m04:02:25.165242 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:25.208604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b8fcd1ec-57b0-49d7-9463-57bc445900db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1945646e10>]}
[0m04:02:25.209012 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:25.228337 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m04:02:25.397178 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m04:02:25.533054 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m04:02:25.533587 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m04:02:25.756427 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m04:02:25.770061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b8fcd1ec-57b0-49d7-9463-57bc445900db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1944365eb0>]}
[0m04:02:25.770507 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:25.826971 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m04:02:25.831573 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m04:02:25.834655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b8fcd1ec-57b0-49d7-9463-57bc445900db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1944356540>]}
[0m04:02:25.835046 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:25.835367 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m04:02:25.836416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b8fcd1ec-57b0-49d7-9463-57bc445900db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f194505c320>]}
[0m04:02:25.836879 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:25.838826 [info ] [MainThread]: 
[0m04:02:25.840252 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:02:25.840995 [info ] [MainThread]: 
[0m04:02:25.841863 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m04:02:25.842940 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m04:02:25.851335 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m04:02:25.851757 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m04:02:25.852021 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m04:02:25.852246 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:02:27.599578 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:02:27.600631 [debug] [ThreadPool]: SQL status: OK in 1.748 seconds
[0m04:02:28.333855 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m04:02:28.334940 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m04:02:28.335748 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m04:02:28.336474 [debug] [ThreadPool]: On list_None_default: Close
[0m04:02:28.823049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b8fcd1ec-57b0-49d7-9463-57bc445900db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1944355070>]}
[0m04:02:28.824052 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:28.839228 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m04:02:28.839776 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m04:02:28.840163 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m04:02:28.846624 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m04:02:28.848405 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m04:02:28.859564 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m04:02:28.860727 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
  select * from s3_database.example_s3_table limit 100
  
  limit 5

[0m04:02:28.861376 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:02:30.591420 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
  select * from s3_database.example_s3_table limit 100
  
  limit 5

[0m04:02:30.592820 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 6:2 cannot recognize input near 'limit' '100' 'limit' in limit clause:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 6:2 cannot recognize input near 'limit' '100' 'limit' in limit clause:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 6:2 cannot recognize input near 'limit' '100' 'limit' in limit clause"), operationHandle=None)
[0m04:02:30.594157 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m04:02:30.848054 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 6:2 cannot recognize input near 'limit' '100' 'limit' in limit clause
[0m04:02:30.849765 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m04:02:30.851122 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 6:2 cannot recognize input near 'limit' '100' 'limit' in limit clause.
[0m04:02:30.853404 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:02:30.854550 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m04:02:30.855734 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
    Error while compiling statement: FAILED: ParseException line 6:2 cannot recognize input near 'limit' '100' 'limit' in limit clause
[0m04:02:30.861790 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 6.048705, "process_in_blocks": "0", "process_kernel_time": 0.890625, "process_mem_max_rss": "104496", "process_out_blocks": "0", "process_user_time": 1.703125}
[0m04:02:30.862879 [debug] [MainThread]: Command `dbt show` failed at 04:02:30.862642 after 6.05 seconds
[0m04:02:30.863735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1945682870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1944362ea0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f19463f5520>]}
[0m04:02:30.864443 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:30.865076 [debug] [MainThread]: Flushing usage events
[0m04:02:56.408854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4fbbf0fe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4fb5da600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4fb1420f0>]}
[0m04:02:56.412413 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 04:02:56.412755 | 52524eab-0b79-45f9-8c1f-d19bf3fe5f41 ==============================
[0m04:02:56.412755 [info ] [MainThread]: Running with dbt=1.9.2
[0m04:02:56.413810 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt show --select vehicle_ext', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m04:02:56.572533 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m04:02:56.572911 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m04:02:56.573183 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m04:02:56.701181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '52524eab-0b79-45f9-8c1f-d19bf3fe5f41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4fab3e030>]}
[0m04:02:56.701641 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:56.745584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '52524eab-0b79-45f9-8c1f-d19bf3fe5f41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4faaaf4d0>]}
[0m04:02:56.746024 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:56.764079 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m04:02:56.933014 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m04:02:57.067311 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m04:02:57.067880 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m04:02:57.291729 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m04:02:57.304917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '52524eab-0b79-45f9-8c1f-d19bf3fe5f41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4f9e4c200>]}
[0m04:02:57.305353 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:57.359782 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m04:02:57.363753 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m04:02:57.366242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '52524eab-0b79-45f9-8c1f-d19bf3fe5f41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4f9e36900>]}
[0m04:02:57.366593 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:57.366876 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m04:02:57.367725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52524eab-0b79-45f9-8c1f-d19bf3fe5f41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4f9e36810>]}
[0m04:02:57.367991 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:02:57.369200 [info ] [MainThread]: 
[0m04:02:57.370041 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:02:57.370779 [info ] [MainThread]: 
[0m04:02:57.371644 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m04:02:57.372700 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m04:02:57.383223 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m04:02:57.383775 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m04:02:57.384084 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m04:02:57.384368 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:02:59.131378 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:02:59.132472 [debug] [ThreadPool]: SQL status: OK in 1.748 seconds
[0m04:02:59.861053 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m04:02:59.862236 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m04:02:59.863081 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m04:02:59.863850 [debug] [ThreadPool]: On list_None_default: Close
[0m04:03:00.354008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52524eab-0b79-45f9-8c1f-d19bf3fe5f41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4fa133b90>]}
[0m04:03:00.355026 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:03:00.375226 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m04:03:00.376513 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m04:03:00.377508 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m04:03:00.391420 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m04:03:00.395092 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m04:03:00.410806 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m04:03:00.411872 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
  select * from s3_database.example_s3_table
  
  limit 5

[0m04:03:00.412697 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:03:02.399138 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m04:03:02.400345 [debug] [Thread-1 (]: SQL status: OK in 1.988 seconds
[0m04:03:04.344659 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m04:03:04.838586 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m04:03:04.845049 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:03:04.846069 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m04:03:04.847748 [debug] [MainThread]: Command end result
[0m04:03:04.893598 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m04:03:04.897272 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m04:03:04.918409 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m04:03:04.919925 [info ] [MainThread]: Previewing node 'vehicle_ext':
| example_s3_table.... | example_s3_table.aid | example_s3_table.... |
| -------------------- | -------------------- | -------------------- |
| 500000               | 804385               | clicks               |
| 500000               | 1096025              | clicks               |
| 500000               | 1160775              | clicks               |
| 500000               | 1586171              | clicks               |
| 500000               | 258814               | clicks               |

[0m04:03:04.924883 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 8.571964, "process_in_blocks": "0", "process_kernel_time": 0.8125, "process_mem_max_rss": "104552", "process_out_blocks": "0", "process_user_time": 1.796875}
[0m04:03:04.925386 [debug] [MainThread]: Command `dbt show` succeeded at 04:03:04.925275 after 8.57 seconds
[0m04:03:04.925768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4fe533140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4fb7ff4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4f9dcba10>]}
[0m04:03:04.926099 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:03:04.926378 [debug] [MainThread]: Flushing usage events
[0m04:04:58.829487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3219164e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa320cc8110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3211b7ad0>]}
[0m04:04:58.832894 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 04:04:58.833211 | 6d0e2d0b-a570-4a32-b5f7-f58c65dc8d14 ==============================
[0m04:04:58.833211 [info ] [MainThread]: Running with dbt=1.9.2
[0m04:04:58.834330 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run-operation create_external_table', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m04:04:58.995669 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m04:04:58.996092 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m04:04:58.996378 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m04:04:59.127999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d0e2d0b-a570-4a32-b5f7-f58c65dc8d14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3211f6330>]}
[0m04:04:59.128394 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:04:59.171203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d0e2d0b-a570-4a32-b5f7-f58c65dc8d14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa32037abd0>]}
[0m04:04:59.171673 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:04:59.172087 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m04:04:59.346298 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m04:04:59.484905 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m04:04:59.485439 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m04:04:59.533879 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m04:04:59.546697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6d0e2d0b-a570-4a32-b5f7-f58c65dc8d14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3201d5ee0>]}
[0m04:04:59.547124 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:04:59.603883 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m04:04:59.607770 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m04:04:59.624381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6d0e2d0b-a570-4a32-b5f7-f58c65dc8d14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3201461b0>]}
[0m04:04:59.624774 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:04:59.625078 [info ] [MainThread]: Found 1 model, 1 source, 524 macros
[0m04:04:59.626063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d0e2d0b-a570-4a32-b5f7-f58c65dc8d14', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3201d69c0>]}
[0m04:04:59.626309 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:04:59.626684 [debug] [MainThread]: Acquiring new spark connection 'macro_create_external_table'
[0m04:04:59.626944 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:04:59.627172 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:04:59.635094 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m04:04:59.636857 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 0.8594816, "process_in_blocks": "0", "process_kernel_time": 0.953125, "process_mem_max_rss": "98944", "process_out_blocks": "0", "process_user_time": 1.359375}
[0m04:04:59.637566 [debug] [MainThread]: Command `dbt run-operation` succeeded at 04:04:59.637430 after 0.86 seconds
[0m04:04:59.638034 [debug] [MainThread]: Connection 'macro_create_external_table' was properly closed.
[0m04:04:59.638469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa320bf3ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3201444a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa3201464e0>]}
[0m04:04:59.638852 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:04:59.639198 [debug] [MainThread]: Flushing usage events
[0m04:07:05.631244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39ec14dd60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39ec14d190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39ec14dbe0>]}
[0m04:07:05.635087 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 04:07:05.635405 | 63bd15f6-91f4-4b36-9a5d-f70eb196ff6d ==============================
[0m04:07:05.635405 [info ] [MainThread]: Running with dbt=1.9.2
[0m04:07:05.636653 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation create_external_table', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m04:07:05.815188 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m04:07:05.815569 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m04:07:05.815844 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m04:07:05.952868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '63bd15f6-91f4-4b36-9a5d-f70eb196ff6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39ec196de0>]}
[0m04:07:05.953264 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:07:05.999113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '63bd15f6-91f4-4b36-9a5d-f70eb196ff6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39ecd89af0>]}
[0m04:07:05.999516 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:07:05.999958 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m04:07:06.147135 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 1 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m04:07:06.150190 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 0.5719689, "process_in_blocks": "0", "process_kernel_time": 0.703125, "process_mem_max_rss": "94312", "process_out_blocks": "0", "process_user_time": 1.3125}
[0m04:07:06.150645 [debug] [MainThread]: Command `dbt run-operation` failed at 04:07:06.150561 after 0.57 seconds
[0m04:07:06.150936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39ec5dec30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39ecc8f020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39eb930440>]}
[0m04:07:06.151167 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:07:06.151371 [debug] [MainThread]: Flushing usage events
[0m04:07:30.125970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9493686de0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94930122a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f949379f7d0>]}
[0m04:07:30.129528 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 04:07:30.129844 | 73e88de9-e205-400f-a6f6-24e9d9fdf70f ==============================
[0m04:07:30.129844 [info ] [MainThread]: Running with dbt=1.9.2
[0m04:07:30.131016 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation create_external_table', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m04:07:30.289666 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m04:07:30.290083 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m04:07:30.290378 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m04:07:30.417091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '73e88de9-e205-400f-a6f6-24e9d9fdf70f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94949a9fd0>]}
[0m04:07:30.417488 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:07:30.460277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '73e88de9-e205-400f-a6f6-24e9d9fdf70f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9493f0c0b0>]}
[0m04:07:30.460729 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:07:30.461143 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m04:07:30.619025 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m04:07:30.742618 [debug] [MainThread]: Partial parsing enabled: 39 files deleted, 0 files added, 0 files changed.
[0m04:07:30.743046 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/redshift/create_external_table.sql
[0m04:07:30.743311 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/redshift/helpers/render_macro.sql
[0m04:07:30.743536 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/fabric/create_external_table.sql
[0m04:07:30.743747 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/fabric/get_external_build_plan.sql
[0m04:07:30.743964 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/common/update_external_table_columns.sql
[0m04:07:30.744173 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/fabric/create_external_schema.sql
[0m04:07:30.744379 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/common/helpers/transaction.sql
[0m04:07:30.744574 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/bigquery/create_external_table.sql
[0m04:07:30.744766 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/redshift/get_external_build_plan.sql
[0m04:07:30.744960 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/spark/get_external_build_plan.sql
[0m04:07:30.745158 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/snowflake/helpers/is_csv.sql
[0m04:07:30.745358 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/snowflake/snowpipe/create_snowpipe.sql
[0m04:07:30.745555 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/redshift/helpers/add_partitions.sql
[0m04:07:30.745826 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/common/get_external_build_plan.sql
[0m04:07:30.746039 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/common/helpers/dropif.sql
[0m04:07:30.746235 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/snowflake/get_external_build_plan.sql
[0m04:07:30.746436 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/common/create_external_schema.sql
[0m04:07:30.746628 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/redshift/refresh_external_table.sql
[0m04:07:30.746822 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/bigquery/create_external_schema.sql
[0m04:07:30.747009 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/redshift/helpers/transaction.sql
[0m04:07:30.747203 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/common/create_external_table.sql
[0m04:07:30.747394 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/redshift/helpers/paths.sql
[0m04:07:30.747674 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/bigquery/update_external_table_columns.sql
[0m04:07:30.747915 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/snowflake/refresh_external_table.sql
[0m04:07:30.748108 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/snowflake/create_external_schema.sql
[0m04:07:30.748304 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/spark/helpers/dropif.sql
[0m04:07:30.748507 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/snowflake/create_external_table.sql
[0m04:07:30.748703 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/snowflake/snowpipe/refresh_snowpipe.sql
[0m04:07:30.748904 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/spark/refresh_external_table.sql
[0m04:07:30.749098 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/common/refresh_external_table.sql
[0m04:07:30.749288 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/redshift/helpers/dropif.sql
[0m04:07:30.749474 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/snowflake/snowpipe/get_copy_sql.sql
[0m04:07:30.749669 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/spark/helpers/recover_partitions.sql
[0m04:07:30.749861 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/spark/create_external_table.sql
[0m04:07:30.750048 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/snowflake/snowpipe/create_empty_table.sql
[0m04:07:30.750232 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/redshift/helpers/is_ext_tbl.sql
[0m04:07:30.750414 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/common/stage_external_sources.sql
[0m04:07:30.750598 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/fabric/helpers/dropif.sql
[0m04:07:30.750780 [debug] [MainThread]: Partial parsing: deleted file: dbt_external_tables://macros/plugins/bigquery/get_external_build_plan.sql
[0m04:07:30.798008 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_emr_app.example
[0m04:07:30.811026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '73e88de9-e205-400f-a6f6-24e9d9fdf70f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94924ae300>]}
[0m04:07:30.811559 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:07:30.869574 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m04:07:30.873268 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m04:07:30.890680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '73e88de9-e205-400f-a6f6-24e9d9fdf70f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94921e4e60>]}
[0m04:07:30.891059 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:07:30.891324 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m04:07:30.892326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '73e88de9-e205-400f-a6f6-24e9d9fdf70f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9492272b10>]}
[0m04:07:30.892593 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:07:30.892992 [debug] [MainThread]: Acquiring new spark connection 'macro_create_external_table'
[0m04:07:30.893254 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:07:30.893472 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:07:30.902720 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m04:07:30.903987 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 0.8307792, "process_in_blocks": "0", "process_kernel_time": 0.859375, "process_mem_max_rss": "98916", "process_out_blocks": "0", "process_user_time": 1.421875}
[0m04:07:30.904434 [debug] [MainThread]: Command `dbt run-operation` succeeded at 04:07:30.904332 after 0.83 seconds
[0m04:07:30.904735 [debug] [MainThread]: Connection 'macro_create_external_table' was properly closed.
[0m04:07:30.905010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94933ad1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9492521100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9493351010>]}
[0m04:07:30.905251 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:07:30.905443 [debug] [MainThread]: Flushing usage events
[0m04:09:48.388003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d1f5e390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d1e3efc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d26c6d80>]}
[0m04:09:48.391613 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 04:09:48.391985 | 3c1c4896-7c1c-4fe4-9c10-4456b24cb5a6 ==============================
[0m04:09:48.391985 [info ] [MainThread]: Running with dbt=1.9.2
[0m04:09:48.393119 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run-operation create_external_table', 'send_anonymous_usage_stats': 'True'}
[0m04:09:48.571755 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m04:09:48.572178 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m04:09:48.572462 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m04:09:48.697456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3c1c4896-7c1c-4fe4-9c10-4456b24cb5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d126f3b0>]}
[0m04:09:48.697861 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:09:48.740774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3c1c4896-7c1c-4fe4-9c10-4456b24cb5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d0b73fe0>]}
[0m04:09:48.741225 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:09:48.741621 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m04:09:48.902768 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m04:09:48.904075 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m04:09:48.905339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3c1c4896-7c1c-4fe4-9c10-4456b24cb5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d12fc2c0>]}
[0m04:09:48.905697 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:09:49.834074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3c1c4896-7c1c-4fe4-9c10-4456b24cb5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d12e3bc0>]}
[0m04:09:49.834872 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:09:49.938437 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m04:09:49.946781 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m04:09:49.970875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3c1c4896-7c1c-4fe4-9c10-4456b24cb5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d0668e00>]}
[0m04:09:49.971321 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:09:49.971665 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m04:09:49.972815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3c1c4896-7c1c-4fe4-9c10-4456b24cb5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d06ceff0>]}
[0m04:09:49.973205 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:09:49.973815 [debug] [MainThread]: Acquiring new spark connection 'macro_create_external_table'
[0m04:09:49.974244 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:09:49.974624 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:09:49.997628 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m04:09:50.000034 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 1.6668688, "process_in_blocks": "0", "process_kernel_time": 1.0, "process_mem_max_rss": "102808", "process_out_blocks": "0", "process_user_time": 2.15625}
[0m04:09:50.000977 [debug] [MainThread]: Command `dbt run-operation` succeeded at 04:09:50.000826 after 1.67 seconds
[0m04:09:50.001258 [debug] [MainThread]: Connection 'macro_create_external_table' was properly closed.
[0m04:09:50.001705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d3266660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d0668a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa0d0668da0>]}
[0m04:09:50.002113 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:09:50.002486 [debug] [MainThread]: Flushing usage events
[0m04:10:10.208884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26eade1730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26eab39bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26ea9b8bc0>]}
[0m04:10:10.212363 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 04:10:10.212685 | a176f462-b9ac-4b3e-af80-2f7b9c3daa97 ==============================
[0m04:10:10.212685 [info ] [MainThread]: Running with dbt=1.9.2
[0m04:10:10.213680 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation create_external_table', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m04:10:10.371078 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m04:10:10.371498 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m04:10:10.371776 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m04:10:10.494389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a176f462-b9ac-4b3e-af80-2f7b9c3daa97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26eaa3f710>]}
[0m04:10:10.494818 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:10:10.537718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a176f462-b9ac-4b3e-af80-2f7b9c3daa97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26e9920e90>]}
[0m04:10:10.538164 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:10:10.538589 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m04:10:10.700087 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m04:10:10.815434 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m04:10:10.815840 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m04:10:10.841441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a176f462-b9ac-4b3e-af80-2f7b9c3daa97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26e94453d0>]}
[0m04:10:10.841950 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:10:10.896418 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m04:10:10.900078 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m04:10:10.916170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a176f462-b9ac-4b3e-af80-2f7b9c3daa97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26e93c9c10>]}
[0m04:10:10.916611 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:10:10.916934 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m04:10:10.917828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a176f462-b9ac-4b3e-af80-2f7b9c3daa97', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26e97069c0>]}
[0m04:10:10.918105 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:10:10.918542 [debug] [MainThread]: Acquiring new spark connection 'macro_create_external_table'
[0m04:10:10.918807 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:10:10.919026 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:10:10.928470 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m04:10:10.929668 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 0.777729, "process_in_blocks": "0", "process_kernel_time": 0.78125, "process_mem_max_rss": "98288", "process_out_blocks": "0", "process_user_time": 1.46875}
[0m04:10:10.930078 [debug] [MainThread]: Command `dbt run-operation` succeeded at 04:10:10.929993 after 0.78 seconds
[0m04:10:10.930343 [debug] [MainThread]: Connection 'macro_create_external_table' was properly closed.
[0m04:10:10.930600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26ea6c22a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26eafb2d80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f26e9469d00>]}
[0m04:10:10.930852 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:10:10.931070 [debug] [MainThread]: Flushing usage events
[0m04:25:03.898309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d23dc2b40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d25162ea0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d251615b0>]}
[0m04:25:03.901831 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 04:25:03.902145 | 090435d4-9aa5-4bdf-b480-bf3cd8f86d8b ==============================
[0m04:25:03.902145 [info ] [MainThread]: Running with dbt=1.9.2
[0m04:25:03.903313 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt show --select vehicle_ext', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m04:25:04.062551 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m04:25:04.062972 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m04:25:04.063260 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m04:25:04.192048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '090435d4-9aa5-4bdf-b480-bf3cd8f86d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d243a74d0>]}
[0m04:25:04.192439 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:04.234919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '090435d4-9aa5-4bdf-b480-bf3cd8f86d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d2389a510>]}
[0m04:25:04.235337 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:04.254300 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m04:25:04.416788 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m04:25:04.534504 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m04:25:04.535040 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m04:25:04.535387 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m04:25:04.771354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '090435d4-9aa5-4bdf-b480-bf3cd8f86d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d22b4eab0>]}
[0m04:25:04.771776 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:04.822281 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m04:25:04.825971 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m04:25:04.828595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '090435d4-9aa5-4bdf-b480-bf3cd8f86d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d22b36ea0>]}
[0m04:25:04.828991 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:04.829297 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m04:25:04.830599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '090435d4-9aa5-4bdf-b480-bf3cd8f86d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d21939d60>]}
[0m04:25:04.831103 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:04.833539 [info ] [MainThread]: 
[0m04:25:04.834558 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:25:04.835401 [info ] [MainThread]: 
[0m04:25:04.836324 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m04:25:04.837907 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m04:25:04.845951 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m04:25:04.846356 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m04:25:04.846611 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m04:25:04.846830 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:25:06.605305 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:25:06.606437 [debug] [ThreadPool]: SQL status: OK in 1.759 seconds
[0m04:25:07.344341 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m04:25:07.345433 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m04:25:07.346284 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m04:25:07.347052 [debug] [ThreadPool]: On list_None_default: Close
[0m04:25:07.841127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '090435d4-9aa5-4bdf-b480-bf3cd8f86d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d22b95010>]}
[0m04:25:07.842131 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:07.863416 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m04:25:07.864637 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m04:25:07.865581 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m04:25:07.879312 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m04:25:07.886453 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m04:25:07.899671 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m04:25:07.900661 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
  select * from vehicle_poc.vehicle_ext 
limit 10
  
  limit 5

[0m04:25:07.901163 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:25:09.590992 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
  select * from vehicle_poc.vehicle_ext 
limit 10
  
  limit 5

[0m04:25:09.592511 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:2 cannot recognize input near 'limit' '10' 'limit' in limit clause:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:2 cannot recognize input near 'limit' '10' 'limit' in limit clause:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:2 cannot recognize input near 'limit' '10' 'limit' in limit clause"), operationHandle=None)
[0m04:25:09.593820 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m04:25:09.848783 [debug] [Thread-1 (]: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 7:2 cannot recognize input near 'limit' '10' 'limit' in limit clause
[0m04:25:09.850521 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m04:25:09.851983 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_ext' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
  Error while compiling statement: FAILED: ParseException line 7:2 cannot recognize input near 'limit' '10' 'limit' in limit clause.
[0m04:25:09.854450 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:25:09.855550 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m04:25:09.856737 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error in model vehicle_ext (models/vehicle_poc/vehicle_ext.sql)
    Error while compiling statement: FAILED: ParseException line 7:2 cannot recognize input near 'limit' '10' 'limit' in limit clause
[0m04:25:09.861614 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 6.018161, "process_in_blocks": "0", "process_kernel_time": 0.90625, "process_mem_max_rss": "104868", "process_out_blocks": "0", "process_user_time": 1.65625}
[0m04:25:09.862652 [debug] [MainThread]: Command `dbt show` failed at 04:25:09.862414 after 6.02 seconds
[0m04:25:09.863492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d23dc2b40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d22fc00e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0d22d2e4b0>]}
[0m04:25:09.864318 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:09.865067 [debug] [MainThread]: Flushing usage events
[0m04:25:23.380162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a7b3d7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a7b3c2c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a7b3d400>]}
[0m04:25:23.383603 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 04:25:23.383929 | fa9260d4-8c51-4e2e-96f2-c9e93a1f1feb ==============================
[0m04:25:23.383929 [info ] [MainThread]: Running with dbt=1.9.2
[0m04:25:23.384883 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt show --select vehicle_ext', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m04:25:23.541413 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m04:25:23.541823 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m04:25:23.542095 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m04:25:23.662839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fa9260d4-8c51-4e2e-96f2-c9e93a1f1feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a8a21700>]}
[0m04:25:23.663230 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:23.706028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fa9260d4-8c51-4e2e-96f2-c9e93a1f1feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a7770bf0>]}
[0m04:25:23.706455 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:23.724757 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m04:25:23.886905 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m04:25:24.005941 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m04:25:24.008226 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m04:25:24.242618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fa9260d4-8c51-4e2e-96f2-c9e93a1f1feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a6e2fd70>]}
[0m04:25:24.243049 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:24.300180 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m04:25:24.303981 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m04:25:24.306501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fa9260d4-8c51-4e2e-96f2-c9e93a1f1feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a6dfdaf0>]}
[0m04:25:24.306865 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:24.307283 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m04:25:24.308152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa9260d4-8c51-4e2e-96f2-c9e93a1f1feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a6e551f0>]}
[0m04:25:24.308520 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:24.310141 [info ] [MainThread]: 
[0m04:25:24.310950 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:25:24.311888 [info ] [MainThread]: 
[0m04:25:24.313003 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m04:25:24.314372 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m04:25:24.322483 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m04:25:24.322911 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m04:25:24.323183 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m04:25:24.323406 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:25:26.164878 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m04:25:26.165692 [debug] [ThreadPool]: SQL status: OK in 1.842 seconds
[0m04:25:26.895705 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m04:25:26.896705 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m04:25:26.897501 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m04:25:26.897905 [debug] [ThreadPool]: On list_None_default: Close
[0m04:25:27.391658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa9260d4-8c51-4e2e-96f2-c9e93a1f1feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a6dfcda0>]}
[0m04:25:27.392678 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:27.412517 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_ext
[0m04:25:27.413784 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_ext)
[0m04:25:27.414799 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_ext
[0m04:25:27.428724 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_ext"
[0m04:25:27.431722 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_ext
[0m04:25:27.442701 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_ext"
[0m04:25:27.443397 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_ext"} */

  
  select * from vehicle_poc.vehicle_ext
  
  limit 5

[0m04:25:27.443888 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:25:29.456550 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m04:25:29.457744 [debug] [Thread-1 (]: SQL status: OK in 2.014 seconds
[0m04:25:30.543628 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_ext: Close
[0m04:25:31.035865 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_ext
[0m04:25:31.042239 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:25:31.043503 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_ext' was properly closed.
[0m04:25:31.046175 [debug] [MainThread]: Command end result
[0m04:25:31.064364 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m04:25:31.066839 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m04:25:31.073319 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m04:25:31.073898 [info ] [MainThread]: Previewing node 'vehicle_ext':
| vehicle_ext.vin | vehicle_ext.county | vehicle_ext.city | vehicle_ext.state | vehicle_ext.posta... | vehicle_ext.model... | ... |
| --------------- | ------------------ | ---------------- | ----------------- | -------------------- | -------------------- | --- |
| 1C4JJXP66P      | Kitsap             | Poulsbo          | WA                | 98370                | 2023                 | ... |
| 1G1FX6S08K      | Snohomish          | Lake Stevens     | WA                | 98258                | 2019                 | ... |
| WBY1Z2C58F      | King               | Seattle          | WA                | 98116                | 2015                 | ... |
| 5YJ3E1EBXK      | King               | Seattle          | WA                | 98178                | 2019                 | ... |
| 5YJSA1V24F      | Yakima             | Selah            | WA                | 98942                | 2015                 | ... |

[0m04:25:31.078032 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 7.7497053, "process_in_blocks": "0", "process_kernel_time": 0.984375, "process_mem_max_rss": "104788", "process_out_blocks": "0", "process_user_time": 1.578125}
[0m04:25:31.078579 [debug] [MainThread]: Command `dbt show` succeeded at 04:25:31.078466 after 7.75 seconds
[0m04:25:31.078922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a839e1e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a87feb40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4a9818ec0>]}
[0m04:25:31.079258 [debug] [MainThread]: An error was encountered while trying to send an event
[0m04:25:31.079526 [debug] [MainThread]: Flushing usage events
[0m05:46:28.998130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28aa97e120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28ac60a0f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28aa97e210>]}
[0m05:46:29.003088 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 05:46:29.003510 | 3305a499-55f1-4395-b4f2-6288f45c9019 ==============================
[0m05:46:29.003510 [info ] [MainThread]: Running with dbt=1.9.2
[0m05:46:29.004922 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation factory', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m05:46:29.238613 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m05:46:29.239140 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m05:46:29.239513 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m05:46:29.395509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3305a499-55f1-4395-b4f2-6288f45c9019', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28aa659be0>]}
[0m05:46:29.396000 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:46:29.453785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3305a499-55f1-4395-b4f2-6288f45c9019', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28ac399ee0>]}
[0m05:46:29.454291 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:46:29.454826 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m05:46:29.681122 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m05:46:29.846450 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:46:29.847128 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m05:46:29.847705 [error] [MainThread]: Encountered an error:
Compilation Error
  Got an unexpected control flow end tag, got endif but never saw a preceeding if (@ 28:4)
[0m05:46:29.851341 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 0.9211393, "process_in_blocks": "0", "process_kernel_time": 1.109375, "process_mem_max_rss": "97688", "process_out_blocks": "0", "process_user_time": 1.765625}
[0m05:46:29.851971 [debug] [MainThread]: Command `dbt run-operation` failed at 05:46:29.851838 after 0.92 seconds
[0m05:46:29.852413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28aac859a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28a9c6a7e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f28a9dc4f50>]}
[0m05:46:29.852771 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:46:29.853061 [debug] [MainThread]: Flushing usage events
[0m05:47:04.081599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9e2aef2c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9e49583e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9e300d940>]}
[0m05:47:04.087347 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 05:47:04.087899 | 3e033031-d377-40b2-8157-f4e44a6c1e18 ==============================
[0m05:47:04.087899 [info ] [MainThread]: Running with dbt=1.9.2
[0m05:47:04.089529 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation factory', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:47:04.301800 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m05:47:04.302336 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m05:47:04.302747 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m05:47:04.454475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3e033031-d377-40b2-8157-f4e44a6c1e18', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9e1eeb0b0>]}
[0m05:47:04.455012 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:04.508955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3e033031-d377-40b2-8157-f4e44a6c1e18', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9e32ab0e0>]}
[0m05:47:04.509461 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:04.510103 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m05:47:04.734392 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m05:47:04.885403 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:47:04.886068 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m05:47:04.972920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3e033031-d377-40b2-8157-f4e44a6c1e18', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9e1ce87d0>]}
[0m05:47:04.973443 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:05.041624 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m05:47:05.046834 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m05:47:05.070774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3e033031-d377-40b2-8157-f4e44a6c1e18', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9e1ae8d40>]}
[0m05:47:05.071541 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:05.072159 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m05:47:05.073695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3e033031-d377-40b2-8157-f4e44a6c1e18', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9e25e5460>]}
[0m05:47:05.074040 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:05.074639 [debug] [MainThread]: Acquiring new spark connection 'macro_factory'
[0m05:47:05.075260 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:05.075808 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:47:05.076569 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  dbt could not find a macro with the name "factory" in any package
[0m05:47:05.079673 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1182, in execute_macro
    raise DbtRuntimeError(
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  dbt could not find a macro with the name "factory" in any package

[0m05:47:05.080831 [error] [MainThread]: Encountered an error:
Internal Error
  dbt could not find a macro with the name 'factory' in any package
[0m05:47:05.086488 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 1.0750602, "process_in_blocks": "0", "process_kernel_time": 0.953125, "process_mem_max_rss": "98780", "process_out_blocks": "0", "process_user_time": 1.90625}
[0m05:47:05.087532 [debug] [MainThread]: Command `dbt run-operation` failed at 05:47:05.087392 after 1.08 seconds
[0m05:47:05.087940 [debug] [MainThread]: Connection 'macro_factory' was properly closed.
[0m05:47:05.088458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9e31f9b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9e1d6f110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9e1dc65d0>]}
[0m05:47:05.089065 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:05.090015 [debug] [MainThread]: Flushing usage events
[0m05:47:28.638110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d9a22f2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d9bc46300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d99fbb5f0>]}
[0m05:47:28.643093 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 05:47:28.643496 | 9f663c52-22d6-42e2-9e97-428b3f12b6fb ==============================
[0m05:47:28.643496 [info ] [MainThread]: Running with dbt=1.9.2
[0m05:47:28.646017 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run-operation factory', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m05:47:28.863530 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m05:47:28.864060 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m05:47:28.864428 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m05:47:29.028847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9f663c52-22d6-42e2-9e97-428b3f12b6fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d99474140>]}
[0m05:47:29.029324 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:29.089695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9f663c52-22d6-42e2-9e97-428b3f12b6fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d99930800>]}
[0m05:47:29.090204 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:29.090719 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m05:47:29.310381 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m05:47:29.460725 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:47:29.461377 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m05:47:29.536424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9f663c52-22d6-42e2-9e97-428b3f12b6fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d994e1160>]}
[0m05:47:29.536910 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:29.731224 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m05:47:29.749068 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m05:47:29.807565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9f663c52-22d6-42e2-9e97-428b3f12b6fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d99230da0>]}
[0m05:47:29.808828 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:29.809875 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m05:47:29.812013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9f663c52-22d6-42e2-9e97-428b3f12b6fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d99edf7a0>]}
[0m05:47:29.812988 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:29.814463 [debug] [MainThread]: Acquiring new spark connection 'macro_factory'
[0m05:47:29.815490 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:47:29.816400 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:47:29.854367 [debug] [MainThread]: Using spark connection "macro_factory"
[0m05:47:29.856065 [debug] [MainThread]: On macro_factory: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_factory"} */

    

        USE vehicle_poc;

        CREATE or replace external TABLE vehicle_ext (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES (
            'separatorChar' = ','
        )
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m05:47:29.857310 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:47:31.516644 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_factory"} */

    

        USE vehicle_poc;

        CREATE or replace external TABLE vehicle_ext (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES (
            'separatorChar' = ','
        )
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m05:47:31.518469 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 5:23 missing EOF at ';' near 'vehicle_poc':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 5:23 missing EOF at ';' near 'vehicle_poc':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 5:23 missing EOF at ';' near 'vehicle_poc'"), operationHandle=None)
[0m05:47:31.520266 [debug] [MainThread]: Spark adapter: Error while running:
macro factory
[0m05:47:31.521264 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 5:23 missing EOF at ';' near 'vehicle_poc'
[0m05:47:31.522312 [debug] [MainThread]: On macro_factory: ROLLBACK
[0m05:47:31.523244 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m05:47:31.524117 [debug] [MainThread]: On macro_factory: Close
[0m05:47:31.780268 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 5:23 missing EOF at ';' near 'vehicle_poc'
[0m05:47:31.802280 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 5:23 missing EOF at ';' near 'vehicle_poc':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 5:23 missing EOF at ';' near 'vehicle_poc':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 5:23 missing EOF at ';' near 'vehicle_poc'"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 35, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 33, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 5:23 missing EOF at ';' near 'vehicle_poc'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 5:23 missing EOF at ';' near 'vehicle_poc'

[0m05:47:31.824981 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m05:47:31.829158 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 3.2579892, "process_in_blocks": "0", "process_kernel_time": 1.109375, "process_mem_max_rss": "99264", "process_out_blocks": "0", "process_user_time": 2.1875}
[0m05:47:31.830603 [debug] [MainThread]: Command `dbt run-operation` failed at 05:47:31.830280 after 3.26 seconds
[0m05:47:31.831639 [debug] [MainThread]: Connection 'macro_factory' was properly closed.
[0m05:47:31.832632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d9a0baae0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d9a9bf320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d9a9be1e0>]}
[0m05:47:31.833705 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:47:31.834383 [debug] [MainThread]: Flushing usage events
[0m05:48:09.807840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1b29ae540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1b26463f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1b131bbc0>]}
[0m05:48:09.812623 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 05:48:09.813045 | 746fd989-3f23-4a31-9e71-2276ff589bf2 ==============================
[0m05:48:09.813045 [info ] [MainThread]: Running with dbt=1.9.2
[0m05:48:09.815228 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run-operation factory', 'send_anonymous_usage_stats': 'True'}
[0m05:48:10.036643 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m05:48:10.037497 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m05:48:10.038220 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m05:48:10.199007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '746fd989-3f23-4a31-9e71-2276ff589bf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1b14c2300>]}
[0m05:48:10.199493 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:10.253244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '746fd989-3f23-4a31-9e71-2276ff589bf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1aff45df0>]}
[0m05:48:10.253755 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:10.254257 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m05:48:10.473946 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m05:48:10.629150 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:48:10.629841 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m05:48:10.706363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '746fd989-3f23-4a31-9e71-2276ff589bf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1afe312b0>]}
[0m05:48:10.706878 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:10.777440 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m05:48:10.782153 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m05:48:10.823207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '746fd989-3f23-4a31-9e71-2276ff589bf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1afc0ce60>]}
[0m05:48:10.824084 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:10.825399 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m05:48:10.827366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '746fd989-3f23-4a31-9e71-2276ff589bf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1afede9c0>]}
[0m05:48:10.828362 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:10.829847 [debug] [MainThread]: Acquiring new spark connection 'macro_factory'
[0m05:48:10.830884 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:10.831816 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:48:10.864752 [debug] [MainThread]: Using spark connection "macro_factory"
[0m05:48:10.866131 [debug] [MainThread]: On macro_factory: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_factory"} */

    

        USE vehicle_poc

        CREATE or replace external TABLE vehicle_ext (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES (
            'separatorChar' = ','
        )
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m05:48:10.867247 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:48:12.475191 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_factory"} */

    

        USE vehicle_poc

        CREATE or replace external TABLE vehicle_ext (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES (
            'separatorChar' = ','
        )
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m05:48:12.477024 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:8 missing EOF at 'CREATE' near 'vehicle_poc':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:8 missing EOF at 'CREATE' near 'vehicle_poc':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:8 missing EOF at 'CREATE' near 'vehicle_poc'"), operationHandle=None)
[0m05:48:12.478768 [debug] [MainThread]: Spark adapter: Error while running:
macro factory
[0m05:48:12.479794 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:8 missing EOF at 'CREATE' near 'vehicle_poc'
[0m05:48:12.480849 [debug] [MainThread]: On macro_factory: ROLLBACK
[0m05:48:12.481785 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m05:48:12.482684 [debug] [MainThread]: On macro_factory: Close
[0m05:48:12.793257 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:8 missing EOF at 'CREATE' near 'vehicle_poc'
[0m05:48:12.813663 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 7:8 missing EOF at 'CREATE' near 'vehicle_poc':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 7:8 missing EOF at 'CREATE' near 'vehicle_poc':35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:241', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 7:8 missing EOF at 'CREATE' near 'vehicle_poc'"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 35, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 33, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 7:8 missing EOF at 'CREATE' near 'vehicle_poc'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 7:8 missing EOF at 'CREATE' near 'vehicle_poc'

[0m05:48:12.836593 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m05:48:12.840002 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 3.1081405, "process_in_blocks": "0", "process_kernel_time": 1.171875, "process_mem_max_rss": "99272", "process_out_blocks": "0", "process_user_time": 1.890625}
[0m05:48:12.841730 [debug] [MainThread]: Command `dbt run-operation` failed at 05:48:12.841403 after 3.11 seconds
[0m05:48:12.842797 [debug] [MainThread]: Connection 'macro_factory' was properly closed.
[0m05:48:12.843810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1b092b9b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1afc0d5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1b00d5160>]}
[0m05:48:12.844718 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:12.845538 [debug] [MainThread]: Flushing usage events
[0m05:48:33.494778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff800bd8470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff80011f7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff80011fd70>]}
[0m05:48:33.499446 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 05:48:33.499846 | 21a50b11-440e-49dc-b786-4e1eb8b5f9e1 ==============================
[0m05:48:33.499846 [info ] [MainThread]: Running with dbt=1.9.2
[0m05:48:33.501372 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run-operation factory', 'send_anonymous_usage_stats': 'True'}
[0m05:48:33.719638 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m05:48:33.720209 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m05:48:33.720607 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m05:48:33.878328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '21a50b11-440e-49dc-b786-4e1eb8b5f9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff800a87530>]}
[0m05:48:33.878825 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:33.933403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '21a50b11-440e-49dc-b786-4e1eb8b5f9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7fff84b60>]}
[0m05:48:33.933904 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:33.934451 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m05:48:34.146109 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m05:48:34.297650 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:48:34.298394 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m05:48:34.372663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '21a50b11-440e-49dc-b786-4e1eb8b5f9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7ff742ff0>]}
[0m05:48:34.373149 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:34.440488 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m05:48:34.445492 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m05:48:34.465814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '21a50b11-440e-49dc-b786-4e1eb8b5f9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7ff46cdd0>]}
[0m05:48:34.466279 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:34.466641 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m05:48:34.467856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21a50b11-440e-49dc-b786-4e1eb8b5f9e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7ff742bd0>]}
[0m05:48:34.468481 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:34.470157 [debug] [MainThread]: Acquiring new spark connection 'macro_factory'
[0m05:48:34.471412 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:34.472310 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:48:34.510895 [debug] [MainThread]: Using spark connection "macro_factory"
[0m05:48:34.512446 [debug] [MainThread]: On macro_factory: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_factory"} */

    

        CREATE or replace external TABLE vehicle_poc.vehicle_ext (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES (
            'separatorChar' = ','
        )
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m05:48:34.513834 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:48:36.107510 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_factory"} */

    

        CREATE or replace external TABLE vehicle_poc.vehicle_ext (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES (
            'separatorChar' = ','
        )
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m05:48:36.109405 [debug] [MainThread]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 5:26 mismatched input 'external' expecting KW_VIEW near 'replace' in create view statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 5:26 mismatched input 'external' expecting KW_VIEW near 'replace' in create view statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 5:26 mismatched input 'external' expecting KW_VIEW near 'replace' in create view statement"), operationHandle=None)
[0m05:48:36.110925 [debug] [MainThread]: Spark adapter: Error while running:
macro factory
[0m05:48:36.111917 [debug] [MainThread]: Spark adapter: Runtime Error
  Error while compiling statement: FAILED: ParseException line 5:26 mismatched input 'external' expecting KW_VIEW near 'replace' in create view statement
[0m05:48:36.112941 [debug] [MainThread]: On macro_factory: ROLLBACK
[0m05:48:36.113840 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m05:48:36.114722 [debug] [MainThread]: On macro_factory: Close
[0m05:48:36.367480 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 5:26 mismatched input 'external' expecting KW_VIEW near 'replace' in create view statement
[0m05:48:36.391971 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 278, in execute
    self._cursor.execute(sql, bindings, async_=True)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 481, in execute
    _check_status(response)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/pyhive/hive.py", line 611, in _check_status
    raise OperationalError(response)
pyhive.exc.OperationalError: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 5:26 mismatched input 'external' expecting KW_VIEW near 'replace' in create view statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 5:26 mismatched input 'external' expecting KW_VIEW near 'replace' in create view statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 5:26 mismatched input 'external' expecting KW_VIEW near 'replace' in create view statement"), operationHandle=None)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 35, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 33, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 365, in exception_handler
    raise DbtRuntimeError(msg)
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Error while compiling statement: FAILED: ParseException line 5:26 mismatched input 'external' expecting KW_VIEW near 'replace' in create view statement

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Error while compiling statement: FAILED: ParseException line 5:26 mismatched input 'external' expecting KW_VIEW near 'replace' in create view statement

[0m05:48:36.414772 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m05:48:36.418659 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 3.005622, "process_in_blocks": "0", "process_kernel_time": 1.015625, "process_mem_max_rss": "99352", "process_out_blocks": "0", "process_user_time": 2.0}
[0m05:48:36.420182 [debug] [MainThread]: Command `dbt run-operation` failed at 05:48:36.419811 after 3.01 seconds
[0m05:48:36.421276 [debug] [MainThread]: Connection 'macro_factory' was properly closed.
[0m05:48:36.422347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff80011fcb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff800bfb200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff800bf9fd0>]}
[0m05:48:36.423277 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:36.424088 [debug] [MainThread]: Flushing usage events
[0m05:48:54.395457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe28679dee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe28679d2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe28679dca0>]}
[0m05:48:54.401735 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 05:48:54.402295 | e476625e-9ba9-4bed-be8a-f25d681390fb ==============================
[0m05:48:54.402295 [info ] [MainThread]: Running with dbt=1.9.2
[0m05:48:54.404297 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run-operation factory', 'send_anonymous_usage_stats': 'True'}
[0m05:48:54.622663 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m05:48:54.623218 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m05:48:54.623594 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m05:48:54.777323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e476625e-9ba9-4bed-be8a-f25d681390fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2872b0200>]}
[0m05:48:54.777817 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:54.831002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e476625e-9ba9-4bed-be8a-f25d681390fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe286783a40>]}
[0m05:48:54.831511 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:54.832029 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m05:48:55.054035 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m05:48:55.209263 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:48:55.209901 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m05:48:55.293517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e476625e-9ba9-4bed-be8a-f25d681390fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2867c7d10>]}
[0m05:48:55.294023 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:55.359845 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m05:48:55.364546 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m05:48:55.384904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e476625e-9ba9-4bed-be8a-f25d681390fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe285cccec0>]}
[0m05:48:55.385396 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:55.385764 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m05:48:55.387152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e476625e-9ba9-4bed-be8a-f25d681390fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe285f4e9c0>]}
[0m05:48:55.387816 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:55.388750 [debug] [MainThread]: Acquiring new spark connection 'macro_factory'
[0m05:48:55.389375 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:48:55.389874 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:48:55.413400 [debug] [MainThread]: Using spark connection "macro_factory"
[0m05:48:55.414840 [debug] [MainThread]: On macro_factory: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_factory"} */

    

        CREATE external TABLE vehicle_poc.vehicle_ext (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES (
            'separatorChar' = ','
        )
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m05:48:55.415971 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:48:57.433335 [debug] [MainThread]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='08S01', errorCode=1, errorMessage='Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table hive.vehicle_poc.vehicle_ext already exists)', taskStatus='[{"returnValue":1,"errorMsg":"org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Table hive.vehicle_poc.vehicle_ext already exists)","beginTime":1742363336773,"endTime":1742363336781,"taskId":"Stage-0","taskState":"FINISHED","taskType":"DDL","name":"DDL"}]', operationStarted=1742363336742, operationCompleted=1742363336782, hasResultSet=False, progressUpdateResponse=TProgressUpdateResp(headerNames=[], rows=[], progressedPercentage=0.0, status=2, footerSummary='', startTime=0))
[0m05:48:57.434741 [debug] [MainThread]: Spark adapter: Poll status: 5
[0m05:48:57.435931 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_factory"} */

    

        CREATE external TABLE vehicle_poc.vehicle_ext (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES (
            'separatorChar' = ','
        )
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m05:48:57.437030 [debug] [MainThread]: Spark adapter: Database Error
  Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table hive.vehicle_poc.vehicle_ext already exists)
[0m05:48:57.438151 [debug] [MainThread]: Spark adapter: Error while running:
macro factory
[0m05:48:57.439112 [debug] [MainThread]: Spark adapter: Runtime Error
  Database Error
    Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table hive.vehicle_poc.vehicle_ext already exists)
[0m05:48:57.440218 [debug] [MainThread]: On macro_factory: ROLLBACK
[0m05:48:57.441162 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m05:48:57.442034 [debug] [MainThread]: On macro_factory: Close
[0m05:48:58.045870 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Database Error
      Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table hive.vehicle_poc.vehicle_ext already exists)
[0m05:48:58.068106 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 300, in execute
    raise DbtDatabaseError(poll_state.errorMessage)
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table hive.vehicle_poc.vehicle_ext already exists)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 35, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 33, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Database Error
    Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table hive.vehicle_poc.vehicle_ext already exists)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Database Error
      Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table hive.vehicle_poc.vehicle_ext already exists)

[0m05:48:58.090443 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m05:48:58.092590 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 3.7684803, "process_in_blocks": "0", "process_kernel_time": 1.046875, "process_mem_max_rss": "99220", "process_out_blocks": "0", "process_user_time": 1.78125}
[0m05:48:58.093831 [debug] [MainThread]: Command `dbt run-operation` failed at 05:48:58.093379 after 3.77 seconds
[0m05:48:58.095465 [debug] [MainThread]: Connection 'macro_factory' was properly closed.
[0m05:48:58.097130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe28679d040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe28679d2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe285f4e480>]}
[0m05:48:58.098560 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:48:58.099905 [debug] [MainThread]: Flushing usage events
[0m05:49:32.115937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2aa189b410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2aa3526480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2aa1f9ac00>]}
[0m05:49:32.120713 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 05:49:32.121162 | dd847272-412c-4f91-b7ff-083d2c6fd2dc ==============================
[0m05:49:32.121162 [info ] [MainThread]: Running with dbt=1.9.2
[0m05:49:32.122681 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation factory', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m05:49:32.330588 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m05:49:32.331121 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m05:49:32.331534 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m05:49:32.485541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dd847272-412c-4f91-b7ff-083d2c6fd2dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2aa1631430>]}
[0m05:49:32.486312 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:49:32.544582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dd847272-412c-4f91-b7ff-083d2c6fd2dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2aa1000200>]}
[0m05:49:32.545131 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:49:32.545721 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m05:49:32.765561 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m05:49:32.914250 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m05:49:32.914726 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m05:49:32.948879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dd847272-412c-4f91-b7ff-083d2c6fd2dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2aa0d57f20>]}
[0m05:49:32.949346 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:49:33.018145 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m05:49:33.024970 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m05:49:33.044849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dd847272-412c-4f91-b7ff-083d2c6fd2dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2aa0aa4d40>]}
[0m05:49:33.045336 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:49:33.045847 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m05:49:33.047458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dd847272-412c-4f91-b7ff-083d2c6fd2dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2aa0e691c0>]}
[0m05:49:33.047978 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:49:33.048580 [debug] [MainThread]: Acquiring new spark connection 'macro_factory'
[0m05:49:33.048951 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:49:33.049294 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:49:33.059782 [debug] [MainThread]: Using spark connection "macro_factory"
[0m05:49:33.060299 [debug] [MainThread]: On macro_factory: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_factory"} */

    

        CREATE external TABLE vehicle_poc.vehicle_ext (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES (
            'separatorChar' = ','
        )
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m05:49:33.060653 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:49:35.012564 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m05:49:35.013931 [debug] [MainThread]: SQL status: OK in 1.953 seconds
[0m05:49:35.017375 [debug] [MainThread]: On macro_factory: ROLLBACK
[0m05:49:35.018520 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m05:49:35.019697 [debug] [MainThread]: On macro_factory: Close
[0m05:49:35.646703 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m05:49:35.650444 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 3.6051993, "process_in_blocks": "0", "process_kernel_time": 1.1875, "process_mem_max_rss": "98460", "process_out_blocks": "0", "process_user_time": 1.765625}
[0m05:49:35.651866 [debug] [MainThread]: Command `dbt run-operation` succeeded at 05:49:35.651550 after 3.61 seconds
[0m05:49:35.652844 [debug] [MainThread]: Connection 'macro_factory' was properly closed.
[0m05:49:35.653836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2aa0b4ba40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2aa1f9ac00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2aa17c3d10>]}
[0m05:49:35.654919 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:49:35.655768 [debug] [MainThread]: Flushing usage events
[0m05:51:17.263166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcebaca8bc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fceba2f2060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fceb9407f50>]}
[0m05:51:17.269779 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 05:51:17.270355 | f4d7c769-acc7-4844-8106-f79d840dbdfe ==============================
[0m05:51:17.270355 [info ] [MainThread]: Running with dbt=1.9.2
[0m05:51:17.272138 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation create_external_table', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m05:51:17.483386 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m05:51:17.483926 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m05:51:17.484307 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m05:51:17.646504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f4d7c769-acc7-4844-8106-f79d840dbdfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fceb85c3500>]}
[0m05:51:17.647079 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:51:17.703490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f4d7c769-acc7-4844-8106-f79d840dbdfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fceba170d40>]}
[0m05:51:17.704021 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:51:17.704551 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m05:51:17.944688 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m05:51:18.120583 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m05:51:18.121315 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m05:51:18.214535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f4d7c769-acc7-4844-8106-f79d840dbdfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fceb8890aa0>]}
[0m05:51:18.215244 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:51:18.343818 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m05:51:18.364398 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m05:51:18.432134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f4d7c769-acc7-4844-8106-f79d840dbdfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fceb83a0e00>]}
[0m05:51:18.434262 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:51:18.435990 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m05:51:18.438520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4d7c769-acc7-4844-8106-f79d840dbdfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fceb8799a00>]}
[0m05:51:18.439507 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:51:18.440979 [debug] [MainThread]: Acquiring new spark connection 'macro_create_external_table'
[0m05:51:18.442060 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m05:51:18.443020 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m05:51:18.477259 [debug] [MainThread]: Using spark connection "macro_create_external_table"
[0m05:51:18.478861 [debug] [MainThread]: On macro_create_external_table: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_create_external_table"} */

    

        CREATE external TABLE if not exists vehicle_poc.vehicle_ext (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES (
            'separatorChar' = ','
        )
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m05:51:18.480070 [debug] [MainThread]: Opening a new connection, currently in state init
[0m05:51:20.481540 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m05:51:20.482930 [debug] [MainThread]: SQL status: OK in 2.003 seconds
[0m05:51:20.486395 [debug] [MainThread]: On macro_create_external_table: ROLLBACK
[0m05:51:20.487508 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m05:51:20.488503 [debug] [MainThread]: On macro_create_external_table: Close
[0m05:51:21.116715 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m05:51:21.120597 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 3.9319336, "process_in_blocks": "0", "process_kernel_time": 1.0, "process_mem_max_rss": "98952", "process_out_blocks": "0", "process_user_time": 2.03125}
[0m05:51:21.122095 [debug] [MainThread]: Command `dbt run-operation` succeeded at 05:51:21.121750 after 3.93 seconds
[0m05:51:21.123129 [debug] [MainThread]: Connection 'macro_create_external_table' was properly closed.
[0m05:51:21.124199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fceb912bfb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fceb83637d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fceb9656a20>]}
[0m05:51:21.125177 [debug] [MainThread]: An error was encountered while trying to send an event
[0m05:51:21.126052 [debug] [MainThread]: Flushing usage events
[0m06:16:24.670261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854efde3c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854f072b40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854f071d30>]}
[0m06:16:24.673995 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 06:16:24.674317 | fc1dba42-75a4-4e12-9976-5dac8ac8376f ==============================
[0m06:16:24.674317 [info ] [MainThread]: Running with dbt=1.9.2
[0m06:16:24.675445 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --select vehicle_bronze', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m06:16:24.861649 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:16:24.862071 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:16:24.862353 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:16:24.985887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fc1dba42-75a4-4e12-9976-5dac8ac8376f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854f04ed20>]}
[0m06:16:24.986278 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:16:25.032730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fc1dba42-75a4-4e12-9976-5dac8ac8376f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8550ad1b50>]}
[0m06:16:25.033182 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:16:25.033638 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m06:16:25.197780 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m06:16:25.322295 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 1 files changed.
[0m06:16:25.322781 [debug] [MainThread]: Partial parsing: added file: dbt_spark_emr_app://models/vehicle_poc/vehicle_bronze.sql
[0m06:16:25.323182 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/schema.yml
[0m06:16:25.323428 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_emr_app://models/vehicle_poc/vehicle_ext.sql
[0m06:16:25.538314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fc1dba42-75a4-4e12-9976-5dac8ac8376f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854cce10a0>]}
[0m06:16:25.538694 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:16:25.578788 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m06:16:25.582597 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m06:16:25.604970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fc1dba42-75a4-4e12-9976-5dac8ac8376f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854fb097c0>]}
[0m06:16:25.605546 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:16:25.605841 [info ] [MainThread]: Found 1 model, 474 macros
[0m06:16:25.606550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc1dba42-75a4-4e12-9976-5dac8ac8376f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854cce2ea0>]}
[0m06:16:25.606792 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:16:25.607751 [info ] [MainThread]: 
[0m06:16:25.608368 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:16:25.608919 [info ] [MainThread]: 
[0m06:16:25.609591 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m06:16:25.610622 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m06:16:25.618699 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m06:16:25.619136 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m06:16:25.619414 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:16:27.449732 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m06:16:27.450895 [debug] [ThreadPool]: SQL status: OK in 1.831 seconds
[0m06:16:28.219084 [debug] [ThreadPool]: On list_schemas: Close
[0m06:16:28.729547 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m06:16:28.741524 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m06:16:28.742596 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m06:16:28.743523 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m06:16:28.744262 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:16:30.811760 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m06:16:30.812970 [debug] [ThreadPool]: SQL status: OK in 2.069 seconds
[0m06:16:31.578393 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m06:16:31.579346 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m06:16:31.579687 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m06:16:31.579992 [debug] [ThreadPool]: On list_None_default: Close
[0m06:16:32.088142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc1dba42-75a4-4e12-9976-5dac8ac8376f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854cdd2f30>]}
[0m06:16:32.089179 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:16:32.090101 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:16:32.090918 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:16:32.110492 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_bronze
[0m06:16:32.111918 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_bronze ............................ [RUN]
[0m06:16:32.113755 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_bronze)
[0m06:16:32.114394 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_bronze
[0m06:16:32.125965 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_bronze"
[0m06:16:32.130838 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_bronze
[0m06:16:32.170795 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_bronze"
[0m06:16:32.171490 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_bronze"} */
drop table if exists default.vehicle_bronze
[0m06:16:32.171945 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:16:34.173804 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m06:16:34.175038 [debug] [Thread-1 (]: SQL status: OK in 2.003 seconds
[0m06:16:34.213691 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_bronze"
[0m06:16:34.220867 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m06:16:34.221852 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_bronze"
[0m06:16:34.222729 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_bronze"} */

  
    
        create table default.vehicle_bronze
      
      
      
      
      
      
      
      

      as
      create or replace table vehicle_poc.vehicle_bronze
as
select * from vehicle_poc.vehicle_ext
  
[0m06:16:34.479448 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_bronze"} */

  
    
        create table default.vehicle_bronze
      
      
      
      
      
      
      
      

      as
      create or replace table vehicle_poc.vehicle_bronze
as
select * from vehicle_poc.vehicle_ext
  
[0m06:16:34.480965 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'create' 'or' 'replace' in create table statement:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 16:6 cannot recognize input near 'create' 'or' 'replace' in create table statement:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'create' 'or' 'replace' in create table statement"), operationHandle=None)
[0m06:16:34.482355 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: ROLLBACK
[0m06:16:34.483320 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m06:16:34.484236 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: Close
[0m06:16:34.755735 [debug] [Thread-1 (]: Runtime Error in model vehicle_bronze (models/vehicle_poc/vehicle_bronze.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'create' 'or' 'replace' in create table statement
[0m06:16:34.760538 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc1dba42-75a4-4e12-9976-5dac8ac8376f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854e0f63c0>]}
[0m06:16:34.761380 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m06:16:34.762622 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_bronze ................... [[31mERROR[0m in 2.64s]
[0m06:16:34.764835 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_bronze
[0m06:16:34.765753 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_bronze' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_bronze (models/vehicle_poc/vehicle_bronze.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'create' 'or' 'replace' in create table statement.
[0m06:16:34.770714 [debug] [MainThread]: On master: ROLLBACK
[0m06:16:34.771645 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:16:36.314521 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m06:16:36.315299 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:16:36.315782 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:16:36.316211 [debug] [MainThread]: On master: ROLLBACK
[0m06:16:36.316676 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m06:16:36.316936 [debug] [MainThread]: On master: Close
[0m06:16:36.571304 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:16:36.571930 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_bronze' was properly closed.
[0m06:16:36.572591 [info ] [MainThread]: 
[0m06:16:36.574437 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.96 seconds (10.96s).
[0m06:16:36.575757 [debug] [MainThread]: Command end result
[0m06:16:36.606382 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m06:16:36.609122 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m06:16:36.625515 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m06:16:36.626971 [info ] [MainThread]: 
[0m06:16:36.628998 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m06:16:36.630792 [info ] [MainThread]: 
[0m06:16:36.632884 [error] [MainThread]:   Runtime Error in model vehicle_bronze (models/vehicle_poc/vehicle_bronze.sql)
  Error while compiling statement: FAILED: ParseException line 16:6 cannot recognize input near 'create' 'or' 'replace' in create table statement
[0m06:16:36.635341 [info ] [MainThread]: 
[0m06:16:36.636339 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m06:16:36.638820 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 12.021546, "process_in_blocks": "0", "process_kernel_time": 1.125, "process_mem_max_rss": "105916", "process_out_blocks": "0", "process_user_time": 1.625}
[0m06:16:36.639536 [debug] [MainThread]: Command `dbt run` failed at 06:16:36.639395 after 12.02 seconds
[0m06:16:36.640038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854de89040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854f77d8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f854e39ec30>]}
[0m06:16:36.640405 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:16:36.640712 [debug] [MainThread]: Flushing usage events
[0m06:46:50.325830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7144afd9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7144afc590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7144afd8b0>]}
[0m06:46:50.361859 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 06:46:50.363102 | 46a9fc9e-712c-46e1-95e3-71be5f7c5c9a ==============================
[0m06:46:50.363102 [info ] [MainThread]: Running with dbt=1.9.2
[0m06:46:50.365689 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run --select vehicle_bronze', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m06:46:56.099512 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:46:56.101034 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:46:56.102201 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:46:56.668386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '46a9fc9e-712c-46e1-95e3-71be5f7c5c9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7148513140>]}
[0m06:46:56.669705 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:46:56.855596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '46a9fc9e-712c-46e1-95e3-71be5f7c5c9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71453b1160>]}
[0m06:46:56.856941 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:46:56.858476 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m06:46:57.780806 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m06:46:59.363030 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:46:59.364990 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_bronze.sql
[0m06:47:00.318844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '46a9fc9e-712c-46e1-95e3-71be5f7c5c9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f714442a030>]}
[0m06:47:00.320110 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:47:00.481982 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m06:47:00.503982 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m06:47:00.868653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '46a9fc9e-712c-46e1-95e3-71be5f7c5c9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71442ff950>]}
[0m06:47:00.869378 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:47:00.870043 [info ] [MainThread]: Found 1 model, 474 macros
[0m06:47:00.871534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '46a9fc9e-712c-46e1-95e3-71be5f7c5c9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f714403d9d0>]}
[0m06:47:00.872484 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:47:00.876149 [info ] [MainThread]: 
[0m06:47:00.877595 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m06:47:00.879174 [info ] [MainThread]: 
[0m06:47:00.881516 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m06:47:00.884873 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m06:47:00.915647 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m06:47:00.917131 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m06:47:00.918275 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:47:02.942360 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m06:47:02.943723 [debug] [ThreadPool]: SQL status: OK in 2.025 seconds
[0m06:47:03.915623 [debug] [ThreadPool]: On list_schemas: Close
[0m06:47:04.570619 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m06:47:04.586190 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m06:47:04.587498 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m06:47:04.588568 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m06:47:04.589585 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m06:47:06.977882 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m06:47:06.979246 [debug] [ThreadPool]: SQL status: OK in 2.390 seconds
[0m06:47:07.802023 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m06:47:07.803374 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m06:47:07.804448 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m06:47:07.805771 [debug] [ThreadPool]: On list_None_default: Close
[0m06:47:08.356906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '46a9fc9e-712c-46e1-95e3-71be5f7c5c9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71442fffb0>]}
[0m06:47:08.358223 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:47:08.359345 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:47:08.360381 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:47:08.388471 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_bronze
[0m06:47:08.390514 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_bronze ............................ [RUN]
[0m06:47:08.393325 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_bronze)
[0m06:47:08.394582 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_bronze
[0m06:47:08.415712 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_bronze"
[0m06:47:08.421085 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_bronze
[0m06:47:08.497119 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_bronze"
[0m06:47:08.498809 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_bronze"} */
drop table if exists default.vehicle_bronze
[0m06:47:08.499963 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m06:47:11.132074 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m06:47:11.133538 [debug] [Thread-1 (]: SQL status: OK in 2.633 seconds
[0m06:47:11.254573 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_bronze"
[0m06:47:11.259351 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m06:47:11.260614 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_bronze"
[0m06:47:11.261746 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_bronze"} */

  
    
        create table default.vehicle_bronze
      
      
      
      
      
      
      
      

      as
      select * from vehicle_poc.vehicle_ext
  
[0m06:47:12.453416 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:13.272880 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:14.091184 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:14.913510 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:15.731922 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:16.495091 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:17.367713 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:18.186563 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:19.007740 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:19.768629 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:20.545816 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:21.362772 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:22.692387 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m06:47:23.000043 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='08S01', errorCode=2, errorMessage="Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0004_1_00, diagnostics=[Vertex vertex_1741739386091_0004_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0004_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')\n\tat org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)\n\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)\n\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)\n\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)\n\tat org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)\n\tat com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)\n\tat com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)\n\tat com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0", taskStatus='[{"returnValue":2,"errorMsg":"org.apache.hadoop.hive.ql.metadata.HiveException: Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0004_1_00, diagnostics=[Vertex vertex_1741739386091_0004_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0004_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader \'bootstrap\'; org.apache.hadoop.fs.Seekable is in unnamed module of loader \'app\')\\n\\tat org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)\\n\\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)\\n\\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)\\n\\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)\\n\\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)\\n\\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)\\n\\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)\\n\\tat org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)\\n\\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\\n\\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)\\n\\tat com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)\\n\\tat com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)\\n\\tat com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0","beginTime":1742366831160,"endTime":1742366842327,"taskId":"Stage-1","taskState":"FINISHED","taskType":"MAPRED","name":"TEZ"}]', operationStarted=1742366831102, operationCompleted=1742366842328, hasResultSet=False, progressUpdateResponse=TProgressUpdateResp(headerNames=[], rows=[], progressedPercentage=0.0, status=2, footerSummary='', startTime=0))
[0m06:47:23.002148 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m06:47:23.003695 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_bronze"} */

  
    
        create table default.vehicle_bronze
      
      
      
      
      
      
      
      

      as
      select * from vehicle_poc.vehicle_ext
  
[0m06:47:23.005106 [debug] [Thread-1 (]: Spark adapter: Database Error
  Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0004_1_00, diagnostics=[Vertex vertex_1741739386091_0004_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0004_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
  	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
  	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
  	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
  	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
  	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
  	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
  	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
  	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
[0m06:47:23.006768 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: ROLLBACK
[0m06:47:23.007902 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m06:47:23.008941 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: Close
[0m06:47:23.904636 [debug] [Thread-1 (]: Runtime Error in model vehicle_bronze (models/vehicle_poc/vehicle_bronze.sql)
  Database Error
    Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0004_1_00, diagnostics=[Vertex vertex_1741739386091_0004_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0004_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
    	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
    	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
    	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
    	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
[0m06:47:23.913579 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '46a9fc9e-712c-46e1-95e3-71be5f7c5c9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7142c1b170>]}
[0m06:47:23.914776 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m06:47:23.916638 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_bronze ................... [[31mERROR[0m in 15.51s]
[0m06:47:23.920164 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_bronze
[0m06:47:23.922164 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_bronze' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_bronze (models/vehicle_poc/vehicle_bronze.sql)
  Database Error
    Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0004_1_00, diagnostics=[Vertex vertex_1741739386091_0004_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0004_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
    	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
    	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
    	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
    	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0.
[0m06:47:23.932355 [debug] [MainThread]: On master: ROLLBACK
[0m06:47:23.933561 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:47:25.456714 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m06:47:25.458059 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m06:47:25.459048 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m06:47:25.459987 [debug] [MainThread]: On master: ROLLBACK
[0m06:47:25.460898 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m06:47:25.461746 [debug] [MainThread]: On master: Close
[0m06:47:25.764137 [debug] [MainThread]: Connection 'master' was properly closed.
[0m06:47:25.765426 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_bronze' was properly closed.
[0m06:47:25.766453 [info ] [MainThread]: 
[0m06:47:25.768941 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 24.89 seconds (24.89s).
[0m06:47:25.772117 [debug] [MainThread]: Command end result
[0m06:47:25.830925 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m06:47:25.838472 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m06:47:25.856777 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m06:47:25.857959 [info ] [MainThread]: 
[0m06:47:25.860507 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m06:47:25.862519 [info ] [MainThread]: 
[0m06:47:25.864808 [error] [MainThread]:   Runtime Error in model vehicle_bronze (models/vehicle_poc/vehicle_bronze.sql)
  Database Error
    Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0004_1_00, diagnostics=[Vertex vertex_1741739386091_0004_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0004_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
    	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
    	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
    	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
    	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
[0m06:47:25.885495 [info ] [MainThread]: 
[0m06:47:25.887960 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m06:47:25.915692 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 35.91279, "process_in_blocks": "0", "process_kernel_time": 5.828125, "process_mem_max_rss": "105708", "process_out_blocks": "0", "process_user_time": 7.140625}
[0m06:47:25.917429 [debug] [MainThread]: Command `dbt run` failed at 06:47:25.917026 after 35.91 seconds
[0m06:47:25.918955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7144afc350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71457beb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7143ee7b30>]}
[0m06:47:25.920064 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:47:25.921208 [debug] [MainThread]: Flushing usage events
[0m09:48:57.714989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f227804d820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22788ddd60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f227804c8c0>]}
[0m09:48:57.721941 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 09:48:57.722292 | 9a51266a-6ff3-47a1-9cf4-d4d45ce1f8d6 ==============================
[0m09:48:57.722292 [info ] [MainThread]: Running with dbt=1.9.2
[0m09:48:57.723372 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select vehicle_bronze', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:48:58.381141 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:48:58.382154 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:48:58.383027 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:48:58.711376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9a51266a-6ff3-47a1-9cf4-d4d45ce1f8d6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2277ff1ac0>]}
[0m09:48:58.712153 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:48:58.766697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9a51266a-6ff3-47a1-9cf4-d4d45ce1f8d6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22780754f0>]}
[0m09:48:58.767291 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:48:58.767808 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m09:48:59.063467 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m09:48:59.302429 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:48:59.303020 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_bronze.sql
[0m09:48:59.713605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9a51266a-6ff3-47a1-9cf4-d4d45ce1f8d6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22762ebfe0>]}
[0m09:48:59.714633 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:48:59.797999 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m09:48:59.808426 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m09:48:59.905980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9a51266a-6ff3-47a1-9cf4-d4d45ce1f8d6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2279414a40>]}
[0m09:48:59.907191 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:48:59.908287 [info ] [MainThread]: Found 1 model, 474 macros
[0m09:48:59.910217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a51266a-6ff3-47a1-9cf4-d4d45ce1f8d6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22779781d0>]}
[0m09:48:59.911219 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:48:59.914593 [info ] [MainThread]: 
[0m09:48:59.916032 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:48:59.917223 [info ] [MainThread]: 
[0m09:48:59.918481 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:48:59.919878 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m09:48:59.936467 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m09:48:59.937458 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m09:48:59.938243 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:49:01.747681 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:49:01.748808 [debug] [ThreadPool]: SQL status: OK in 1.811 seconds
[0m09:49:02.499719 [debug] [ThreadPool]: On list_schemas: Close
[0m09:49:03.005997 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m09:49:03.018002 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:49:03.018988 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m09:49:03.019813 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m09:49:03.020546 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:49:05.046516 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:49:05.047596 [debug] [ThreadPool]: SQL status: OK in 2.027 seconds
[0m09:49:05.803091 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m09:49:05.804388 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m09:49:05.804873 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m09:49:05.805182 [debug] [ThreadPool]: On list_None_default: Close
[0m09:49:06.305653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a51266a-6ff3-47a1-9cf4-d4d45ce1f8d6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22778fe390>]}
[0m09:49:06.306669 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:49:06.307594 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:49:06.308386 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:49:06.329884 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_bronze
[0m09:49:06.330651 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_bronze ............................ [RUN]
[0m09:49:06.332278 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_bronze)
[0m09:49:06.332747 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_bronze
[0m09:49:06.348293 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_bronze"
[0m09:49:06.352938 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_bronze
[0m09:49:06.385507 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_bronze"
[0m09:49:06.385955 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_bronze"} */
drop table if exists default.vehicle_bronze
[0m09:49:06.386276 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:49:08.353993 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:49:08.355181 [debug] [Thread-1 (]: SQL status: OK in 1.969 seconds
[0m09:49:08.393626 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_bronze"
[0m09:49:08.396721 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:49:08.397189 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_bronze"
[0m09:49:08.397569 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_bronze"} */

  
    
        create table default.vehicle_bronze
      
      
      
      
      
      
      
      

      as
      select 1 as id, "thamu" as name
  
[0m09:49:09.838675 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:10.588487 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:11.337123 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:12.093047 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:12.846299 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:13.595886 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:14.349629 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:15.105679 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:15.857970 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:16.609153 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:17.359829 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:18.110184 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:18.860534 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:20.112255 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:21.360972 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:22.615095 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:49:23.010663 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:49:23.011171 [debug] [Thread-1 (]: SQL status: OK in 14.613 seconds
[0m09:49:23.053071 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: ROLLBACK
[0m09:49:23.053772 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:49:23.054303 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: Close
[0m09:49:23.579658 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a51266a-6ff3-47a1-9cf4-d4d45ce1f8d6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2279d29370>]}
[0m09:49:23.580084 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m09:49:23.580646 [info ] [Thread-1 (]: 1 of 1 OK created sql table model default.vehicle_bronze ....................... [[32mOK[0m in 17.25s]
[0m09:49:23.581592 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_bronze
[0m09:49:23.584152 [debug] [MainThread]: On master: ROLLBACK
[0m09:49:23.584544 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:49:25.336267 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:49:25.338368 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:49:25.340061 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:49:25.341553 [debug] [MainThread]: On master: ROLLBACK
[0m09:49:25.343006 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:49:25.344435 [debug] [MainThread]: On master: Close
[0m09:49:25.683141 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:49:25.685051 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_bronze' was properly closed.
[0m09:49:25.686753 [info ] [MainThread]: 
[0m09:49:25.742555 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 25.77 seconds (25.77s).
[0m09:49:25.795302 [debug] [MainThread]: Command end result
[0m09:49:25.885131 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m09:49:25.894658 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m09:49:25.958364 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m09:49:25.960127 [info ] [MainThread]: 
[0m09:49:25.969442 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:49:25.972486 [info ] [MainThread]: 
[0m09:49:25.974379 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m09:49:25.980859 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 28.322975, "process_in_blocks": "0", "process_kernel_time": 2.4375, "process_mem_max_rss": "105264", "process_out_blocks": "0", "process_user_time": 2.28125}
[0m09:49:25.983312 [debug] [MainThread]: Command `dbt run` succeeded at 09:49:25.982751 after 28.33 seconds
[0m09:49:25.985148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f227804d460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f227804c860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f227804d6a0>]}
[0m09:49:25.986648 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:49:25.988142 [debug] [MainThread]: Flushing usage events
[0m09:52:04.132001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faab03f5850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaae878140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaafeb3650>]}
[0m09:52:04.136883 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 09:52:04.137360 | a358bd31-b460-4222-b2a1-bcd536347e7f ==============================
[0m09:52:04.137360 [info ] [MainThread]: Running with dbt=1.9.2
[0m09:52:04.138482 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt compile --select vehicle_bronze', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:52:04.367356 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:52:04.367870 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:52:04.368237 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:52:04.529683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a358bd31-b460-4222-b2a1-bcd536347e7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaae5a70e0>]}
[0m09:52:04.530175 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:52:04.584484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a358bd31-b460-4222-b2a1-bcd536347e7f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaade3f4d0>]}
[0m09:52:04.584987 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:52:04.585495 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m09:52:04.805041 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m09:52:04.955995 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:52:04.956933 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_bronze.sql
[0m09:52:05.521635 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_spark_emr_app.vehicle_bronze' (models/vehicle_poc/vehicle_bronze.sql) depends on a source named 'create_bronze_table.vehicle_bronze' which was not found
[0m09:52:05.528652 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": false, "command_wall_clock_time": 1.4693787, "process_in_blocks": "0", "process_kernel_time": 1.046875, "process_mem_max_rss": "101296", "process_out_blocks": "0", "process_user_time": 2.296875}
[0m09:52:05.530183 [debug] [MainThread]: Command `dbt compile` failed at 09:52:05.529839 after 1.47 seconds
[0m09:52:05.531367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faab1fd3140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaad9db170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaad9c7770>]}
[0m09:52:05.532420 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:52:05.533363 [debug] [MainThread]: Flushing usage events
[0m09:54:21.519467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37b31b4a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37b1917ec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37b1a7f320>]}
[0m09:54:21.524324 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 09:54:21.524739 | 0ac935cf-de9b-4ce9-b25b-abba578f1945 ==============================
[0m09:54:21.524739 [info ] [MainThread]: Running with dbt=1.9.2
[0m09:54:21.526532 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt compile --select vehicle_bronze', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m09:54:21.764707 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:54:21.765281 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:54:21.765631 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:54:21.930788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0ac935cf-de9b-4ce9-b25b-abba578f1945', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37b13a6f90>]}
[0m09:54:21.931262 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:54:21.992208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0ac935cf-de9b-4ce9-b25b-abba578f1945', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37b3079d60>]}
[0m09:54:21.992716 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:54:21.993231 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m09:54:22.217152 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m09:54:22.359893 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:54:22.360527 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_bronze.sql
[0m09:54:22.967346 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_spark_emr_app.vehicle_bronze' (models/vehicle_poc/vehicle_bronze.sql) depends on a source named 'create_bronze_table.vehicle_bronze' which was not found
[0m09:54:22.974020 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": false, "command_wall_clock_time": 1.5246407, "process_in_blocks": "0", "process_kernel_time": 1.078125, "process_mem_max_rss": "101308", "process_out_blocks": "0", "process_user_time": 2.296875}
[0m09:54:22.975408 [debug] [MainThread]: Command `dbt compile` failed at 09:54:22.975079 after 1.53 seconds
[0m09:54:22.976544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37b137d280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37b08bf320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37b0758770>]}
[0m09:54:22.977488 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:54:22.978392 [debug] [MainThread]: Flushing usage events
[0m09:55:22.590520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fcab24cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fc921f170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fc921dcd0>]}
[0m09:55:22.595084 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 09:55:22.595503 | 7b8c64d3-c07d-4b89-a35f-1dca44434515 ==============================
[0m09:55:22.595503 [info ] [MainThread]: Running with dbt=1.9.2
[0m09:55:22.596826 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt compile --select vehicle_bronze', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:55:22.848466 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:55:22.849008 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:55:22.849369 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:55:23.029672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7b8c64d3-c07d-4b89-a35f-1dca44434515', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fc8c91280>]}
[0m09:55:23.030175 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:55:23.110034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7b8c64d3-c07d-4b89-a35f-1dca44434515', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fc8cd5ca0>]}
[0m09:55:23.110548 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:55:23.111077 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m09:55:23.357156 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m09:55:23.655649 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 1 files changed.
[0m09:55:23.657392 [debug] [MainThread]: Partial parsing: added file: dbt_spark_emr_app://models/schema.yml
[0m09:55:23.658881 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_bronze.sql
[0m09:55:24.450275 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_spark_emr_app.vehicle_bronze' (models/vehicle_poc/vehicle_bronze.sql) depends on a source named 'create_bronze_table.vehicle_bronze' which was not found
[0m09:55:24.458769 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": false, "command_wall_clock_time": 1.93693, "process_in_blocks": "0", "process_kernel_time": 1.390625, "process_mem_max_rss": "101312", "process_out_blocks": "0", "process_user_time": 2.5}
[0m09:55:24.460278 [debug] [MainThread]: Command `dbt compile` failed at 09:55:24.459936 after 1.94 seconds
[0m09:55:24.461542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fc8cee0f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fc80e7b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fc80e7b60>]}
[0m09:55:24.462636 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:55:24.463524 [debug] [MainThread]: Flushing usage events
[0m09:56:33.396023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55864acf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb557f81700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5583031a0>]}
[0m09:56:33.402039 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 09:56:33.402796 | cb2e5c8c-0b5b-41ec-a638-c0f531962dff ==============================
[0m09:56:33.402796 [info ] [MainThread]: Running with dbt=1.9.2
[0m09:56:33.404442 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt compile --select vehicle_bronze', 'send_anonymous_usage_stats': 'True'}
[0m09:56:33.634544 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:56:33.635089 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:56:33.635444 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:56:33.816298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cb2e5c8c-0b5b-41ec-a638-c0f531962dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb55b0b3170>]}
[0m09:56:33.816824 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:56:33.871343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cb2e5c8c-0b5b-41ec-a638-c0f531962dff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb556f5ffb0>]}
[0m09:56:33.871868 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:56:33.872403 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m09:56:34.088503 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m09:56:34.223656 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 1 files changed.
[0m09:56:34.224257 [debug] [MainThread]: Partial parsing: added file: dbt_spark_emr_app://models/schema.yml
[0m09:56:34.224740 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_bronze.sql
[0m09:56:34.600521 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_spark_emr_app.vehicle_bronze' (models/vehicle_poc/vehicle_bronze.sql) depends on a source named 'create_bronze_table.vehicle_bronze' which was not found
[0m09:56:34.607251 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": false, "command_wall_clock_time": 1.2781585, "process_in_blocks": "0", "process_kernel_time": 0.921875, "process_mem_max_rss": "101316", "process_out_blocks": "0", "process_user_time": 2.1875}
[0m09:56:34.608626 [debug] [MainThread]: Command `dbt compile` failed at 09:56:34.608317 after 1.28 seconds
[0m09:56:34.609934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb558389010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb556e12360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb557008f20>]}
[0m09:56:34.611312 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:56:34.612186 [debug] [MainThread]: Flushing usage events
[0m09:57:14.716399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f342e364cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f342c96ade0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f342d51abd0>]}
[0m09:57:14.722381 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 09:57:14.722935 | d9d7b583-9f5a-4db8-a04d-e44545c73f28 ==============================
[0m09:57:14.722935 [info ] [MainThread]: Running with dbt=1.9.2
[0m09:57:14.725083 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt compile --select vehicle_bronze', 'send_anonymous_usage_stats': 'True'}
[0m09:57:14.950731 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:57:14.951255 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:57:14.951620 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:57:15.104504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd9d7b583-9f5a-4db8-a04d-e44545c73f28', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f342be5bf20>]}
[0m09:57:15.105089 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:15.158408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd9d7b583-9f5a-4db8-a04d-e44545c73f28', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f342bd94d10>]}
[0m09:57:15.158944 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:15.159436 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m09:57:15.386339 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m09:57:15.527901 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 1 files changed.
[0m09:57:15.528496 [debug] [MainThread]: Partial parsing: added file: dbt_spark_emr_app://models/schema.yml
[0m09:57:15.528998 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_bronze.sql
[0m09:57:16.032346 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_spark_emr_app.vehicle_bronze' (models/vehicle_poc/vehicle_bronze.sql) depends on a source named 'create_bronze_table.vehicle_bronze' which was not found
[0m09:57:16.038725 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": false, "command_wall_clock_time": 1.3915355, "process_in_blocks": "0", "process_kernel_time": 0.953125, "process_mem_max_rss": "102400", "process_out_blocks": "0", "process_user_time": 2.15625}
[0m09:57:16.040250 [debug] [MainThread]: Command `dbt compile` failed at 09:57:16.039909 after 1.39 seconds
[0m09:57:16.041599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f342d0edd30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f342d89f7a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f342bce2e70>]}
[0m09:57:16.042295 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:16.043162 [debug] [MainThread]: Flushing usage events
[0m09:57:35.261198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42f5744b00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42f55ead20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42f5747da0>]}
[0m09:57:35.265893 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 09:57:35.266307 | d67d9ab2-f0dc-48c1-954e-e416b80dc06a ==============================
[0m09:57:35.266307 [info ] [MainThread]: Running with dbt=1.9.2
[0m09:57:35.268034 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt compile --select vehicle_bronze', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:57:35.489553 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:57:35.490151 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:57:35.490521 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:57:35.642285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd67d9ab2-f0dc-48c1-954e-e416b80dc06a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42f4b1aae0>]}
[0m09:57:35.642757 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:35.699954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd67d9ab2-f0dc-48c1-954e-e416b80dc06a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42f566c530>]}
[0m09:57:35.700737 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:35.701281 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m09:57:35.921070 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m09:57:36.079203 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 1 files changed.
[0m09:57:36.079816 [debug] [MainThread]: Partial parsing: added file: dbt_spark_emr_app://models/schema.yml
[0m09:57:36.080317 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_bronze.sql
[0m09:57:36.987026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd67d9ab2-f0dc-48c1-954e-e416b80dc06a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42f4679e50>]}
[0m09:57:36.988323 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:37.181341 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m09:57:37.194429 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m09:57:37.268685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd67d9ab2-f0dc-48c1-954e-e416b80dc06a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42f4727bc0>]}
[0m09:57:37.269916 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:37.271031 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m09:57:37.273332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd67d9ab2-f0dc-48c1-954e-e416b80dc06a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42f47bc230>]}
[0m09:57:37.274337 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:37.278207 [info ] [MainThread]: 
[0m09:57:37.280706 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:57:37.282710 [info ] [MainThread]: 
[0m09:57:37.286074 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:57:37.290219 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m09:57:37.320146 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:57:37.321491 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m09:57:37.323207 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m09:57:37.324909 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:57:39.514320 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:57:39.515750 [debug] [ThreadPool]: SQL status: OK in 2.191 seconds
[0m09:57:40.492998 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m09:57:40.494428 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m09:57:40.495536 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m09:57:40.496537 [debug] [ThreadPool]: On list_None_default: Close
[0m09:57:41.158769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd67d9ab2-f0dc-48c1-954e-e416b80dc06a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42ef1c2330>]}
[0m09:57:41.160142 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:41.191145 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_bronze
[0m09:57:41.192763 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_bronze)
[0m09:57:41.194008 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_bronze
[0m09:57:41.214491 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_bronze"
[0m09:57:41.218919 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_bronze
[0m09:57:41.221489 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_bronze
[0m09:57:41.228664 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:57:41.230002 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_bronze' was properly closed.
[0m09:57:41.232027 [debug] [MainThread]: Command end result
[0m09:57:41.288707 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m09:57:41.295512 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m09:57:41.315882 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m09:57:41.316641 [info ] [MainThread]: Compiled node 'vehicle_bronze' is:
select * from vehicle_poc.vehicle_ext
[0m09:57:41.319782 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 6.1348233, "process_in_blocks": "0", "process_kernel_time": 1.28125, "process_mem_max_rss": "105408", "process_out_blocks": "0", "process_user_time": 2.9375}
[0m09:57:41.320753 [debug] [MainThread]: Command `dbt compile` succeeded at 09:57:41.320466 after 6.14 seconds
[0m09:57:41.322198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42f5e4a150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42f548b470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42f4a5e690>]}
[0m09:57:41.323691 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:41.325183 [debug] [MainThread]: Flushing usage events
[0m09:57:54.310288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71b808f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71b04c380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71aed9970>]}
[0m09:57:54.314930 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 09:57:54.315335 | 4bd7105d-b51f-4283-9450-c513dcd891a2 ==============================
[0m09:57:54.315335 [info ] [MainThread]: Running with dbt=1.9.2
[0m09:57:54.316955 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --select vehicle_bronze', 'send_anonymous_usage_stats': 'True'}
[0m09:57:54.547627 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:57:54.548159 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:57:54.548547 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:57:54.700965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4bd7105d-b51f-4283-9450-c513dcd891a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71ab49160>]}
[0m09:57:54.701454 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:54.755505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4bd7105d-b51f-4283-9450-c513dcd891a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71afddf70>]}
[0m09:57:54.756011 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:54.756519 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m09:57:54.997597 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m09:57:55.151047 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:57:55.151496 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:57:55.179989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4bd7105d-b51f-4283-9450-c513dcd891a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71b06e990>]}
[0m09:57:55.180448 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:55.243452 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m09:57:55.248087 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m09:57:55.276038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4bd7105d-b51f-4283-9450-c513dcd891a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe719fe40e0>]}
[0m09:57:55.276502 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:55.276850 [info ] [MainThread]: Found 1 model, 1 source, 474 macros
[0m09:57:55.278311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4bd7105d-b51f-4283-9450-c513dcd891a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71b7b9e80>]}
[0m09:57:55.278671 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:57:55.280233 [info ] [MainThread]: 
[0m09:57:55.281585 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:57:55.282585 [info ] [MainThread]: 
[0m09:57:55.283752 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:57:55.287389 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m09:57:55.322480 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m09:57:55.323874 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m09:57:55.324550 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:57:57.154610 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:57:57.155982 [debug] [ThreadPool]: SQL status: OK in 1.831 seconds
[0m09:57:58.048171 [debug] [ThreadPool]: On list_schemas: Close
[0m09:57:58.668434 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m09:57:58.683052 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:57:58.684282 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m09:57:58.685322 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m09:57:58.686256 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:58:01.018511 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:58:01.019890 [debug] [ThreadPool]: SQL status: OK in 2.334 seconds
[0m09:58:01.943729 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m09:58:01.945182 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m09:58:01.946288 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m09:58:01.947284 [debug] [ThreadPool]: On list_None_default: Close
[0m09:58:02.553137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4bd7105d-b51f-4283-9450-c513dcd891a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71a448230>]}
[0m09:58:02.554470 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:58:02.555594 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:02.556581 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:58:02.584554 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_bronze
[0m09:58:02.586369 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_bronze ............................ [RUN]
[0m09:58:02.589256 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_bronze)
[0m09:58:02.590475 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_bronze
[0m09:58:02.611385 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_bronze"
[0m09:58:02.615711 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_bronze
[0m09:58:02.693399 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_bronze"
[0m09:58:02.694971 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_bronze"} */
drop table if exists default.vehicle_bronze
[0m09:58:02.696168 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:58:04.814347 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:58:04.815808 [debug] [Thread-1 (]: SQL status: OK in 2.120 seconds
[0m09:58:04.935420 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_bronze"
[0m09:58:04.940602 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:04.942531 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_bronze"
[0m09:58:04.943687 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_bronze"} */

  
    
        create table default.vehicle_bronze
      
      
      
      
      
      
      
      

      as
      select * from vehicle_poc.vehicle_ext
  
[0m09:58:06.034748 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:06.956091 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:07.775822 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:08.528864 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:09.313275 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:10.131896 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:10.950160 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:11.769078 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:12.588908 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:13.410870 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:14.229765 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:15.049211 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m09:58:16.072700 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='08S01', errorCode=2, errorMessage="Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0008_1_00, diagnostics=[Vertex vertex_1741739386091_0008_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0008_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')\n\tat org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)\n\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)\n\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)\n\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)\n\tat org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)\n\tat com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)\n\tat com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)\n\tat com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0", taskStatus='[{"returnValue":2,"errorMsg":"org.apache.hadoop.hive.ql.metadata.HiveException: Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0008_1_00, diagnostics=[Vertex vertex_1741739386091_0008_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0008_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader \'bootstrap\'; org.apache.hadoop.fs.Seekable is in unnamed module of loader \'app\')\\n\\tat org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)\\n\\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)\\n\\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)\\n\\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)\\n\\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)\\n\\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)\\n\\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)\\n\\tat org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)\\n\\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\\n\\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)\\n\\tat com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)\\n\\tat com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)\\n\\tat com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0","beginTime":1742378284799,"endTime":1742378295589,"taskId":"Stage-1","taskState":"FINISHED","taskType":"MAPRED","name":"TEZ"}]', operationStarted=1742378284736, operationCompleted=1742378295590, hasResultSet=False, progressUpdateResponse=TProgressUpdateResp(headerNames=[], rows=[], progressedPercentage=0.0, status=2, footerSummary='', startTime=0))
[0m09:58:16.074949 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m09:58:16.076225 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_bronze"} */

  
    
        create table default.vehicle_bronze
      
      
      
      
      
      
      
      

      as
      select * from vehicle_poc.vehicle_ext
  
[0m09:58:16.077603 [debug] [Thread-1 (]: Spark adapter: Database Error
  Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0008_1_00, diagnostics=[Vertex vertex_1741739386091_0008_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0008_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
  	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
  	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
  	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
  	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
  	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
  	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
  	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
  	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
[0m09:58:16.079450 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: ROLLBACK
[0m09:58:16.080599 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:58:16.081703 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_bronze: Close
[0m09:58:16.718107 [debug] [Thread-1 (]: Runtime Error in model vehicle_bronze (models/vehicle_poc/vehicle_bronze.sql)
  Database Error
    Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0008_1_00, diagnostics=[Vertex vertex_1741739386091_0008_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0008_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
    	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
    	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
    	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
    	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
[0m09:58:16.724465 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4bd7105d-b51f-4283-9450-c513dcd891a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71a272b70>]}
[0m09:58:16.725596 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m09:58:16.727396 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_bronze ................... [[31mERROR[0m in 14.13s]
[0m09:58:16.730932 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_bronze
[0m09:58:16.733150 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_bronze' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_bronze (models/vehicle_poc/vehicle_bronze.sql)
  Database Error
    Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0008_1_00, diagnostics=[Vertex vertex_1741739386091_0008_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0008_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
    	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
    	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
    	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
    	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0.
[0m09:58:16.743713 [debug] [MainThread]: On master: ROLLBACK
[0m09:58:16.744929 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:58:18.527537 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:58:18.528894 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:58:18.529905 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:58:18.531125 [debug] [MainThread]: On master: ROLLBACK
[0m09:58:18.532132 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:58:18.533013 [debug] [MainThread]: On master: Close
[0m09:58:18.834682 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:58:18.835936 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_bronze' was properly closed.
[0m09:58:18.836955 [info ] [MainThread]: 
[0m09:58:18.839446 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 23.55 seconds (23.55s).
[0m09:58:18.842687 [debug] [MainThread]: Command end result
[0m09:58:18.899614 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m09:58:18.906688 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m09:58:18.934875 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m09:58:18.936032 [info ] [MainThread]: 
[0m09:58:18.938496 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m09:58:18.940639 [info ] [MainThread]: 
[0m09:58:18.943126 [error] [MainThread]:   Runtime Error in model vehicle_bronze (models/vehicle_poc/vehicle_bronze.sql)
  Database Error
    Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0008_1_00, diagnostics=[Vertex vertex_1741739386091_0008_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0008_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
    	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
    	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
    	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
    	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
[0m09:58:18.962541 [info ] [MainThread]: 
[0m09:58:18.965554 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:58:18.971952 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 24.729126, "process_in_blocks": "0", "process_kernel_time": 1.25, "process_mem_max_rss": "101032", "process_out_blocks": "0", "process_user_time": 2.15625}
[0m09:58:18.973483 [debug] [MainThread]: Command `dbt run` failed at 09:58:18.973165 after 24.73 seconds
[0m09:58:18.974713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71af589b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71a0957f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71a449f40>]}
[0m09:58:18.975735 [debug] [MainThread]: An error was encountered while trying to send an event
[0m09:58:18.976605 [debug] [MainThread]: Flushing usage events
[0m10:00:56.085602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f747ddaec30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f747ddafe00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f747e43da00>]}
[0m10:00:56.090665 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 10:00:56.091173 | 6f3b7dc7-2270-4d56-848b-a4ab221c8da7 ==============================
[0m10:00:56.091173 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:00:56.092405 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation create_broze_table', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:00:56.319052 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:00:56.319583 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:00:56.320003 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:00:56.484557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6f3b7dc7-2270-4d56-848b-a4ab221c8da7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f747db385f0>]}
[0m10:00:56.485048 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:00:56.538739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6f3b7dc7-2270-4d56-848b-a4ab221c8da7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f747d4b5010>]}
[0m10:00:56.539295 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:00:56.539796 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m10:00:56.761534 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m10:00:56.910558 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:00:56.911196 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m10:00:56.979942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6f3b7dc7-2270-4d56-848b-a4ab221c8da7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f747d2907a0>]}
[0m10:00:56.980463 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:00:57.183197 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m10:00:57.195907 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m10:00:57.277823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6f3b7dc7-2270-4d56-848b-a4ab221c8da7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f747d083110>]}
[0m10:00:57.279619 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:00:57.281196 [info ] [MainThread]: Found 1 model, 1 source, 475 macros
[0m10:00:57.283879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6f3b7dc7-2270-4d56-848b-a4ab221c8da7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f747d0f1100>]}
[0m10:00:57.284864 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:00:57.286376 [debug] [MainThread]: Acquiring new spark connection 'macro_create_broze_table'
[0m10:00:57.287433 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:00:57.288363 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:00:57.289636 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  dbt could not find a macro with the name "create_broze_table" in any package
[0m10:00:57.334570 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1182, in execute_macro
    raise DbtRuntimeError(
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  dbt could not find a macro with the name "create_broze_table" in any package

[0m10:00:57.336140 [error] [MainThread]: Encountered an error:
Internal Error
  dbt could not find a macro with the name 'create_broze_table' in any package
[0m10:00:57.342673 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 1.3239713, "process_in_blocks": "0", "process_kernel_time": 1.015625, "process_mem_max_rss": "99128", "process_out_blocks": "0", "process_user_time": 2.171875}
[0m10:00:57.344178 [debug] [MainThread]: Command `dbt run-operation` failed at 10:00:57.343827 after 1.33 seconds
[0m10:00:57.345263 [debug] [MainThread]: Connection 'macro_create_broze_table' was properly closed.
[0m10:00:57.346363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f747cd9c170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f747d083110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f747d083c50>]}
[0m10:00:57.347377 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:00:57.348284 [debug] [MainThread]: Flushing usage events
[0m10:01:11.052749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7f959d850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7f959cdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7f959d7c0>]}
[0m10:01:11.057772 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 10:01:11.058210 | 8f758e4f-622a-4838-ab13-08e8b360b52e ==============================
[0m10:01:11.058210 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:01:11.060076 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation create_bronze_table', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:01:11.276605 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:01:11.277121 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:01:11.277495 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:01:11.438273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8f758e4f-622a-4838-ab13-08e8b360b52e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7f9a8fc80>]}
[0m10:01:11.438757 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:01:11.492729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8f758e4f-622a-4838-ab13-08e8b360b52e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7f91d0050>]}
[0m10:01:11.493243 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:01:11.493771 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m10:01:11.716684 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m10:01:11.857680 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:01:11.858142 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:01:11.886886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8f758e4f-622a-4838-ab13-08e8b360b52e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7f8d57290>]}
[0m10:01:11.887358 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:01:11.949639 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m10:01:11.954261 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m10:01:11.973194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8f758e4f-622a-4838-ab13-08e8b360b52e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7f95f18b0>]}
[0m10:01:11.973745 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:01:11.974327 [info ] [MainThread]: Found 1 model, 1 source, 475 macros
[0m10:01:11.975961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8f758e4f-622a-4838-ab13-08e8b360b52e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7fa239640>]}
[0m10:01:11.976345 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:01:11.976917 [debug] [MainThread]: Acquiring new spark connection 'macro_create_bronze_table'
[0m10:01:11.977317 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:01:11.977637 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:01:11.989391 [debug] [MainThread]: Using spark connection "macro_create_bronze_table"
[0m10:01:11.989878 [debug] [MainThread]: On macro_create_bronze_table: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_create_bronze_table"} */

    

        create table vehicle_poc.vehicle_bronze
        AS
        select * from vehicle_poc.vehicle_ext;

    
  
[0m10:01:11.990188 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:01:14.555924 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:15.310636 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:16.098675 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:16.854712 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:17.628253 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:18.382561 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:19.137042 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:19.986124 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:20.782130 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:21.621504 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:22.443350 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:23.261494 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m10:01:23.567337 [debug] [MainThread]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='08S01', errorCode=2, errorMessage="Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0009_1_00, diagnostics=[Vertex vertex_1741739386091_0009_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0009_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')\n\tat org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)\n\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)\n\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)\n\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)\n\tat org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)\n\tat com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)\n\tat com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)\n\tat com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0", taskStatus='[{"returnValue":2,"errorMsg":"org.apache.hadoop.hive.ql.metadata.HiveException: Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0009_1_00, diagnostics=[Vertex vertex_1741739386091_0009_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0009_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader \'bootstrap\'; org.apache.hadoop.fs.Seekable is in unnamed module of loader \'app\')\\n\\tat org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)\\n\\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)\\n\\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)\\n\\tat org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)\\n\\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)\\n\\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)\\n\\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)\\n\\tat org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)\\n\\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\\n\\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)\\n\\tat org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)\\n\\tat com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)\\n\\tat com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)\\n\\tat com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0","beginTime":1742378473278,"endTime":1742378482778,"taskId":"Stage-1","taskState":"FINISHED","taskType":"MAPRED","name":"TEZ"}]', operationStarted=1742378473220, operationCompleted=1742378482779, hasResultSet=False, progressUpdateResponse=TProgressUpdateResp(headerNames=[], rows=[], progressedPercentage=0.0, status=2, footerSummary='', startTime=0))
[0m10:01:23.569670 [debug] [MainThread]: Spark adapter: Poll status: 5
[0m10:01:23.570856 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_create_bronze_table"} */

    

        create table vehicle_poc.vehicle_bronze
        AS
        select * from vehicle_poc.vehicle_ext;

    
  
[0m10:01:23.572156 [debug] [MainThread]: Spark adapter: Database Error
  Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0009_1_00, diagnostics=[Vertex vertex_1741739386091_0009_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0009_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
  	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
  	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
  	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
  	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
  	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
  	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
  	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
  	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
[0m10:01:23.573605 [debug] [MainThread]: Spark adapter: Error while running:
macro create_bronze_table
[0m10:01:23.574852 [debug] [MainThread]: Spark adapter: Runtime Error
  Database Error
    Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0009_1_00, diagnostics=[Vertex vertex_1741739386091_0009_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0009_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
    	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
    	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
    	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
    	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
[0m10:01:23.576219 [debug] [MainThread]: On macro_create_bronze_table: ROLLBACK
[0m10:01:23.577190 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:01:23.578088 [debug] [MainThread]: On macro_create_bronze_table: Close
[0m10:01:24.189168 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Database Error
      Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0009_1_00, diagnostics=[Vertex vertex_1741739386091_0009_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0009_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
      	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
      	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
      	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
      	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
      	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
      	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
      	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
      	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
      	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
      	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
      	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
      	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
      	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
      	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
      	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
      	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
      	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
      	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
      	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
      	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
      	at java.base/java.lang.Thread.run(Thread.java:840)
      ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
[0m10:01:24.229704 [debug] [MainThread]: Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 147, in add_query
    _execute_query_with_retry(
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 97, in _execute_query_with_retry
    cursor.execute(sql, bindings)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 300, in execute
    raise DbtDatabaseError(poll_state.errorMessage)
dbt_common.exceptions.base.DbtDatabaseError: Database Error
  Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0009_1_00, diagnostics=[Vertex vertex_1741739386091_0009_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0009_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
  	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
  	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
  	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
  	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
  	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
  	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
  	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
  	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
  	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
  	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 354, in exception_handler
    yield
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1198, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 421, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 35, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 33, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/clients/jinja.py", line 82, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt_common/clients/jinja.py", line 389, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 770, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 784, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/sandbox.py", line 401, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/jinja2/runtime.py", line 303, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 404, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 218, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/sql/connections.py", line 130, in add_query
    with self.exception_handler(sql):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Database Error
    Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0009_1_00, diagnostics=[Vertex vertex_1741739386091_0009_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0009_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
    	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
    	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
    	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
    	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
    	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
    	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
    	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
    	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    	at java.base/java.lang.Thread.run(Thread.java:840)
    ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 67, in run
    self._run_unsafe(package_name, macro_name)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/task/run_operation.py", line 48, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/base/impl.py", line 1197, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/lib/python3.12/site-packages/dbt/adapters/spark/connections.py", line 367, in exception_handler
    raise DbtRuntimeError(str(exc))
dbt_common.exceptions.base.DbtRuntimeError: Runtime Error
  Runtime Error
    Database Error
      Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1741739386091_0009_1_00, diagnostics=[Vertex vertex_1741739386091_0009_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: vehicle_ext initializer failed, vertex=vertex_1741739386091_0009_1_00 [Map 1], java.lang.ClassCastException: class java.io.PushbackInputStream cannot be cast to class org.apache.hadoop.fs.Seekable (java.io.PushbackInputStream is in module java.base of loader 'bootstrap'; org.apache.hadoop.fs.Seekable is in unnamed module of loader 'app')
      	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:85)
      	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:123)
      	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:76)
      	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:69)
      	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:352)
      	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:549)
      	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:821)
      	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
      	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:281)
      	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:272)
      	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
      	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
      	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
      	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:272)
      	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:256)
      	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:131)
      	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:74)
      	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:82)
      	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
      	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
      	at java.base/java.lang.Thread.run(Thread.java:840)
      ]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0

[0m10:01:24.254923 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m10:01:24.258803 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 13.27501, "process_in_blocks": "0", "process_kernel_time": 1.109375, "process_mem_max_rss": "98560", "process_out_blocks": "0", "process_user_time": 1.84375}
[0m10:01:24.260164 [debug] [MainThread]: Command `dbt run-operation` failed at 10:01:24.259857 after 13.28 seconds
[0m10:01:24.261158 [debug] [MainThread]: Connection 'macro_create_bronze_table' was properly closed.
[0m10:01:24.262147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7f959d850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7fa25b1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7fa25a180>]}
[0m10:01:24.263028 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:01:24.263910 [debug] [MainThread]: Flushing usage events
[0m10:50:58.901718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaff266f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faafedc1550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faaff267e60>]}
[0m10:50:58.906874 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 10:50:58.907306 | 56a12622-a5da-4edb-920c-7de1042c3443 ==============================
[0m10:50:58.907306 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:50:58.909079 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt show --select vehicle_silver', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:50:59.165318 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:50:59.165842 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:50:59.166206 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:50:59.326882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '56a12622-a5da-4edb-920c-7de1042c3443', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faafe71d0d0>]}
[0m10:50:59.327357 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:50:59.381399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '56a12622-a5da-4edb-920c-7de1042c3443', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faafe626420>]}
[0m10:50:59.381924 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:50:59.463485 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m10:50:59.701803 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m10:51:00.057493 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 2 files changed.
[0m10:51:00.059806 [debug] [MainThread]: Partial parsing: added file: dbt_spark_emr_app://models/vehicle_poc/vehicle_silver.sql
[0m10:51:00.061831 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/schema.yml
[0m10:51:00.063655 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_bronze.sql
[0m10:51:01.082957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '56a12622-a5da-4edb-920c-7de1042c3443', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faafd9e20f0>]}
[0m10:51:01.084394 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:01.276427 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m10:51:01.288085 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m10:51:01.300959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '56a12622-a5da-4edb-920c-7de1042c3443', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faafd9d2cc0>]}
[0m10:51:01.302753 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:01.304420 [info ] [MainThread]: Found 2 models, 1 source, 475 macros
[0m10:51:01.306849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '56a12622-a5da-4edb-920c-7de1042c3443', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faafd9d36e0>]}
[0m10:51:01.308359 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:01.314213 [info ] [MainThread]: 
[0m10:51:01.315993 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:51:01.317356 [info ] [MainThread]: 
[0m10:51:01.319318 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m10:51:01.338537 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m10:51:01.370422 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:51:01.371873 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:51:01.373188 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m10:51:01.374251 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:03.624253 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:51:03.625634 [debug] [ThreadPool]: SQL status: OK in 2.251 seconds
[0m10:51:04.548521 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m10:51:04.549957 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m10:51:04.551048 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:51:04.552040 [debug] [ThreadPool]: On list_None_default: Close
[0m10:51:05.158626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '56a12622-a5da-4edb-920c-7de1042c3443', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faafe3515e0>]}
[0m10:51:05.159909 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:05.189563 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:05.191174 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_silver)
[0m10:51:05.192458 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:05.211612 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_silver"
[0m10:51:05.218029 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:05.238252 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_silver"
[0m10:51:05.240311 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_silver"} */

  
  select * from vehicle_poc.vehicle_silver;
  
  limit 5

[0m10:51:05.242002 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:51:07.101853 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_silver"} */

  
  select * from vehicle_poc.vehicle_silver;
  
  limit 5

[0m10:51:07.103685 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: ParseException line 4:42 cannot recognize input near ';' 'limit' '5' in table source:28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.ParseException:line 4:42 cannot recognize input near ';' 'limit' '5' in table source:35:8", 'org.apache.hadoop.hive.ql.parse.ParseDriver:parse:ParseDriver.java:233', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:79', 'org.apache.hadoop.hive.ql.parse.ParseUtils:parse:ParseUtils.java:72', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:631', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42000', errorCode=40000, errorMessage="Error while compiling statement: FAILED: ParseException line 4:42 cannot recognize input near ';' 'limit' '5' in table source"), operationHandle=None)
[0m10:51:07.105310 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: Close
[0m10:51:07.471612 [debug] [Thread-1 (]: Runtime Error in model vehicle_silver (models/vehicle_poc/vehicle_silver.sql)
  Error while compiling statement: FAILED: ParseException line 4:42 cannot recognize input near ';' 'limit' '5' in table source
[0m10:51:07.473884 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:07.475655 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_silver' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_silver (models/vehicle_poc/vehicle_silver.sql)
  Error while compiling statement: FAILED: ParseException line 4:42 cannot recognize input near ';' 'limit' '5' in table source.
[0m10:51:07.478724 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:51:07.480101 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_silver' was properly closed.
[0m10:51:07.481611 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error in model vehicle_silver (models/vehicle_poc/vehicle_silver.sql)
    Error while compiling statement: FAILED: ParseException line 4:42 cannot recognize input near ';' 'limit' '5' in table source
[0m10:51:07.485955 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 8.653795, "process_in_blocks": "0", "process_kernel_time": 1.484375, "process_mem_max_rss": "104568", "process_out_blocks": "0", "process_user_time": 3.09375}
[0m10:51:07.487444 [debug] [MainThread]: Command `dbt show` failed at 10:51:07.487099 after 8.66 seconds
[0m10:51:07.488740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faafec0dd30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faafd9d3560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faafda7fb30>]}
[0m10:51:07.489801 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:07.490684 [debug] [MainThread]: Flushing usage events
[0m10:51:32.579241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f921456eb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9213ee7380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f92142a7590>]}
[0m10:51:32.584195 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 10:51:32.584631 | ac2ba67e-f885-4caa-96da-4e59857be09f ==============================
[0m10:51:32.584631 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:51:32.586531 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt show --select vehicle_silver', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:51:32.799846 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:51:32.800377 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:51:32.800749 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:51:32.952248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ac2ba67e-f885-4caa-96da-4e59857be09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f92140f36b0>]}
[0m10:51:32.952742 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:33.005963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ac2ba67e-f885-4caa-96da-4e59857be09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9213b54e60>]}
[0m10:51:33.006462 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:33.029669 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m10:51:33.257828 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m10:51:33.408277 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:51:33.408933 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_silver.sql
[0m10:51:34.315442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ac2ba67e-f885-4caa-96da-4e59857be09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9212892a50>]}
[0m10:51:34.317267 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:34.517878 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m10:51:34.537541 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m10:51:34.547764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac2ba67e-f885-4caa-96da-4e59857be09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9212881250>]}
[0m10:51:34.549187 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:34.550471 [info ] [MainThread]: Found 2 models, 1 source, 475 macros
[0m10:51:34.552981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ac2ba67e-f885-4caa-96da-4e59857be09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9212880e30>]}
[0m10:51:34.553995 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:34.559035 [info ] [MainThread]: 
[0m10:51:34.560914 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:51:34.562442 [info ] [MainThread]: 
[0m10:51:34.564282 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m10:51:34.584103 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m10:51:34.614711 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:51:34.615894 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:51:34.616952 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m10:51:34.618142 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:36.605276 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:51:36.606660 [debug] [ThreadPool]: SQL status: OK in 1.989 seconds
[0m10:51:37.419127 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m10:51:37.420593 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m10:51:37.421697 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:51:37.422832 [debug] [ThreadPool]: On list_None_default: Close
[0m10:51:38.026438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ac2ba67e-f885-4caa-96da-4e59857be09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9212801e50>]}
[0m10:51:38.027755 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:38.056441 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:38.058088 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_silver)
[0m10:51:38.059381 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:38.077829 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_silver"
[0m10:51:38.082373 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:38.102062 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_silver"
[0m10:51:38.103495 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_silver"} */

  
  select * from vehicle_poc.vehicle_silver
  
  limit 5

[0m10:51:38.104792 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:51:40.024981 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_silver"} */

  
  select * from vehicle_poc.vehicle_silver
  
  limit 5

[0m10:51:40.026897 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 4:16 Table not found 'vehicle_silver':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.SemanticException:Line 4:16 Table not found 'vehicle_silver':38:11", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2166', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2079', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:12242', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12336', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:365', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:295', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:674', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42S02', errorCode=10001, errorMessage="Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 4:16 Table not found 'vehicle_silver'"), operationHandle=None)
[0m10:51:40.028614 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: Close
[0m10:51:40.294598 [debug] [Thread-1 (]: Runtime Error in model vehicle_silver (models/vehicle_poc/vehicle_silver.sql)
  Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 4:16 Table not found 'vehicle_silver'
[0m10:51:40.297168 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:40.299338 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_silver' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_silver (models/vehicle_poc/vehicle_silver.sql)
  Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 4:16 Table not found 'vehicle_silver'.
[0m10:51:40.302731 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:51:40.304549 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_silver' was properly closed.
[0m10:51:40.306549 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error in model vehicle_silver (models/vehicle_poc/vehicle_silver.sql)
    Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 4:16 Table not found 'vehicle_silver'
[0m10:51:40.313994 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": false, "command_wall_clock_time": 7.8067284, "process_in_blocks": "0", "process_kernel_time": 1.265625, "process_mem_max_rss": "104576", "process_out_blocks": "0", "process_user_time": 2.890625}
[0m10:51:40.315485 [debug] [MainThread]: Command `dbt show` failed at 10:51:40.315166 after 7.81 seconds
[0m10:51:40.316641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9213a7e270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9212880cb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9212890c20>]}
[0m10:51:40.317580 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:40.318443 [debug] [MainThread]: Flushing usage events
[0m10:51:47.598490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e4004ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e3dd3320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e3f9e450>]}
[0m10:51:47.603376 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 10:51:47.603882 | ff50d590-45a8-4de8-96f3-d2f985e43c05 ==============================
[0m10:51:47.603882 [info ] [MainThread]: Running with dbt=1.9.2
[0m10:51:47.605347 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt show --select vehicle_silver', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:51:47.819511 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:51:47.820044 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:51:47.820472 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:51:47.972267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ff50d590-45a8-4de8-96f3-d2f985e43c05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e3f9c0e0>]}
[0m10:51:47.972761 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:48.026504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ff50d590-45a8-4de8-96f3-d2f985e43c05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e36a2ab0>]}
[0m10:51:48.027033 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:48.050788 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m10:51:48.277726 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m10:51:48.419414 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:51:48.420137 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_silver.sql
[0m10:51:49.025635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ff50d590-45a8-4de8-96f3-d2f985e43c05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e2d41fd0>]}
[0m10:51:49.027081 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:49.221963 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m10:51:49.234839 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m10:51:49.245585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ff50d590-45a8-4de8-96f3-d2f985e43c05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e2d2bb00>]}
[0m10:51:49.247343 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:49.248962 [info ] [MainThread]: Found 2 models, 1 source, 475 macros
[0m10:51:49.252043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff50d590-45a8-4de8-96f3-d2f985e43c05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e2d2b5c0>]}
[0m10:51:49.253601 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:49.261906 [info ] [MainThread]: 
[0m10:51:49.264073 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:51:49.266215 [info ] [MainThread]: 
[0m10:51:49.268309 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m10:51:49.286527 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m10:51:49.316854 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:51:49.318277 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:51:49.319373 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m10:51:49.320246 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:51.238560 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:51:51.239935 [debug] [ThreadPool]: SQL status: OK in 1.920 seconds
[0m10:51:52.168724 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m10:51:52.170105 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m10:51:52.171181 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:51:52.172152 [debug] [ThreadPool]: On list_None_default: Close
[0m10:51:52.775939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff50d590-45a8-4de8-96f3-d2f985e43c05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e57caea0>]}
[0m10:51:52.777875 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:52.814085 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:52.816569 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_silver)
[0m10:51:52.818583 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:52.837707 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_silver"
[0m10:51:52.842161 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:52.856894 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_silver"
[0m10:51:52.858484 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_silver"} */

  
  select * from vehicle_poc.vehicle_bronze
  
  limit 5

[0m10:51:52.859648 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:51:55.134256 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:51:55.135785 [debug] [Thread-1 (]: SQL status: OK in 2.276 seconds
[0m10:51:56.054229 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: Close
[0m10:51:56.599087 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_silver
[0m10:51:56.607737 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:51:56.608936 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_silver' was properly closed.
[0m10:51:56.611027 [debug] [MainThread]: Command end result
[0m10:51:56.667905 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m10:51:56.675173 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m10:51:56.693806 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m10:51:56.695311 [info ] [MainThread]: Previewing node 'vehicle_silver':
| vehicle_bronze.vin | vehicle_bronze.co... | vehicle_bronze.city | vehicle_bronze.state | vehicle_bronze.po... | vehicle_bronze.mo... | ... |
| ------------------ | -------------------- | ------------------- | -------------------- | -------------------- | -------------------- | --- |
| 1C4JJXP66P         | Kitsap               | Poulsbo             | WA                   | 98370                | 2023                 | ... |
| 1G1FX6S08K         | Snohomish            | Lake Stevens        | WA                   | 98258                | 2019                 | ... |
| 1N4BZ0CP8H         | Snohomish            | Lynnwood            | WA                   | 98087                | 2017                 | ... |
| JTJAAAAB4P         | Kitsap               | Olalla              | WA                   | 98359                | 2023                 | ... |
| KNDC3DLC6R         | Kitsap               | Poulsbo             | WA                   | 98370                | 2024                 | ... |

[0m10:51:56.706159 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 9.183986, "process_in_blocks": "0", "process_kernel_time": 1.0625, "process_mem_max_rss": "104708", "process_out_blocks": "0", "process_user_time": 2.78125}
[0m10:51:56.708284 [debug] [MainThread]: Command `dbt show` succeeded at 10:51:56.707825 after 9.19 seconds
[0m10:51:56.709955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e410dee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e2cb20f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f13e46cf3b0>]}
[0m10:51:56.711443 [debug] [MainThread]: An error was encountered while trying to send an event
[0m10:51:56.712444 [debug] [MainThread]: Flushing usage events
[0m11:02:25.391056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b5245940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b563f0e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b583e4e0>]}
[0m11:02:25.395658 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:02:25.396078 | d8921776-6015-4868-b365-ec55fa1d72e1 ==============================
[0m11:02:25.396078 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:02:25.397563 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --select vehicle_silver', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:02:25.621691 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:02:25.622209 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:02:25.622572 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:02:25.779079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd8921776-6015-4868-b365-ec55fa1d72e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b4250ce0>]}
[0m11:02:25.779669 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:02:25.836665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd8921776-6015-4868-b365-ec55fa1d72e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b442b4a0>]}
[0m11:02:25.837143 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:02:25.837655 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:02:26.062356 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:02:26.210046 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 3 files changed.
[0m11:02:26.210627 [debug] [MainThread]: Partial parsing: added file: dbt_spark_emr_app://models/vehicle_poc/vehicle_gold.sql
[0m11:02:26.211084 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/schema.yml
[0m11:02:26.211512 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_bronze.sql
[0m11:02:26.211920 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_silver.sql
[0m11:02:26.878654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd8921776-6015-4868-b365-ec55fa1d72e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b2cf57f0>]}
[0m11:02:26.879970 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:02:27.081965 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:02:27.094866 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:02:27.171674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd8921776-6015-4868-b365-ec55fa1d72e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b43c28a0>]}
[0m11:02:27.172835 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:02:27.174243 [info ] [MainThread]: Found 3 models, 1 source, 475 macros
[0m11:02:27.176274 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8921776-6015-4868-b365-ec55fa1d72e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b4510aa0>]}
[0m11:02:27.177471 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:02:27.182404 [info ] [MainThread]: 
[0m11:02:27.184558 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:02:27.185733 [info ] [MainThread]: 
[0m11:02:27.187338 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:02:27.191181 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:02:27.223568 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:02:27.224896 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:02:27.226025 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:02:29.291726 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:02:29.293256 [debug] [ThreadPool]: SQL status: OK in 2.067 seconds
[0m11:02:30.061043 [debug] [ThreadPool]: On list_schemas: Close
[0m11:02:30.643956 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:02:30.658486 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:02:30.659758 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:02:30.660817 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:02:30.661836 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:02:32.981265 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:02:32.982840 [debug] [ThreadPool]: SQL status: OK in 2.321 seconds
[0m11:02:33.846548 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m11:02:33.848119 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:02:33.849627 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:02:33.851143 [debug] [ThreadPool]: On list_None_default: Close
[0m11:02:34.517209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8921776-6015-4868-b365-ec55fa1d72e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b4510740>]}
[0m11:02:34.518536 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:02:34.519700 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:02:34.520731 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:02:34.551153 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_silver
[0m11:02:34.552972 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_silver ............................ [RUN]
[0m11:02:34.555736 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_silver)
[0m11:02:34.557452 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_silver
[0m11:02:34.575638 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_silver"
[0m11:02:34.580257 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_silver
[0m11:02:34.641110 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_silver"
[0m11:02:34.642546 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_silver"} */
drop table if exists default.vehicle_silver
[0m11:02:34.643726 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:02:36.611445 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:02:36.612935 [debug] [Thread-1 (]: SQL status: OK in 1.969 seconds
[0m11:02:36.732303 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_silver"
[0m11:02:36.741530 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:02:36.742898 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_silver"
[0m11:02:36.744078 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_silver"} */

  
    
        create table default.vehicle_silver
      
      
      
      
      
      
      
      

      as
      select * from vehicle_poc.vehicle_bronze
where electric_range > 200
  
[0m11:02:37.826022 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:38.578689 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:39.430756 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:40.252552 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:41.074567 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:41.888278 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:42.706373 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:43.525591 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:44.347436 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:45.104042 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:45.858595 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:46.612268 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:47.365319 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:48.618071 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:49.977807 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:51.311258 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:02:52.538265 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:02:52.540073 [debug] [Thread-1 (]: SQL status: OK in 15.795 seconds
[0m11:02:52.593462 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: ROLLBACK
[0m11:02:52.595012 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:02:52.596211 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: Close
[0m11:02:53.159586 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8921776-6015-4868-b365-ec55fa1d72e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b67f9130>]}
[0m11:02:53.160768 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m11:02:53.162435 [info ] [Thread-1 (]: 1 of 1 OK created sql table model default.vehicle_silver ....................... [[32mOK[0m in 18.60s]
[0m11:02:53.165322 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_silver
[0m11:02:53.174123 [debug] [MainThread]: On master: ROLLBACK
[0m11:02:53.175450 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:02:54.896163 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:02:54.897438 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:02:54.898408 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:02:54.899537 [debug] [MainThread]: On master: ROLLBACK
[0m11:02:54.900475 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:02:54.901356 [debug] [MainThread]: On master: Close
[0m11:02:55.200233 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:02:55.201480 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_silver' was properly closed.
[0m11:02:55.202585 [info ] [MainThread]: 
[0m11:02:55.204924 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 28.02 seconds (28.02s).
[0m11:02:55.208044 [debug] [MainThread]: Command end result
[0m11:02:55.268777 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:02:55.276231 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:02:55.294736 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m11:02:55.296038 [info ] [MainThread]: 
[0m11:02:55.298533 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:02:55.301130 [info ] [MainThread]: 
[0m11:02:55.303727 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m11:02:55.309076 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 29.985836, "process_in_blocks": "0", "process_kernel_time": 1.234375, "process_mem_max_rss": "105596", "process_out_blocks": "0", "process_user_time": 3.21875}
[0m11:02:55.352871 [debug] [MainThread]: Command `dbt run` succeeded at 11:02:55.351850 after 30.03 seconds
[0m11:02:55.355111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b535e180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b502c9b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55b5095520>]}
[0m11:02:55.356782 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:02:55.357997 [debug] [MainThread]: Flushing usage events
[0m11:03:05.209104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd308e6c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd406e570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd308f5f0>]}
[0m11:03:05.214091 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:03:05.214540 | 26da82b5-5487-4d34-88e1-40c8777790db ==============================
[0m11:03:05.214540 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:03:05.216190 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select vehicle_gold', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:03:05.435047 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:03:05.435567 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:03:05.435939 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:03:05.592769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '26da82b5-5487-4d34-88e1-40c8777790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd2f8fd70>]}
[0m11:03:05.593246 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:03:05.650588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '26da82b5-5487-4d34-88e1-40c8777790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd27c2810>]}
[0m11:03:05.651108 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:03:05.651632 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:03:05.877072 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:03:06.037334 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:03:06.037790 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:03:06.066564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '26da82b5-5487-4d34-88e1-40c8777790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd2566db0>]}
[0m11:03:06.067007 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:03:06.129792 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:03:06.135369 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:03:06.167210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '26da82b5-5487-4d34-88e1-40c8777790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd22ad880>]}
[0m11:03:06.167707 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:03:06.168104 [info ] [MainThread]: Found 3 models, 1 source, 475 macros
[0m11:03:06.169818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '26da82b5-5487-4d34-88e1-40c8777790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd2366960>]}
[0m11:03:06.170374 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:03:06.172979 [info ] [MainThread]: 
[0m11:03:06.174912 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:03:06.176169 [info ] [MainThread]: 
[0m11:03:06.177958 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:03:06.181226 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:03:06.196389 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:03:06.196885 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:03:06.197193 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:03:08.252988 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:03:08.254351 [debug] [ThreadPool]: SQL status: OK in 2.057 seconds
[0m11:03:09.231354 [debug] [ThreadPool]: On list_schemas: Close
[0m11:03:09.813884 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:03:09.826981 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:03:09.828342 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:03:09.829406 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:03:09.830346 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:03:11.994437 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:03:11.995772 [debug] [ThreadPool]: SQL status: OK in 2.165 seconds
[0m11:03:12.922080 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m11:03:12.923407 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:03:12.924463 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:03:12.925411 [debug] [ThreadPool]: On list_None_default: Close
[0m11:03:13.480804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '26da82b5-5487-4d34-88e1-40c8777790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd25b3b90>]}
[0m11:03:13.482048 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:03:13.483173 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:03:13.484190 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:03:13.511615 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_gold
[0m11:03:13.513447 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_gold .............................. [RUN]
[0m11:03:13.515930 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_gold)
[0m11:03:13.517137 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_gold
[0m11:03:13.535584 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_gold"
[0m11:03:13.542507 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_gold
[0m11:03:13.614125 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_gold"
[0m11:03:13.615627 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_gold: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_gold"} */
drop table if exists default.vehicle_gold
[0m11:03:13.616843 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:03:15.577381 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:03:15.579717 [debug] [Thread-1 (]: SQL status: OK in 1.963 seconds
[0m11:03:15.781607 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_gold"
[0m11:03:15.787824 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:03:15.789662 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_gold"
[0m11:03:15.791446 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_gold: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_gold"} */

  
    
        create table default.vehicle_gold
      
      
      
      
      
      
      
      

      as
      select model_year, state, count(1) as count_of_vehicle from vehicle_poc.vehicle_silver
group by model_year, state
  
[0m11:03:16.089453 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_gold"} */

  
    
        create table default.vehicle_gold
      
      
      
      
      
      
      
      

      as
      select model_year, state, count(1) as count_of_vehicle from vehicle_poc.vehicle_silver
group by model_year, state
  
[0m11:03:16.091331 [debug] [Thread-1 (]: Spark adapter: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=["*org.apache.hive.service.cli.HiveSQLException:Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 16:66 Table not found 'vehicle_silver':28:27", 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:342', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:198', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:261', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:256', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:550', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementAsync:HiveSessionImpl.java:536', 'jdk.internal.reflect.GeneratedMethodAccessor17:invoke::-1', 'jdk.internal.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:569', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:712', 'javax.security.auth.Subject:doAs:Subject.java:439', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1953', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'jdk.proxy2.$Proxy41:executeStatementAsync::-1', 'org.apache.hive.service.cli.CLIService:executeStatementAsync:CLIService.java:318', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:576', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1550', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1530', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:38', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:38', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:313', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1136', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:635', 'java.lang.Thread:run:Thread.java:840', "*org.apache.hadoop.hive.ql.parse.SemanticException:Line 16:66 Table not found 'vehicle_silver':38:11", 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2166', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:getMetaData:SemanticAnalyzer.java:2079', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:genResolvedParseTree:SemanticAnalyzer.java:12242', 'org.apache.hadoop.hive.ql.parse.SemanticAnalyzer:analyzeInternal:SemanticAnalyzer.java:12336', 'org.apache.hadoop.hive.ql.parse.CalcitePlanner:analyzeInternal:CalcitePlanner.java:365', 'org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer:analyze:BaseSemanticAnalyzer.java:295', 'org.apache.hadoop.hive.ql.Driver:compile:Driver.java:674', 'org.apache.hadoop.hive.ql.Driver:compileInternal:Driver.java:1871', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1818', 'org.apache.hadoop.hive.ql.Driver:compileAndRespond:Driver.java:1813', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:compileAndRespond:ReExecDriver.java:126', 'org.apache.hive.service.cli.operation.SQLOperation:prepare:SQLOperation.java:196'], sqlState='42S02', errorCode=10001, errorMessage="Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 16:66 Table not found 'vehicle_silver'"), operationHandle=None)
[0m11:03:16.093362 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_gold: ROLLBACK
[0m11:03:16.094536 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:03:16.095578 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_gold: Close
[0m11:03:16.372091 [debug] [Thread-1 (]: Runtime Error in model vehicle_gold (models/vehicle_poc/vehicle_gold.sql)
  Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 16:66 Table not found 'vehicle_silver'
[0m11:03:16.378224 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '26da82b5-5487-4d34-88e1-40c8777790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd2501b20>]}
[0m11:03:16.379328 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m11:03:16.381240 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.vehicle_gold ..................... [[31mERROR[0m in 2.86s]
[0m11:03:16.384205 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_gold
[0m11:03:16.386134 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_emr_app.vehicle_gold' to be skipped because of status 'error'.  Reason: Runtime Error in model vehicle_gold (models/vehicle_poc/vehicle_gold.sql)
  Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 16:66 Table not found 'vehicle_silver'.
[0m11:03:16.397101 [debug] [MainThread]: On master: ROLLBACK
[0m11:03:16.398368 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:03:18.089199 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:03:18.090698 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:03:18.091703 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:03:18.092592 [debug] [MainThread]: On master: ROLLBACK
[0m11:03:18.093450 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:03:18.094275 [debug] [MainThread]: On master: Close
[0m11:03:18.427495 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:03:18.428737 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_gold' was properly closed.
[0m11:03:18.429733 [info ] [MainThread]: 
[0m11:03:18.431952 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.25 seconds (12.25s).
[0m11:03:18.435012 [debug] [MainThread]: Command end result
[0m11:03:18.493469 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:03:18.500492 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:03:18.518746 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m11:03:18.519873 [info ] [MainThread]: 
[0m11:03:18.522185 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:03:18.524048 [info ] [MainThread]: 
[0m11:03:18.526526 [error] [MainThread]:   Runtime Error in model vehicle_gold (models/vehicle_poc/vehicle_gold.sql)
  Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 16:66 Table not found 'vehicle_silver'
[0m11:03:18.529172 [info ] [MainThread]: 
[0m11:03:18.531796 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:03:18.537964 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 13.397271, "process_in_blocks": "0", "process_kernel_time": 1.15625, "process_mem_max_rss": "101112", "process_out_blocks": "0", "process_user_time": 2.421875}
[0m11:03:18.539991 [debug] [MainThread]: Command `dbt run` failed at 11:03:18.539482 after 13.40 seconds
[0m11:03:18.542340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd67a3140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd2323fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcbd2323f80>]}
[0m11:03:18.543996 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:03:18.545396 [debug] [MainThread]: Flushing usage events
[0m11:05:37.270528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a12b3fe00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a145564e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a134f1700>]}
[0m11:05:37.275125 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:05:37.275519 | 7e3a1912-bb77-4596-a98e-817a55cff843 ==============================
[0m11:05:37.275519 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:05:37.276820 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt compile --select vehicle_silver', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:05:37.504957 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:05:37.505829 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:05:37.506404 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:05:37.678277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7e3a1912-bb77-4596-a98e-817a55cff843', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a11d80ce0>]}
[0m11:05:37.678772 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:05:37.732813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7e3a1912-bb77-4596-a98e-817a55cff843', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a125c5f70>]}
[0m11:05:37.733325 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:05:37.733853 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:05:37.951914 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:05:38.105373 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m11:05:38.106050 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/schema.yml
[0m11:05:38.106523 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_gold.sql
[0m11:05:38.106947 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_silver.sql
[0m11:05:38.935484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7e3a1912-bb77-4596-a98e-817a55cff843', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a11c3b7a0>]}
[0m11:05:38.936917 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:05:39.135290 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:05:39.150799 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:05:39.220103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7e3a1912-bb77-4596-a98e-817a55cff843', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a11c19eb0>]}
[0m11:05:39.221270 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:05:39.222331 [info ] [MainThread]: Found 3 models, 2 sources, 475 macros
[0m11:05:39.224193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e3a1912-bb77-4596-a98e-817a55cff843', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a11f3a960>]}
[0m11:05:39.225140 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:05:39.229014 [info ] [MainThread]: 
[0m11:05:39.230730 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:05:39.232071 [info ] [MainThread]: 
[0m11:05:39.233580 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:05:39.251833 [debug] [ThreadPool]: Acquiring new spark connection 'list_None_default'
[0m11:05:39.282042 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:05:39.283459 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:05:39.284579 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:05:39.285525 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:05:41.195904 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:05:41.197262 [debug] [ThreadPool]: SQL status: OK in 1.912 seconds
[0m11:05:42.020604 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m11:05:42.022066 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:05:42.023146 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:05:42.024124 [debug] [ThreadPool]: On list_None_default: Close
[0m11:05:42.624696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e3a1912-bb77-4596-a98e-817a55cff843', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a14372db0>]}
[0m11:05:42.625966 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:05:42.656268 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_silver
[0m11:05:42.657916 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_silver)
[0m11:05:42.659194 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_silver
[0m11:05:42.680216 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_silver"
[0m11:05:42.686143 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_silver
[0m11:05:42.688436 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_silver
[0m11:05:42.695826 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:05:42.697007 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_silver' was properly closed.
[0m11:05:42.699348 [debug] [MainThread]: Command end result
[0m11:05:42.759812 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:05:42.768963 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:05:42.793430 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m11:05:42.794681 [info ] [MainThread]: Compiled node 'vehicle_silver' is:
select * from vehicle_poc.vehicle_bronze
where electric_range > 200
[0m11:05:42.800979 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 5.6080527, "process_in_blocks": "0", "process_kernel_time": 1.203125, "process_mem_max_rss": "105696", "process_out_blocks": "0", "process_user_time": 2.953125}
[0m11:05:42.802744 [debug] [MainThread]: Command `dbt compile` succeeded at 11:05:42.802303 after 5.61 seconds
[0m11:05:42.803904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a1604f230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a12ba4ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a12ba7dd0>]}
[0m11:05:42.804869 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:05:42.805743 [debug] [MainThread]: Flushing usage events
[0m11:06:34.427211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f981919d8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f981962f440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f981919d820>]}
[0m11:06:34.432408 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:06:34.432801 | 34d00219-f6c2-4327-ab96-f3a50f6b88ae ==============================
[0m11:06:34.432801 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:06:34.435168 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select vehicle_silver', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:06:34.651193 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:06:34.651726 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:06:34.652106 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:06:34.841622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '34d00219-f6c2-4327-ab96-f3a50f6b88ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98191e5400>]}
[0m11:06:34.842119 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:06:34.897136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '34d00219-f6c2-4327-ab96-f3a50f6b88ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f981a5bb260>]}
[0m11:06:34.897645 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:06:34.898178 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:06:35.129185 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:06:35.336896 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m11:06:35.337850 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/schema.yml
[0m11:06:35.338514 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://models/vehicle_poc/vehicle_gold.sql
[0m11:06:36.531263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '34d00219-f6c2-4327-ab96-f3a50f6b88ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98132b8ce0>]}
[0m11:06:36.533485 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:06:36.733162 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:06:36.746298 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:06:36.826448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '34d00219-f6c2-4327-ab96-f3a50f6b88ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98131f0560>]}
[0m11:06:36.827659 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:06:36.828635 [info ] [MainThread]: Found 3 models, 3 sources, 475 macros
[0m11:06:36.829989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '34d00219-f6c2-4327-ab96-f3a50f6b88ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98132b9790>]}
[0m11:06:36.830619 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:06:36.833022 [info ] [MainThread]: 
[0m11:06:36.835191 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:06:36.837634 [info ] [MainThread]: 
[0m11:06:36.840024 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:06:36.842866 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:06:36.872595 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:06:36.874014 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:06:36.875119 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:06:38.842833 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:06:38.844182 [debug] [ThreadPool]: SQL status: OK in 1.969 seconds
[0m11:06:39.769312 [debug] [ThreadPool]: On list_schemas: Close
[0m11:06:40.352774 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:06:40.368458 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:06:40.369939 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:06:40.370944 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:06:40.371891 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:06:42.819296 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:06:42.820637 [debug] [ThreadPool]: SQL status: OK in 2.449 seconds
[0m11:06:43.575256 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about default: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m11:06:43.576689 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:06:43.577795 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:06:43.579183 [debug] [ThreadPool]: On list_None_default: Close
[0m11:06:44.188156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '34d00219-f6c2-4327-ab96-f3a50f6b88ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98131f0530>]}
[0m11:06:44.190061 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:06:44.191851 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:06:44.193808 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:06:44.227769 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_silver
[0m11:06:44.229606 [info ] [Thread-1 (]: 1 of 1 START sql table model default.vehicle_silver ............................ [RUN]
[0m11:06:44.232100 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_emr_app.vehicle_silver)
[0m11:06:44.233314 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_silver
[0m11:06:44.255086 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_silver"
[0m11:06:44.261151 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_silver
[0m11:06:44.337605 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_silver"
[0m11:06:44.339788 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_silver"} */
drop table if exists default.vehicle_silver
[0m11:06:44.340991 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:06:46.464412 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:06:46.465919 [debug] [Thread-1 (]: SQL status: OK in 2.125 seconds
[0m11:06:46.584260 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_silver"
[0m11:06:46.589313 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:06:46.590929 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_silver"
[0m11:06:46.592748 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_silver"} */

  
    
        create table default.vehicle_silver
      
      
      
      
      
      
      
      

      as
      select * from vehicle_poc.vehicle_bronze
where electric_range > 200
  
[0m11:06:47.697017 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:48.676358 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:49.697381 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:50.448129 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:51.236807 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:52.052946 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:52.871594 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:53.627432 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:54.406898 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:55.226373 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:55.977162 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:56.727953 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:57.991548 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:06:59.325501 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:07:00.759050 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:07:01.986462 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:07:01.988230 [debug] [Thread-1 (]: SQL status: OK in 15.394 seconds
[0m11:07:02.044303 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: ROLLBACK
[0m11:07:02.045940 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:07:02.047446 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: Close
[0m11:07:03.220889 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '34d00219-f6c2-4327-ab96-f3a50f6b88ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f981329bb90>]}
[0m11:07:03.222149 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m11:07:03.223762 [info ] [Thread-1 (]: 1 of 1 OK created sql table model default.vehicle_silver ....................... [[32mOK[0m in 18.98s]
[0m11:07:03.227095 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_silver
[0m11:07:03.235973 [debug] [MainThread]: On master: ROLLBACK
[0m11:07:03.237183 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:07:05.949553 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:07:05.950877 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:05.951860 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:07:05.952798 [debug] [MainThread]: On master: ROLLBACK
[0m11:07:05.953793 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:07:05.954611 [debug] [MainThread]: On master: Close
[0m11:07:06.211047 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:07:06.212356 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_silver' was properly closed.
[0m11:07:06.213442 [info ] [MainThread]: 
[0m11:07:06.215892 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 29.37 seconds (29.37s).
[0m11:07:06.218778 [debug] [MainThread]: Command end result
[0m11:07:06.275470 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:07:06.283656 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:07:06.305358 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m11:07:06.306705 [info ] [MainThread]: 
[0m11:07:06.309938 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:07:06.312618 [info ] [MainThread]: 
[0m11:07:06.315128 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m11:07:06.321608 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 31.963457, "process_in_blocks": "0", "process_kernel_time": 1.265625, "process_mem_max_rss": "106732", "process_out_blocks": "0", "process_user_time": 3.796875}
[0m11:07:06.323284 [debug] [MainThread]: Command `dbt run` succeeded at 11:07:06.322900 after 31.97 seconds
[0m11:07:06.324623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f981919c6b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f981919d8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98186bcf80>]}
[0m11:07:06.325652 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:07:06.326521 [debug] [MainThread]: Flushing usage events
[0m11:08:06.384890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe1958cbfe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe1973362a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe195af0200>]}
[0m11:08:06.391543 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:08:06.392132 | 06f6e21b-b31f-439b-bb49-407e091ab83c ==============================
[0m11:08:06.392132 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:08:06.393775 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:08:06.460101 [info ] [MainThread]: dbt version: 1.9.2
[0m11:08:06.461211 [info ] [MainThread]: python version: 3.12.3
[0m11:08:06.462136 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m11:08:06.462814 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m11:08:06.669216 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:08:06.669916 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:08:06.670331 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:08:06.701779 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m11:08:06.703940 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m11:08:06.704860 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m11:08:06.705880 [info ] [MainThread]: adapter type: spark
[0m11:08:06.706844 [info ] [MainThread]: adapter version: 1.9.1
[0m11:08:06.816527 [info ] [MainThread]: Configuration:
[0m11:08:06.818124 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:08:06.819139 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:08:06.820270 [info ] [MainThread]: Required dependencies:
[0m11:08:06.821299 [debug] [MainThread]: Executing "git --help"
[0m11:08:07.251093 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:08:07.252225 [debug] [MainThread]: STDERR: "b''"
[0m11:08:07.252626 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:08:07.254785 [info ] [MainThread]: Connection:
[0m11:08:07.256236 [info ] [MainThread]:   host: localhost
[0m11:08:07.257609 [info ] [MainThread]:   port: 10000
[0m11:08:07.258734 [info ] [MainThread]:   cluster: None
[0m11:08:07.259659 [info ] [MainThread]:   endpoint: None
[0m11:08:07.261216 [info ] [MainThread]:   schema: vehicle_poc
[0m11:08:07.262594 [info ] [MainThread]:   organization: 0
[0m11:08:07.264905 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:08:07.754526 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m11:08:07.755825 [debug] [MainThread]: Using spark connection "debug"
[0m11:08:07.756879 [debug] [MainThread]: On debug: select 1 as id
[0m11:08:07.757839 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:08:10.184706 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m11:08:10.186040 [debug] [MainThread]: SQL status: OK in 2.428 seconds
[0m11:08:10.188723 [debug] [MainThread]: On debug: Close
[0m11:08:10.799804 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:08:10.803037 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:08:10.807943 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 4.4936123, "process_in_blocks": "0", "process_kernel_time": 1.328125, "process_mem_max_rss": "94560", "process_out_blocks": "0", "process_user_time": 1.546875}
[0m11:08:10.809362 [debug] [MainThread]: Command `dbt debug` succeeded at 11:08:10.809036 after 4.50 seconds
[0m11:08:10.810434 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:08:10.811541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe195c803b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe1956ab590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe195421670>]}
[0m11:08:10.812545 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:08:10.813443 [debug] [MainThread]: Flushing usage events
[0m11:08:57.499309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7701e4fc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7701d3af30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7701e4ec90>]}
[0m11:08:57.505301 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:08:57.505765 | 024f99dc-c4e4-46dc-b249-715c034b0fb4 ==============================
[0m11:08:57.505765 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:08:57.507663 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --select vehicle_silver', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:08:57.730728 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:08:57.731289 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:08:57.731694 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:08:57.907701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '024f99dc-c4e4-46dc-b249-715c034b0fb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f770122b140>]}
[0m11:08:57.908187 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:08:57.961956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '024f99dc-c4e4-46dc-b249-715c034b0fb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f77012f4f20>]}
[0m11:08:57.962456 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:08:57.962980 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:08:58.188879 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:08:58.264758 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m11:08:58.266148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '024f99dc-c4e4-46dc-b249-715c034b0fb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7700f2c260>]}
[0m11:08:58.266509 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:09:01.071038 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid sources config given in models/schema.yml @ sources: {'name': None, 'schema': 'vehicle_poc', 'tables': [{'name': 'vehicle_silver'}]} - at path ['name']: None is not of type 'string'
[0m11:09:01.076845 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.6481845, "process_in_blocks": "0", "process_kernel_time": 0.953125, "process_mem_max_rss": "101568", "process_out_blocks": "0", "process_user_time": 4.578125}
[0m11:09:01.078342 [debug] [MainThread]: Command `dbt run` failed at 11:09:01.078003 after 3.65 seconds
[0m11:09:01.079474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f770119c9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f770012a750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7701946d50>]}
[0m11:09:01.080416 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:09:01.081277 [debug] [MainThread]: Flushing usage events
[0m11:09:46.866585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a059aa420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a07504b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a06248860>]}
[0m11:09:46.871849 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:09:46.872257 | 6aa71eb0-fe7d-444f-a46f-65481d0d8cf9 ==============================
[0m11:09:46.872257 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:09:46.873345 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select vehicle_silver', 'send_anonymous_usage_stats': 'True'}
[0m11:09:47.107513 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:09:47.108073 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:09:47.108435 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:09:47.258797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6aa71eb0-fe7d-444f-a46f-65481d0d8cf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a056ce1e0>]}
[0m11:09:47.259291 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:09:47.312538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6aa71eb0-fe7d-444f-a46f-65481d0d8cf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a07451cd0>]}
[0m11:09:47.313052 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:09:47.313559 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:09:47.555334 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:09:47.639535 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m11:09:47.640851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6aa71eb0-fe7d-444f-a46f-65481d0d8cf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a04cf8b90>]}
[0m11:09:47.641199 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:09:50.944018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6aa71eb0-fe7d-444f-a46f-65481d0d8cf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a04ceb440>]}
[0m11:09:50.945378 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:09:51.148150 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:09:51.168629 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:09:51.242171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6aa71eb0-fe7d-444f-a46f-65481d0d8cf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a040d17f0>]}
[0m11:09:51.243317 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:09:51.244327 [info ] [MainThread]: Found 3 models, 3 sources, 475 macros
[0m11:09:51.246404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6aa71eb0-fe7d-444f-a46f-65481d0d8cf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a058afe00>]}
[0m11:09:51.246954 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:09:51.249845 [info ] [MainThread]: 
[0m11:09:51.251664 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:09:51.253458 [info ] [MainThread]: 
[0m11:09:51.255522 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:09:51.259706 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:09:51.293577 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:09:51.295044 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:09:51.296125 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:09:53.404312 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:09:53.405933 [debug] [ThreadPool]: SQL status: OK in 2.110 seconds
[0m11:09:54.227486 [debug] [ThreadPool]: On list_schemas: Close
[0m11:09:54.757139 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_vehicle_poc)
[0m11:09:54.771339 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:54.772018 [debug] [ThreadPool]: Using spark connection "list_None_vehicle_poc"
[0m11:09:54.772490 [debug] [ThreadPool]: On list_None_vehicle_poc: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_vehicle_poc"} */
show table extended in vehicle_poc like '*'
  
[0m11:09:54.773158 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:09:56.911250 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:09:56.912648 [debug] [ThreadPool]: SQL status: OK in 2.139 seconds
[0m11:09:57.755566 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about vehicle_poc: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m11:09:57.757018 [debug] [ThreadPool]: On list_None_vehicle_poc: ROLLBACK
[0m11:09:57.758141 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:09:57.759124 [debug] [ThreadPool]: On list_None_vehicle_poc: Close
[0m11:09:58.266927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6aa71eb0-fe7d-444f-a46f-65481d0d8cf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a04e92ae0>]}
[0m11:09:58.268291 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:09:58.269465 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:58.270453 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:09:58.299691 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_silver
[0m11:09:58.301622 [info ] [Thread-1 (]: 1 of 1 START sql table model vehicle_poc.vehicle_silver ........................ [RUN]
[0m11:09:58.304124 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_vehicle_poc, now model.dbt_spark_emr_app.vehicle_silver)
[0m11:09:58.305419 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_silver
[0m11:09:58.326688 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_silver"
[0m11:09:58.331068 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_silver
[0m11:09:58.395677 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_silver"
[0m11:09:58.397123 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_silver"} */
drop table if exists vehicle_poc.vehicle_silver
[0m11:09:58.398292 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:10:00.676085 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:10:00.677564 [debug] [Thread-1 (]: SQL status: OK in 2.279 seconds
[0m11:10:00.796953 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_silver"
[0m11:10:00.801660 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:10:00.803052 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_silver"
[0m11:10:00.804217 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_silver"} */

  
    
        create table vehicle_poc.vehicle_silver
      
      
      
      
      
      
      
      

      as
      select * from vehicle_poc.vehicle_bronze
where electric_range > 200
  
[0m11:10:02.006547 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:02.825297 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:03.644309 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:04.462924 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:05.283196 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:06.105806 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:06.859392 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:07.611619 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:08.362016 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:09.114391 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:09.867469 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:10.619501 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:11.426445 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:12.677851 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:13.929109 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:15.114981 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:10:15.116422 [debug] [Thread-1 (]: SQL status: OK in 14.311 seconds
[0m11:10:15.170080 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: ROLLBACK
[0m11:10:15.171616 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:10:15.172808 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_silver: Close
[0m11:10:15.839160 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6aa71eb0-fe7d-444f-a46f-65481d0d8cf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a0638c740>]}
[0m11:10:15.840321 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m11:10:15.841996 [info ] [Thread-1 (]: 1 of 1 OK created sql table model vehicle_poc.vehicle_silver ................... [[32mOK[0m in 17.53s]
[0m11:10:15.845147 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_silver
[0m11:10:15.854245 [debug] [MainThread]: On master: ROLLBACK
[0m11:10:15.855488 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:10:17.569267 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:10:17.571912 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:10:17.573079 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:10:17.574043 [debug] [MainThread]: On master: ROLLBACK
[0m11:10:17.574986 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:10:17.575854 [debug] [MainThread]: On master: Close
[0m11:10:17.877333 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:10:17.879154 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_silver' was properly closed.
[0m11:10:17.880948 [info ] [MainThread]: 
[0m11:10:17.883466 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 26.63 seconds (26.63s).
[0m11:10:17.887067 [debug] [MainThread]: Command end result
[0m11:10:17.974350 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:10:17.981646 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:10:18.005167 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m11:10:18.006880 [info ] [MainThread]: 
[0m11:10:18.010007 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:10:18.012348 [info ] [MainThread]: 
[0m11:10:18.014314 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m11:10:18.019772 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 31.224636, "process_in_blocks": "0", "process_kernel_time": 1.421875, "process_mem_max_rss": "104728", "process_out_blocks": "0", "process_user_time": 5.671875}
[0m11:10:18.022163 [debug] [MainThread]: Command `dbt run` succeeded at 11:10:18.021646 after 31.23 seconds
[0m11:10:18.024014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a07206de0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a06b05760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0a06086780>]}
[0m11:10:18.025531 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:10:18.026955 [debug] [MainThread]: Flushing usage events
[0m11:10:36.581258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f259d7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f259c5f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f259d760>]}
[0m11:10:36.586640 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:10:36.587045 | a812b137-6c27-4e50-bc71-6929d7910e7d ==============================
[0m11:10:36.587045 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:10:36.588671 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --select vehicle_gold', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:10:36.802605 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:10:36.803115 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:10:36.803474 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:10:36.959513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a812b137-6c27-4e50-bc71-6929d7910e7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f2586bd0>]}
[0m11:10:36.960026 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:10:37.025905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a812b137-6c27-4e50-bc71-6929d7910e7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f21d0350>]}
[0m11:10:37.026659 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:10:37.027404 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:10:37.250003 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:10:37.394912 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:10:37.395384 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:10:37.428431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a812b137-6c27-4e50-bc71-6929d7910e7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f1ae39b0>]}
[0m11:10:37.428919 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:10:37.552200 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:10:37.564851 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:10:37.634895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a812b137-6c27-4e50-bc71-6929d7910e7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f1da3d40>]}
[0m11:10:37.636100 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:10:37.637150 [info ] [MainThread]: Found 3 models, 3 sources, 475 macros
[0m11:10:37.639412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a812b137-6c27-4e50-bc71-6929d7910e7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f1d81a30>]}
[0m11:10:37.640365 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:10:37.644906 [info ] [MainThread]: 
[0m11:10:37.646914 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:10:37.648740 [info ] [MainThread]: 
[0m11:10:37.651071 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:10:37.654917 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:10:37.689915 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:10:37.691379 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:10:37.692436 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:10:39.688278 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:10:39.689611 [debug] [ThreadPool]: SQL status: OK in 1.997 seconds
[0m11:10:40.514030 [debug] [ThreadPool]: On list_schemas: Close
[0m11:10:41.141676 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_vehicle_poc)
[0m11:10:41.156414 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:10:41.157659 [debug] [ThreadPool]: Using spark connection "list_None_vehicle_poc"
[0m11:10:41.158674 [debug] [ThreadPool]: On list_None_vehicle_poc: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "list_None_vehicle_poc"} */
show table extended in vehicle_poc like '*'
  
[0m11:10:41.159642 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:10:43.275648 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:10:43.277018 [debug] [ThreadPool]: SQL status: OK in 2.117 seconds
[0m11:10:44.196312 [debug] [ThreadPool]: Spark adapter: Error while retrieving information about vehicle_poc: Invalid value from "show tables extended ...", got 1 values, expected 4
[0m11:10:44.197736 [debug] [ThreadPool]: On list_None_vehicle_poc: ROLLBACK
[0m11:10:44.198786 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:10:44.199771 [debug] [ThreadPool]: On list_None_vehicle_poc: Close
[0m11:10:44.811110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a812b137-6c27-4e50-bc71-6929d7910e7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f259ff20>]}
[0m11:10:44.812373 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:10:44.813504 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:10:44.814512 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:10:44.843192 [debug] [Thread-1 (]: Began running node model.dbt_spark_emr_app.vehicle_gold
[0m11:10:44.844975 [info ] [Thread-1 (]: 1 of 1 START sql table model vehicle_poc.vehicle_gold .......................... [RUN]
[0m11:10:44.847547 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_vehicle_poc, now model.dbt_spark_emr_app.vehicle_gold)
[0m11:10:44.848773 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_emr_app.vehicle_gold
[0m11:10:44.869281 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_emr_app.vehicle_gold"
[0m11:10:44.873729 [debug] [Thread-1 (]: Began executing node model.dbt_spark_emr_app.vehicle_gold
[0m11:10:44.936319 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_gold"
[0m11:10:44.937865 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_gold: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_gold"} */
drop table if exists vehicle_poc.vehicle_gold
[0m11:10:44.939051 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:10:47.165926 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:10:47.168067 [debug] [Thread-1 (]: SQL status: OK in 2.229 seconds
[0m11:10:47.378804 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_emr_app.vehicle_gold"
[0m11:10:47.385156 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:10:47.386795 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_emr_app.vehicle_gold"
[0m11:10:47.388212 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_gold: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "node_id": "model.dbt_spark_emr_app.vehicle_gold"} */

  
    
        create table vehicle_poc.vehicle_gold
      
      
      
      
      
      
      
      

      as
      select model_year, state, count(1) as count_of_vehicle 
from vehicle_poc.vehicle_silver
group by model_year, state
  
[0m11:10:48.599330 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:49.351046 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:50.136619 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:50.888385 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:51.640055 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:52.489451 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:53.241842 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:54.023917 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:54.843879 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:55.599234 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:56.353393 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:57.106687 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:57.917511 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:10:59.169265 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:11:00.421215 [debug] [Thread-1 (]: Spark adapter: Poll status: 1, sleeping
[0m11:11:01.094385 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:11:01.095841 [debug] [Thread-1 (]: SQL status: OK in 13.706 seconds
[0m11:11:01.149537 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_gold: ROLLBACK
[0m11:11:01.151086 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:11:01.152196 [debug] [Thread-1 (]: On model.dbt_spark_emr_app.vehicle_gold: Close
[0m11:11:01.813476 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a812b137-6c27-4e50-bc71-6929d7910e7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f42990d0>]}
[0m11:11:01.814608 [debug] [Thread-1 (]: An error was encountered while trying to send an event
[0m11:11:01.816284 [info ] [Thread-1 (]: 1 of 1 OK created sql table model vehicle_poc.vehicle_gold ..................... [[32mOK[0m in 16.96s]
[0m11:11:01.819784 [debug] [Thread-1 (]: Finished running node model.dbt_spark_emr_app.vehicle_gold
[0m11:11:01.828796 [debug] [MainThread]: On master: ROLLBACK
[0m11:11:01.830199 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:11:03.546699 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:11:03.548014 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:11:03.548966 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:11:03.549832 [debug] [MainThread]: On master: ROLLBACK
[0m11:11:03.550693 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:11:03.551496 [debug] [MainThread]: On master: Close
[0m11:11:03.855317 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:11:03.856575 [debug] [MainThread]: Connection 'model.dbt_spark_emr_app.vehicle_gold' was properly closed.
[0m11:11:03.857578 [info ] [MainThread]: 
[0m11:11:03.859984 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 26.21 seconds (26.21s).
[0m11:11:03.862913 [debug] [MainThread]: Command end result
[0m11:11:03.919858 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m11:11:03.927187 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m11:11:03.945573 [debug] [MainThread]: Wrote artifact RunExecutionResult to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m11:11:03.946716 [info ] [MainThread]: 
[0m11:11:03.949323 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:11:03.951518 [info ] [MainThread]: 
[0m11:11:03.953559 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m11:11:03.958126 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 27.45125, "process_in_blocks": "0", "process_kernel_time": 1.28125, "process_mem_max_rss": "100676", "process_out_blocks": "0", "process_user_time": 2.515625}
[0m11:11:03.959513 [debug] [MainThread]: Command `dbt run` succeeded at 11:11:03.959188 after 27.45 seconds
[0m11:11:03.960651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f2b36780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f1e1f7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f2783560>]}
[0m11:11:03.961557 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:11:03.962471 [debug] [MainThread]: Flushing usage events
[0m15:13:50.966419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17116888c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f171168b740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1711b6d9a0>]}
[0m15:13:50.986720 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 15:13:50.987963 | fcf9f476-6b57-47a8-8d9a-81409e81be0a ==============================
[0m15:13:50.987963 [info ] [MainThread]: Running with dbt=1.9.2
[0m15:13:50.990769 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt run-operation create_external_table', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:13:52.475652 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:13:52.477186 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:13:52.478388 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:13:53.161621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fcf9f476-6b57-47a8-8d9a-81409e81be0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17119d2db0>]}
[0m15:13:53.162921 [debug] [MainThread]: An error was encountered while trying to send an event
[0m15:13:53.341418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fcf9f476-6b57-47a8-8d9a-81409e81be0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f171133f230>]}
[0m15:13:53.342850 [debug] [MainThread]: An error was encountered while trying to send an event
[0m15:13:53.344332 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m15:13:53.948035 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m15:13:54.599491 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:13:54.601405 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m15:13:54.848195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fcf9f476-6b57-47a8-8d9a-81409e81be0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f171083e5d0>]}
[0m15:13:54.850286 [debug] [MainThread]: An error was encountered while trying to send an event
[0m15:13:55.065963 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m15:13:55.083359 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m15:13:55.217179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fcf9f476-6b57-47a8-8d9a-81409e81be0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f171083fa10>]}
[0m15:13:55.218847 [debug] [MainThread]: An error was encountered while trying to send an event
[0m15:13:55.220926 [info ] [MainThread]: Found 3 models, 3 sources, 476 macros
[0m15:13:55.223556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fcf9f476-6b57-47a8-8d9a-81409e81be0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f171060b6b0>]}
[0m15:13:55.224586 [debug] [MainThread]: An error was encountered while trying to send an event
[0m15:13:55.226054 [debug] [MainThread]: Acquiring new spark connection 'macro_create_external_table'
[0m15:13:55.227252 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:55.228192 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:13:55.263936 [debug] [MainThread]: Using spark connection "macro_create_external_table"
[0m15:13:55.265338 [debug] [MainThread]: On macro_create_external_table: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_create_external_table"} */

    

        CREATE external TABLE if not exists vehicle_poc.vehicle_ext (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
            ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
            WITH SERDEPROPERTIES (
            'separatorChar' = ','
        )
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/bronze/'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m15:13:55.266740 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:16:11.973046 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m15:16:11.974704 [debug] [MainThread]: SQL status: OK in 136.708 seconds
[0m15:16:11.978615 [debug] [MainThread]: On macro_create_external_table: ROLLBACK
[0m15:16:11.979982 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:16:11.980864 [debug] [MainThread]: On macro_create_external_table: Close
[0m15:16:13.267805 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m15:16:13.275759 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 143.42918, "process_in_blocks": "0", "process_kernel_time": 5.140625, "process_mem_max_rss": "98908", "process_out_blocks": "0", "process_user_time": 3.828125}
[0m15:16:13.277448 [debug] [MainThread]: Command `dbt run-operation` succeeded at 15:16:13.276967 after 143.43 seconds
[0m15:16:13.279009 [debug] [MainThread]: Connection 'macro_create_external_table' was properly closed.
[0m15:16:13.281953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1711722030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f171083ecf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f171083f710>]}
[0m15:16:13.283219 [debug] [MainThread]: An error was encountered while trying to send an event
[0m15:16:13.284585 [debug] [MainThread]: Flushing usage events
[0m06:43:30.037848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0fcfcf5af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0fcfa445c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0fcfd97470>]}
[0m06:43:30.098921 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 06:43:30.100779 | 642614f2-82fc-42c2-8ea4-ef8e9e586f73 ==============================
[0m06:43:30.100779 [info ] [MainThread]: Running with dbt=1.9.2
[0m06:43:30.104170 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m06:43:30.807824 [info ] [MainThread]: dbt version: 1.9.2
[0m06:43:30.810061 [info ] [MainThread]: python version: 3.12.3
[0m06:43:30.811366 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m06:43:30.812812 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m06:43:41.582667 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m06:43:41.584198 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m06:43:41.585389 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m06:43:41.747186 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m06:43:41.750383 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m06:43:41.753145 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m06:43:41.755694 [info ] [MainThread]: adapter type: spark
[0m06:43:41.757711 [info ] [MainThread]: adapter version: 1.9.1
[0m06:43:42.153180 [info ] [MainThread]: Configuration:
[0m06:43:42.155848 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m06:43:42.158396 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m06:43:42.160867 [info ] [MainThread]: Required dependencies:
[0m06:43:42.163608 [debug] [MainThread]: Executing "git --help"
[0m06:43:42.528191 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m06:43:42.529770 [debug] [MainThread]: STDERR: "b''"
[0m06:43:42.531158 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m06:43:42.534400 [info ] [MainThread]: Connection:
[0m06:43:42.536498 [info ] [MainThread]:   host: localhost
[0m06:43:42.538478 [info ] [MainThread]:   port: 10000
[0m06:43:42.541159 [info ] [MainThread]:   cluster: None
[0m06:43:42.543568 [info ] [MainThread]:   endpoint: None
[0m06:43:42.545606 [info ] [MainThread]:   schema: vehicle_poc
[0m06:43:42.547677 [info ] [MainThread]:   organization: 0
[0m06:43:42.550356 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m06:43:43.588021 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m06:43:43.590192 [debug] [MainThread]: Using spark connection "debug"
[0m06:43:43.591746 [debug] [MainThread]: On debug: select 1 as id
[0m06:43:43.593179 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:43:46.076627 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m06:43:46.078550 [debug] [MainThread]: SQL status: OK in 2.485 seconds
[0m06:43:46.082800 [debug] [MainThread]: On debug: Close
[0m06:43:46.683753 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m06:43:46.687234 [info ] [MainThread]: [32mAll checks passed![0m
[0m06:43:46.716344 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 16.95653, "process_in_blocks": "0", "process_kernel_time": 5.953125, "process_mem_max_rss": "94552", "process_out_blocks": "0", "process_user_time": 5.015625}
[0m06:43:46.718610 [debug] [MainThread]: Command `dbt debug` succeeded at 06:43:46.718136 after 16.96 seconds
[0m06:43:46.720105 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m06:43:46.721661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0fcfbbaba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0fcf07a5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0fcf824bc0>]}
[0m06:43:46.723065 [debug] [MainThread]: An error was encountered while trying to send an event
[0m06:43:46.724343 [debug] [MainThread]: Flushing usage events
[0m11:54:20.918049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0067e1dcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0067e1d1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0067e1dc40>]}
[0m11:54:20.946655 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:54:20.947143 | 66f01ebd-9771-4816-8816-14fa4fc8d8e1 ==============================
[0m11:54:20.947143 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:54:20.948398 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt debug', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:54:21.293031 [info ] [MainThread]: dbt version: 1.9.2
[0m11:54:21.294198 [info ] [MainThread]: python version: 3.12.3
[0m11:54:21.294962 [info ] [MainThread]: python path: /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/pyenv/bin/python3
[0m11:54:21.295646 [info ] [MainThread]: os info: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.39
[0m11:54:26.152989 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:54:26.153513 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:54:26.153925 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:54:26.205096 [info ] [MainThread]: Using profiles dir at /root/.dbt
[0m11:54:26.206358 [info ] [MainThread]: Using profiles.yml file at /root/.dbt/profiles.yml
[0m11:54:26.207418 [info ] [MainThread]: Using dbt_project.yml file at /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/dbt_project.yml
[0m11:54:26.208430 [info ] [MainThread]: adapter type: spark
[0m11:54:26.209117 [info ] [MainThread]: adapter version: 1.9.1
[0m11:54:26.378687 [info ] [MainThread]: Configuration:
[0m11:54:26.380241 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:54:26.381447 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:54:26.382275 [info ] [MainThread]: Required dependencies:
[0m11:54:26.383186 [debug] [MainThread]: Executing "git --help"
[0m11:54:26.713821 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:54:26.714611 [debug] [MainThread]: STDERR: "b''"
[0m11:54:26.715106 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:54:26.716666 [info ] [MainThread]: Connection:
[0m11:54:26.717582 [info ] [MainThread]:   host: localhost
[0m11:54:26.718269 [info ] [MainThread]:   port: 10000
[0m11:54:26.718979 [info ] [MainThread]:   cluster: None
[0m11:54:26.719784 [info ] [MainThread]:   endpoint: None
[0m11:54:26.720413 [info ] [MainThread]:   schema: vehicle_poc
[0m11:54:26.721106 [info ] [MainThread]:   organization: 0
[0m11:54:26.721998 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:54:27.155143 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m11:54:27.155565 [debug] [MainThread]: Using spark connection "debug"
[0m11:54:27.155863 [debug] [MainThread]: On debug: select 1 as id
[0m11:54:27.156110 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:54:29.362266 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m11:54:29.362958 [debug] [MainThread]: SQL status: OK in 2.207 seconds
[0m11:54:29.364774 [debug] [MainThread]: On debug: Close
[0m11:54:29.853284 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:54:29.855590 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:54:29.876113 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 9.082064, "process_in_blocks": "0", "process_kernel_time": 3.28125, "process_mem_max_rss": "94568", "process_out_blocks": "0", "process_user_time": 2.46875}
[0m11:54:29.876810 [debug] [MainThread]: Command `dbt debug` succeeded at 11:54:29.876673 after 9.08 seconds
[0m11:54:29.877169 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:54:29.877548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0067e1c320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00677041d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00675bfc20>]}
[0m11:54:29.877853 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:54:29.878077 [debug] [MainThread]: Flushing usage events
[0m11:57:44.659011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd39ac8d8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd39ac8c740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd39ac8d760>]}
[0m11:57:44.662584 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:57:44.662935 | d4e22ad3-a609-40c4-8650-7d321c65b265 ==============================
[0m11:57:44.662935 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:57:44.664115 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run-operation create_bronze_ext', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:57:44.838641 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:57:44.839118 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:57:44.839442 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:57:44.998526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd4e22ad3-a609-40c4-8650-7d321c65b265', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd39b84da60>]}
[0m11:57:44.998954 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:57:45.044164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd4e22ad3-a609-40c4-8650-7d321c65b265', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd39ae6fa40>]}
[0m11:57:45.044636 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:57:45.045044 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:57:45.265383 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:57:47.219259 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:57:47.220589 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m11:57:47.248654 [error] [MainThread]: Encountered an error:
Compilation Error
  expected token 'end of print statement', got 'run_query'
    line 24
      {{ do run_query(query) }}
[0m11:57:47.253583 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 2.6695077, "process_in_blocks": "0", "process_kernel_time": 1.078125, "process_mem_max_rss": "97720", "process_out_blocks": "0", "process_user_time": 1.25}
[0m11:57:47.254622 [debug] [MainThread]: Command `dbt run-operation` failed at 11:57:47.254385 after 2.67 seconds
[0m11:57:47.255447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd39ac8cfe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd39e534200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd39acb4350>]}
[0m11:57:47.256143 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:57:47.256735 [debug] [MainThread]: Flushing usage events
[0m11:58:49.194818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5d132cdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5d132dd30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5d112b770>]}
[0m11:58:49.198428 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:58:49.198762 | 778617c0-63c1-4f6b-bc53-182014a188dc ==============================
[0m11:58:49.198762 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:58:49.200062 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run-operation bronze', 'send_anonymous_usage_stats': 'True'}
[0m11:58:49.361629 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:58:49.362043 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:58:49.362323 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:58:49.485498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '778617c0-63c1-4f6b-bc53-182014a188dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5d0fdfa70>]}
[0m11:58:49.485907 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:58:49.529937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '778617c0-63c1-4f6b-bc53-182014a188dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5d136a690>]}
[0m11:58:49.530375 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:58:49.530784 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:58:49.692887 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:58:49.807981 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:58:49.808508 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m11:58:49.812082 [error] [MainThread]: Encountered an error:
Compilation Error
  expected token 'end of print statement', got 'run_query'
    line 24
      {{ do run_query(query) }}
[0m11:58:49.815880 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 0.6749381, "process_in_blocks": "0", "process_kernel_time": 0.875, "process_mem_max_rss": "97720", "process_out_blocks": "0", "process_user_time": 1.328125}
[0m11:58:49.816260 [debug] [MainThread]: Command `dbt run-operation` failed at 11:58:49.816179 after 0.68 seconds
[0m11:58:49.816587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5d2d46060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5d0574170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff5d1397e60>]}
[0m11:58:49.816845 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:58:49.817050 [debug] [MainThread]: Flushing usage events
[0m11:59:32.280024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f20f281f3e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f20f2858470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f20f3b02660>]}
[0m11:59:32.283474 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 11:59:32.283813 | 099d1d48-44d4-4724-a9d1-95c2bcff35c8 ==============================
[0m11:59:32.283813 [info ] [MainThread]: Running with dbt=1.9.2
[0m11:59:32.284985 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run-operation bronze', 'send_anonymous_usage_stats': 'True'}
[0m11:59:32.445132 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:59:32.445594 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:59:32.445881 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:59:32.573300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '099d1d48-44d4-4724-a9d1-95c2bcff35c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f20f1337fb0>]}
[0m11:59:32.573721 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:59:32.616987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '099d1d48-44d4-4724-a9d1-95c2bcff35c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f20f1312600>]}
[0m11:59:32.617382 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:59:32.617782 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m11:59:32.775018 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m11:59:32.888565 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:59:32.889093 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m11:59:32.892560 [error] [MainThread]: Encountered an error:
Compilation Error
  expected token 'end of print statement', got 'run_query'
    line 24
      {{ do run_query(create_bronze_ext) }}
[0m11:59:32.896183 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": false, "command_wall_clock_time": 0.6705432, "process_in_blocks": "0", "process_kernel_time": 0.890625, "process_mem_max_rss": "97708", "process_out_blocks": "0", "process_user_time": 1.296875}
[0m11:59:32.896629 [debug] [MainThread]: Command `dbt run-operation` failed at 11:59:32.896516 after 0.67 seconds
[0m11:59:32.896945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f20f24e73e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f20f118e720>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f20f1335100>]}
[0m11:59:32.897172 [debug] [MainThread]: An error was encountered while trying to send an event
[0m11:59:32.897367 [debug] [MainThread]: Flushing usage events
[0m12:00:32.131361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe09164e330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe092c26750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe091979a30>]}
[0m12:00:32.134931 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:00:32.135298 | a63c432d-b9d8-4e98-a073-bda325557903 ==============================
[0m12:00:32.135298 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:00:32.136395 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run-operation bronze', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:00:32.295175 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:00:32.295555 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:00:32.295833 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:00:32.420749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a63c432d-b9d8-4e98-a073-bda325557903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0911ed8b0>]}
[0m12:00:32.421134 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:00:32.465001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a63c432d-b9d8-4e98-a073-bda325557903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0908f84d0>]}
[0m12:00:32.465425 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:00:32.465845 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:00:32.632853 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m12:00:32.747463 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:00:32.748033 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m12:00:32.807812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a63c432d-b9d8-4e98-a073-bda325557903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe090437e30>]}
[0m12:00:32.808295 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:00:32.861808 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m12:00:32.893060 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m12:00:32.922474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a63c432d-b9d8-4e98-a073-bda325557903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0903dfe60>]}
[0m12:00:32.922858 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:00:32.923182 [info ] [MainThread]: Found 3 models, 3 sources, 479 macros
[0m12:00:32.924533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a63c432d-b9d8-4e98-a073-bda325557903', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe090436c60>]}
[0m12:00:32.924774 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:00:32.925171 [debug] [MainThread]: Acquiring new spark connection 'macro_bronze'
[0m12:00:32.925432 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:00:32.925643 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:00:32.933695 [debug] [MainThread]: Using spark connection "macro_bronze"
[0m12:00:32.934106 [debug] [MainThread]: On macro_bronze: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_bronze"} */

    
        CREATE external TABLE if not exists s3_dbt.bronze (
        VIN string, County string, City string, State string, Postal_Code integer, 
        Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
        Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
        Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
        Electric_Utility string, Census_Tract float
        )
        ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
        WITH SERDEPROPERTIES ('separatorChar' = ',')
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/bronze/'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');

    
  
[0m12:00:32.934365 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:00:34.725373 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m12:00:34.726443 [debug] [MainThread]: SQL status: OK in 1.792 seconds
[0m12:00:34.729317 [debug] [MainThread]: On macro_bronze: ROLLBACK
[0m12:00:34.730184 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:00:34.731079 [debug] [MainThread]: On macro_bronze: Close
[0m12:00:35.235317 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m12:00:35.239605 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 3.1656692, "process_in_blocks": "0", "process_kernel_time": 0.640625, "process_mem_max_rss": "98952", "process_out_blocks": "0", "process_user_time": 1.640625}
[0m12:00:35.241297 [debug] [MainThread]: Command `dbt run-operation` succeeded at 12:00:35.240941 after 3.17 seconds
[0m12:00:35.242515 [debug] [MainThread]: Connection 'macro_bronze' was properly closed.
[0m12:00:35.243762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0946d3110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0901a1490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0903df5c0>]}
[0m12:00:35.245160 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:00:35.246251 [debug] [MainThread]: Flushing usage events
[0m12:01:08.192452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528e883b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5284b30b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb528577110>]}
[0m12:01:08.195939 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:01:08.196256 | d48b1cda-5757-46b2-95fe-6dd8e19cbeee ==============================
[0m12:01:08.196256 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:01:08.197339 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation silver', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:01:08.358476 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:01:08.358856 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:01:08.359160 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:01:08.484846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd48b1cda-5757-46b2-95fe-6dd8e19cbeee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb52970a2a0>]}
[0m12:01:08.485251 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:01:08.529202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd48b1cda-5757-46b2-95fe-6dd8e19cbeee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5276610a0>]}
[0m12:01:08.529617 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:01:08.530034 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:01:08.692860 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m12:01:08.808872 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:01:08.809237 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:01:08.832708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd48b1cda-5757-46b2-95fe-6dd8e19cbeee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb5271e72f0>]}
[0m12:01:08.833084 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:01:08.885906 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m12:01:08.889738 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m12:01:08.906151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd48b1cda-5757-46b2-95fe-6dd8e19cbeee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb526fa3560>]}
[0m12:01:08.906568 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:01:08.906863 [info ] [MainThread]: Found 3 models, 3 sources, 479 macros
[0m12:01:08.907812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd48b1cda-5757-46b2-95fe-6dd8e19cbeee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb526f93950>]}
[0m12:01:08.908050 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:01:08.908441 [debug] [MainThread]: Acquiring new spark connection 'macro_silver'
[0m12:01:08.908726 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:01:08.908964 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:01:08.919240 [debug] [MainThread]: Using spark connection "macro_silver"
[0m12:01:08.920079 [debug] [MainThread]: On macro_silver: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_silver"} */

    

        insert overwrite directory 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/silver/'
        row format delimited
        fields terminated by ','
        select * from s3_dbt.bronze where make not in ("LEXUS", "MITSUBISHI", "RAM");

    
  
[0m12:01:08.920468 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:01:11.643639 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:12.386014 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:13.128721 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:13.874064 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:14.617848 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:15.366955 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:16.110692 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:16.857374 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:17.619536 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:18.374209 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:19.123476 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:19.874717 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:20.617668 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:21.860544 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:23.126295 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:24.370876 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:25.619392 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:26.863057 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:28.105980 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:29.399697 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:30.749828 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:32.494968 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:01:33.542987 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m12:01:33.544089 [debug] [MainThread]: SQL status: OK in 24.624 seconds
[0m12:01:33.546063 [debug] [MainThread]: Using spark connection "macro_silver"
[0m12:01:33.546388 [debug] [MainThread]: On macro_silver: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_silver"} */

    

        CREATE external TABLE  s3_dbt.silver(
            VIN string, County string, City string, State string, Postal_Code integer, 
            Model_Year integer, Make string, Model string, Electric_Vehicle_Type string, 
            Clean_Alternative_Fuel_Vehicle string, Electric_Range integer, Base_MSRP integer, 
            Legislative_District integer, DOL_Vehicle_ID integer, Vehicle_Location string, 
            Electric_Utility string, Census_Tract float
            )
        ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
        WITH SERDEPROPERTIES ('separatorChar' = ',')
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/silver/'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');
        
    
  
[0m12:01:34.039240 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m12:01:34.040305 [debug] [MainThread]: SQL status: OK in 0.493 seconds
[0m12:01:34.043014 [debug] [MainThread]: On macro_silver: ROLLBACK
[0m12:01:34.043865 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:01:34.044603 [debug] [MainThread]: On macro_silver: Close
[0m12:01:34.600699 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m12:01:34.605299 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 26.466322, "process_in_blocks": "0", "process_kernel_time": 0.953125, "process_mem_max_rss": "98404", "process_out_blocks": "0", "process_user_time": 1.34375}
[0m12:01:34.607015 [debug] [MainThread]: Command `dbt run-operation` succeeded at 12:01:34.606636 after 26.47 seconds
[0m12:01:34.608314 [debug] [MainThread]: Connection 'macro_silver' was properly closed.
[0m12:01:34.609898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb527d0f650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb526fa38c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb526f40f80>]}
[0m12:01:34.611116 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:01:34.612276 [debug] [MainThread]: Flushing usage events
[0m12:02:12.979185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01538b4f80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01539e6360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01526ff050>]}
[0m12:02:12.982753 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:02:12.983076 | 1428eb55-3edb-479e-b2dc-93ab84a94e7e ==============================
[0m12:02:12.983076 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:02:12.984218 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/root/.dbt', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run-operation gold', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:02:13.145526 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:02:13.145941 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:02:13.146227 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:02:13.272374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1428eb55-3edb-479e-b2dc-93ab84a94e7e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0151311160>]}
[0m12:02:13.272854 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:02:13.322644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1428eb55-3edb-479e-b2dc-93ab84a94e7e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01522859d0>]}
[0m12:02:13.323107 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:02:13.323551 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:02:13.487333 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m12:02:13.601556 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:02:13.601939 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:02:13.626563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1428eb55-3edb-479e-b2dc-93ab84a94e7e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0150fe3fe0>]}
[0m12:02:13.627058 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:02:13.677532 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m12:02:13.681248 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m12:02:13.697386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1428eb55-3edb-479e-b2dc-93ab84a94e7e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01526fc0e0>]}
[0m12:02:13.697794 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:02:13.698118 [info ] [MainThread]: Found 3 models, 3 sources, 479 macros
[0m12:02:13.698972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1428eb55-3edb-479e-b2dc-93ab84a94e7e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01513c8560>]}
[0m12:02:13.699256 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:02:13.699695 [debug] [MainThread]: Acquiring new spark connection 'macro_gold'
[0m12:02:13.699990 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:02:13.700238 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:02:13.709315 [debug] [MainThread]: Using spark connection "macro_gold"
[0m12:02:13.709738 [debug] [MainThread]: On macro_gold: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_gold"} */

    
        INSERT OVERWRITE DIRECTORY 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/gold/'
        row format delimited
        fields terminated by ','
        select *, 
            sum(cnt) over(partition by make, model_year order by make rows unbounded preceding) as cons_sum
        from (select make, model_year, count(make) as cnt from s3_dbt.silver group by model_year, make order by make
        ) as tb1;
    
  
[0m12:02:13.709998 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:02:16.255310 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:17.000395 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:17.748951 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:18.492641 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:19.235252 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:19.991274 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:20.734200 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:21.479266 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:22.222631 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:22.966146 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:23.709902 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:24.603424 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:25.347852 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:26.595818 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:27.839798 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:29.082607 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:30.326076 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:31.644261 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:32.890645 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:34.131950 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:35.374977 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:37.215814 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:38.968981 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:02:39.232273 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m12:02:39.233441 [debug] [MainThread]: SQL status: OK in 25.523 seconds
[0m12:02:39.236629 [debug] [MainThread]: Using spark connection "macro_gold"
[0m12:02:39.237521 [debug] [MainThread]: On macro_gold: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_gold"} */

    

        CREATE EXTERNAL TABLE if not exists s3_dbt.gold (
            make string,
            model_year int,
            cnt int, 
            cons_sum int
        )
        ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
        WITH SERDEPROPERTIES ('separatorChar' = ',')
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/gold/'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');
        
    
  
[0m12:02:39.962604 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m12:02:39.963669 [debug] [MainThread]: SQL status: OK in 0.725 seconds
[0m12:02:39.966382 [debug] [MainThread]: On macro_gold: ROLLBACK
[0m12:02:39.967322 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:02:39.968132 [debug] [MainThread]: On macro_gold: Close
[0m12:02:40.520020 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m12:02:40.522772 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 27.596485, "process_in_blocks": "0", "process_kernel_time": 0.84375, "process_mem_max_rss": "98428", "process_out_blocks": "0", "process_user_time": 1.453125}
[0m12:02:40.523855 [debug] [MainThread]: Command `dbt run-operation` succeeded at 12:02:40.523609 after 27.60 seconds
[0m12:02:40.524597 [debug] [MainThread]: Connection 'macro_gold' was properly closed.
[0m12:02:40.525363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0151fcf320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01512a1cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0151a0f170>]}
[0m12:02:40.526049 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:02:40.526668 [debug] [MainThread]: Flushing usage events
[0m12:05:23.025894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe45a69ade0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe45ad93230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe45ad93110>]}
[0m12:05:23.029511 [debug] [MainThread]: An error was encountered while trying to send an event


============================== 12:05:23.029861 | 6b56d7b4-c14b-4f32-86cc-d8b88200713b ==============================
[0m12:05:23.029861 [info ] [MainThread]: Running with dbt=1.9.2
[0m12:05:23.030849 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/root/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt run-operation gold', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:05:23.192424 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:05:23.192833 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:05:23.193117 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:05:23.319505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6b56d7b4-c14b-4f32-86cc-d8b88200713b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe459e96090>]}
[0m12:05:23.319892 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:05:23.362875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6b56d7b4-c14b-4f32-86cc-d8b88200713b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe45aa0a810>]}
[0m12:05:23.363280 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:05:23.363679 [info ] [MainThread]: Registered adapter: spark=1.9.1
[0m12:05:23.525320 [debug] [MainThread]: checksum: 12b12750b70de726cfd89136b8e24afc3f3e77597a97bff40ab7e5f9b39d5e18, vars: {}, profile: , target: , version: 1.9.2
[0m12:05:23.637905 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:05:23.638468 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_emr_app://macros/factory.sql
[0m12:05:23.698140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6b56d7b4-c14b-4f32-86cc-d8b88200713b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe459a63da0>]}
[0m12:05:23.698577 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:05:23.750550 [debug] [MainThread]: Wrote artifact WritableManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/manifest.json
[0m12:05:23.754526 [debug] [MainThread]: Wrote artifact SemanticManifest to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/semantic_manifest.json
[0m12:05:23.770674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6b56d7b4-c14b-4f32-86cc-d8b88200713b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe459987950>]}
[0m12:05:23.771069 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:05:23.771378 [info ] [MainThread]: Found 3 models, 3 sources, 479 macros
[0m12:05:23.772534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6b56d7b4-c14b-4f32-86cc-d8b88200713b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe459739280>]}
[0m12:05:23.772778 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:05:23.773260 [debug] [MainThread]: Acquiring new spark connection 'macro_gold'
[0m12:05:23.773529 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:23.773759 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:05:23.782374 [debug] [MainThread]: Using spark connection "macro_gold"
[0m12:05:23.782786 [debug] [MainThread]: On macro_gold: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_gold"} */

    
        INSERT OVERWRITE DIRECTORY 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/gold/'
        row format delimited
        fields terminated by ','
        select *, 
            sum(cnt) over(partition by model_year order by model_year rows unbounded preceding) as cons_sum
        from (select make, model_year, count(make) as cnt from s3_dbt.silver group by model_year, make order by make
        ) as tb1;
    
  
[0m12:05:23.783039 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:05:26.315417 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:27.065077 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:27.811624 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:28.557064 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:29.302542 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:30.053158 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:30.798916 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:31.544579 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:32.290499 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:33.041002 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:33.786691 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:34.533614 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:35.282879 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:36.529655 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:37.775519 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:39.023719 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:40.284227 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:41.530031 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:42.774631 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:44.021588 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:45.267945 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:47.015077 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:48.760485 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:50.505279 [debug] [MainThread]: Spark adapter: Poll status: 1, sleeping
[0m12:05:51.766330 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m12:05:51.768002 [debug] [MainThread]: SQL status: OK in 27.985 seconds
[0m12:05:51.773271 [debug] [MainThread]: Using spark connection "macro_gold"
[0m12:05:51.774723 [debug] [MainThread]: On macro_gold: /* {"app": "dbt", "dbt_version": "1.9.2", "profile_name": "dbt_spark_emr_app", "target_name": "dev", "connection_name": "macro_gold"} */

    

        CREATE EXTERNAL TABLE if not exists s3_dbt.gold (
            make string,
            model_year int,
            cnt int, 
            cons_sum int
        )
        ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
        WITH SERDEPROPERTIES ('separatorChar' = ',')
        STORED AS TEXTFILE
        LOCATION 's3://dev-mdf-data-lake/dbt-spark-emr-poc/data/gold/'
        TBLPROPERTIES (
        'serialization.null.format' = '',
        'skip.header.line.count' = '1');
        
    
  
[0m12:05:52.276511 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m12:05:52.277902 [debug] [MainThread]: SQL status: OK in 0.502 seconds
[0m12:05:52.280652 [debug] [MainThread]: On macro_gold: ROLLBACK
[0m12:05:52.281065 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m12:05:52.281395 [debug] [MainThread]: On macro_gold: Close
[0m12:05:52.858985 [debug] [MainThread]: Wrote artifact RunResultsArtifact to /usr/lib/spark/spark-3.5.3-bin-hadoop3/DBT/dbt-apps/dbt-spark-emr/dbt_spark_emr_app/target/run_results.json
[0m12:05:52.861595 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 29.887564, "process_in_blocks": "0", "process_kernel_time": 0.828125, "process_mem_max_rss": "98940", "process_out_blocks": "0", "process_user_time": 1.46875}
[0m12:05:52.863221 [debug] [MainThread]: Command `dbt run-operation` succeeded at 12:05:52.862860 after 29.89 seconds
[0m12:05:52.864377 [debug] [MainThread]: Connection 'macro_gold' was properly closed.
[0m12:05:52.865550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe45a43fb30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe459739490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe459985c70>]}
[0m12:05:52.866361 [debug] [MainThread]: An error was encountered while trying to send an event
[0m12:05:52.866619 [debug] [MainThread]: Flushing usage events
